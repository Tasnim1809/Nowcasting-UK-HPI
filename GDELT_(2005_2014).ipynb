{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ======================= GDELT 1.0 EVENTS → 2005 monthly UK AvgTone =======================\n",
        "# Outputs (saved to Drive: /content/drive/MyDrive/msc_project/gdelt_events_1p0):\n",
        "#   - events_uk_monthly_2005.csv\n",
        "#   - events_uk_economic_monthly_2005.csv\n",
        "\n",
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import io, zipfile, requests, pandas as pd, numpy as np, math, re\n",
        "from pathlib import Path\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "YEAR = 2005  # change if needed\n",
        "BASE = \"http://data.gdeltproject.org/events\"  # HTTP avoids SSL hostname issues\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt_events_1p0\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UK_FIPS = \"UK\"    # ActionGeo_* country code (FIPS-2)\n",
        "UK_ISO3 = \"GBR\"   # Actor*CountryCode (ISO-3/CAMEO)\n",
        "CHUNK = 200_000   # rows per chunk\n",
        "\n",
        "# Events 1.0 column positions (pre-2013 format; 57 columns total: 0..56)\n",
        "# 1=SQLDATE, 7=Actor1CountryCode, 17=Actor2CountryCode, 51=ActionGeo_CountryCode,\n",
        "# 34=AvgTone, 28=EventRootCode\n",
        "COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT = 1, 7, 17, 51, 34, 28\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-events-2005/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def fetch_year_zip(year: int):\n",
        "    \"\"\"Try the direct yearly ZIP (1979–2005). Fallback to index listing if needed.\"\"\"\n",
        "    url_direct = f\"{BASE}/{year}.zip\"\n",
        "    try:\n",
        "        r = session().get(url_direct, timeout=180)\n",
        "        if r.ok:\n",
        "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "            inner = z.namelist()[0]\n",
        "            return url_direct, z, inner\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Fallback via /events/index.html\n",
        "    idx_html = session().get(f\"{BASE}/index.html\", timeout=120).text\n",
        "    m = re.search(rf'href=\"(.*?/{year}\\.zip)\"', idx_html, re.I)\n",
        "    if not m:\n",
        "        raise RuntimeError(f\"Could not find a yearly ZIP for {year} on the events index.\")\n",
        "    url = m.group(1)\n",
        "    if url.startswith(\"/\"):\n",
        "        url = \"http://data.gdeltproject.org\" + url\n",
        "    r = session().get(url, timeout=180); r.raise_for_status()\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    inner = z.namelist()[0]\n",
        "    return url, z, inner\n",
        "\n",
        "def add_chunk(df, counts, tone_sum, tone_n):\n",
        "    \"\"\"Aggregate UK-related rows in this chunk into monthly Docs and AvgTone sums.\"\"\"\n",
        "    # UK if ActionGeo FIPS==UK OR Actor1/2 ISO3==GBR\n",
        "    m_uk = (df[COL_ACTC].eq(UK_FIPS)) | (df[COL_A1C].eq(UK_ISO3)) | (df[COL_A2C].eq(UK_ISO3))\n",
        "    if not m_uk.any():\n",
        "        return\n",
        "\n",
        "    sub = df.loc[m_uk, [COL_SQLDATE, COL_TONE]].copy()\n",
        "\n",
        "    # Month = calendar month start (YYYY-MM-01)\n",
        "    sub[\"Month\"] = (\n",
        "        pd.to_datetime(sub[COL_SQLDATE], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "          .dt.to_period(\"M\")\n",
        "          .dt.to_timestamp()                  # start-of-month\n",
        "    )\n",
        "    sub[\"AvgTone\"] = pd.to_numeric(sub[COL_TONE], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[\"Month\"])\n",
        "    if sub.empty:\n",
        "        return\n",
        "\n",
        "    # Vectorized per-chunk aggregation\n",
        "    docs = sub.groupby(\"Month\", as_index=False).size().rename(columns={\"size\": \"Docs\"})\n",
        "    tsum = sub.dropna(subset=[\"AvgTone\"]).groupby(\"Month\", as_index=False)[\"AvgTone\"].sum()\\\n",
        "              .rename(columns={\"AvgTone\": \"ToneSum\"})\n",
        "    tnum = sub[\"AvgTone\"].notna().groupby(sub[\"Month\"]).sum().reset_index(name=\"ToneN\")\n",
        "\n",
        "    tmp = docs.merge(tsum, on=\"Month\", how=\"left\").merge(tnum, on=\"Month\", how=\"left\").fillna(0)\n",
        "    for _, r in tmp.iterrows():\n",
        "        m = r[\"Month\"]\n",
        "        counts[m]   += int(r[\"Docs\"])\n",
        "        tone_sum[m] += float(r[\"ToneSum\"])\n",
        "        tone_n[m]   += int(r[\"ToneN\"])\n",
        "\n",
        "def add_chunk_econ(df, counts, tone_sum, tone_n):\n",
        "    \"\"\"Economic family rows only: EventRootCode starts with '04'.\"\"\"\n",
        "    m_econ = df[COL_ROOT].astype(str).str.startswith(\"04\", na=False)\n",
        "    if m_econ.any():\n",
        "        add_chunk(df.loc[m_econ, :], counts, tone_sum, tone_n)\n",
        "\n",
        "def finalize(counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m] / tone_n[m]) if tone_n[m] else np.nan\n",
        "        rows.append({\"Month\": m, \"Docs\": n, \"AvgTone\": avg})\n",
        "    return pd.DataFrame(rows).sort_values(\"Month\").reset_index(drop=True)\n",
        "\n",
        "# ---------- run ----------\n",
        "url, z, inner = fetch_year_zip(YEAR)\n",
        "print(f\"Fetched: {url} | Inner: {inner}\")\n",
        "\n",
        "reader = pd.read_csv(\n",
        "    z.open(inner),\n",
        "    sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "    usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "    dtype={COL_SQLDATE: str, COL_A1C: str, COL_A2C: str, COL_ACTC: str, COL_TONE: float, COL_ROOT: str}\n",
        ")\n",
        "\n",
        "uk_counts,   uk_tsum,   uk_tn   = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "econ_counts, econ_tsum, econ_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "total_rows = 0\n",
        "for i, chunk in enumerate(reader, 1):\n",
        "    total_rows += len(chunk)\n",
        "    add_chunk(chunk,      uk_counts,   uk_tsum,   uk_tn)\n",
        "    add_chunk_econ(chunk, econ_counts, econ_tsum, econ_tn)\n",
        "    if i % 5 == 0:\n",
        "        print(f\"Processed ~{total_rows:,} rows…\")\n",
        "\n",
        "df_uk   = finalize(uk_counts,   uk_tsum,   uk_tn)\n",
        "df_econ = finalize(econ_counts, econ_tsum, econ_tn)\n",
        "\n",
        "# Save to Drive\n",
        "out_uk   = OUT_DIR / f\"events_uk_monthly_{YEAR}.csv\"\n",
        "out_econ = OUT_DIR / f\"events_uk_economic_monthly_{YEAR}.csv\"\n",
        "df_uk.to_csv(out_uk, index=False, date_format=\"%Y-%m-%d\")\n",
        "df_econ.to_csv(out_econ, index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n",
        "print(\"UK-all monthly:\")\n",
        "print(df_uk.to_string(index=False))\n",
        "print(\"\\nUK-economic monthly (EventRootCode '04*'):\")\n",
        "print(df_econ.to_string(index=False))\n",
        "print(f\"\\nWrote:\\n  {out_uk}\\n  {out_econ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVoPQ80a2o4W",
        "outputId": "5ec43b7b-eb03-4b82-93cd-9810749cc849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Fetched: http://data.gdeltproject.org/events/2005.zip | Inner: 2005.csv\n",
            "Processed ~1,000,000 rows…\n",
            "Processed ~2,000,000 rows…\n",
            "Processed ~3,000,000 rows…\n",
            "\n",
            "--- Done ---\n",
            "UK-all monthly:\n",
            "     Month  Docs  AvgTone\n",
            "2005-01-01 12223 5.485720\n",
            "2005-02-01 14461 5.838248\n",
            "2005-03-01 12688 5.779465\n",
            "2005-04-01  9765 5.893368\n",
            "2005-05-01 10942 5.626196\n",
            "2005-06-01 14543 5.742366\n",
            "2005-07-01 25623 4.728918\n",
            "2005-08-01 14541 4.783105\n",
            "2005-09-01 14552 5.197001\n",
            "2005-10-01 14015 5.401413\n",
            "2005-11-01 14822 5.419917\n",
            "2005-12-01 12740 5.520385\n",
            "\n",
            "UK-economic monthly (EventRootCode '04*'):\n",
            "     Month  Docs  AvgTone\n",
            "2005-01-01  3443 5.659953\n",
            "2005-02-01  4481 5.849820\n",
            "2005-03-01  3601 6.084276\n",
            "2005-04-01  2613 6.143649\n",
            "2005-05-01  2777 5.747168\n",
            "2005-06-01  4020 5.632774\n",
            "2005-07-01  6308 5.016168\n",
            "2005-08-01  3363 4.927621\n",
            "2005-09-01  3698 5.500993\n",
            "2005-10-01  3940 5.579145\n",
            "2005-11-01  4564 5.763567\n",
            "2005-12-01  2984 5.612645\n",
            "\n",
            "Wrote:\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2005.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2005.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2006"
      ],
      "metadata": {
        "id": "3tZ0KAvj636Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= GDELT 1.0 EVENTS → 2006 monthly UK AvgTone =======================\n",
        "# Saves to Google Drive: /content/drive/MyDrive/msc_project/gdelt_events_1p0\n",
        "# Outputs:\n",
        "#   - events_uk_monthly_2006.csv\n",
        "#   - events_uk_economic_monthly_2006.csv\n",
        "# Notes: 2006 files are monthly ZIPs like 200601.zip, 200602.zip, ... (through 201303.zip).\n",
        "\n",
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import io, zipfile, requests, pandas as pd, numpy as np, math, re\n",
        "from pathlib import Path\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "YEAR = 2006\n",
        "BASE = \"http://data.gdeltproject.org/events\"   # index lists 200601.zip, 200602.zip, ... for 2006\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt_events_1p0\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UK_FIPS = \"UK\"    # ActionGeo_* uses FIPS-2\n",
        "UK_ISO3 = \"GBR\"   # Actor*CountryCode uses ISO-3\n",
        "CHUNK = 200_000\n",
        "\n",
        "# Column positions in Events 1.0 (57 cols total: 0..56)\n",
        "# 1=SQLDATE, 7=Actor1CountryCode, 17=Actor2CountryCode, 51=ActionGeo_CountryCode,\n",
        "# 34=AvgTone, 28=EventRootCode\n",
        "COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT = 1, 7, 17, 51, 34, 28\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-events-monthly/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def monthly_urls(year: int):\n",
        "    # 2006–2012 → 12 months; 2013 → months 1..3\n",
        "    last_m = 3 if year == 2013 else 12\n",
        "    return [f\"{BASE}/{year}{m:02d}.zip\" for m in range(1, last_m+1)]\n",
        "\n",
        "def stream_month(url: str):\n",
        "    \"\"\"Yield a (ZipFile, inner_csv_name) pair for a monthly ZIP, or None if 404.\"\"\"\n",
        "    r = session().get(url, timeout=180)\n",
        "    if not r.ok:\n",
        "        return None\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    inner = z.namelist()[0]  # e.g., '200601.csv'\n",
        "    return z, inner\n",
        "\n",
        "def add_chunk(df, counts, tone_sum, tone_n):\n",
        "    # UK if ActionGeo FIPS==UK OR Actor1/2 ISO3==GBR\n",
        "    m_uk = (df[COL_ACTC].eq(UK_FIPS)) | (df[COL_A1C].eq(UK_ISO3)) | (df[COL_A2C].eq(UK_ISO3))\n",
        "    if not m_uk.any():\n",
        "        return\n",
        "    sub = df.loc[m_uk, [COL_SQLDATE, COL_TONE]].copy()\n",
        "    sub[\"Month\"] = (\n",
        "        pd.to_datetime(sub[COL_SQLDATE], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "          .dt.to_period(\"M\")\n",
        "          .dt.to_timestamp()     # month start\n",
        "    )\n",
        "    sub[\"AvgTone\"] = pd.to_numeric(sub[COL_TONE], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[\"Month\"])\n",
        "    if sub.empty:\n",
        "        return\n",
        "\n",
        "    # Vectorized per-chunk aggregation\n",
        "    docs = sub.groupby(\"Month\", as_index=False).size().rename(columns={\"size\": \"Docs\"})\n",
        "    tsum = sub.dropna(subset=[\"AvgTone\"]).groupby(\"Month\", as_index=False)[\"AvgTone\"].sum()\\\n",
        "              .rename(columns={\"AvgTone\": \"ToneSum\"})\n",
        "    tnum = sub[\"AvgTone\"].notna().groupby(sub[\"Month\"]).sum().reset_index(name=\"ToneN\")\n",
        "    tmp = docs.merge(tsum, on=\"Month\", how=\"left\").merge(tnum, on=\"Month\", how=\"left\").fillna(0)\n",
        "\n",
        "    for _, r in tmp.iterrows():\n",
        "        m = r[\"Month\"]\n",
        "        counts[m]   += int(r[\"Docs\"])\n",
        "        tone_sum[m] += float(r[\"ToneSum\"])\n",
        "        tone_n[m]   += int(r[\"ToneN\"])\n",
        "\n",
        "def add_chunk_econ(df, counts, tone_sum, tone_n):\n",
        "    # Economic family: EventRootCode starts with '04'\n",
        "    m_econ = df[COL_ROOT].astype(str).str.startswith(\"04\", na=False)\n",
        "    if m_econ.any():\n",
        "        add_chunk(df.loc[m_econ, :], counts, tone_sum, tone_n)\n",
        "\n",
        "def finalize(counts, tone_sum, tone_n, year:int):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        if m.year != year:\n",
        "            continue\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m] / tone_n[m]) if tone_n[m] else np.nan\n",
        "        rows.append({\"Month\": m, \"Docs\": n, \"AvgTone\": avg})\n",
        "    # reindex to full year months so any missing appear explicitly\n",
        "    df = pd.DataFrame(rows).sort_values(\"Month\").reset_index(drop=True)\n",
        "    allm = pd.date_range(f\"{year}-01-01\", f\"{year}-12-01\" if year != 2013 else f\"{year}-03-01\", freq=\"MS\")\n",
        "    df = (df.set_index(\"Month\")\n",
        "            .reindex(allm)\n",
        "            .rename_axis(\"Month\")\n",
        "            .reset_index())\n",
        "    return df\n",
        "\n",
        "# ---------- run ----------\n",
        "urls = monthly_urls(YEAR)\n",
        "print(f\"{YEAR}: attempting {len(urls)} monthly files\")\n",
        "uk_counts, uk_tsum, uk_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "ec_counts, ec_tsum, ec_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "for url in urls:\n",
        "    got = stream_month(url)\n",
        "    if got is None:\n",
        "        print(\"SKIP (not found):\", url)\n",
        "        continue\n",
        "    z, inner = got\n",
        "    print(\"Processing:\", inner)\n",
        "    reader = pd.read_csv(\n",
        "        z.open(inner),\n",
        "        sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "        usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "        dtype={COL_SQLDATE:str, COL_A1C:str, COL_A2C:str, COL_ACTC:str, COL_TONE:float, COL_ROOT:str}\n",
        "    )\n",
        "    rows = 0\n",
        "    for i, chunk in enumerate(reader, 1):\n",
        "        rows += len(chunk)\n",
        "        add_chunk(chunk,   uk_counts, uk_tsum, uk_tn)\n",
        "        add_chunk_econ(chunk, ec_counts, ec_tsum, ec_tn)\n",
        "        if i % 5 == 0:\n",
        "            print(f\"  read ~{rows:,} rows…\")\n",
        "\n",
        "# finalize\n",
        "df_uk   = finalize(uk_counts, uk_tsum, uk_tn, YEAR)\n",
        "df_econ = finalize(ec_counts, ec_tsum, ec_tn, YEAR)\n",
        "\n",
        "# save\n",
        "out_uk   = OUT_DIR / f\"events_uk_monthly_{YEAR}.csv\"\n",
        "out_econ = OUT_DIR / f\"events_uk_economic_monthly_{YEAR}.csv\"\n",
        "df_uk.to_csv(out_uk, index=False, date_format=\"%Y-%m-%d\")\n",
        "df_econ.to_csv(out_econ, index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n",
        "print(\"UK-all monthly:\")\n",
        "print(df_uk.to_string(index=False))\n",
        "print(\"\\nUK-economic monthly (EventRootCode '04*'):\")\n",
        "print(df_econ.to_string(index=False))\n",
        "print(f\"\\nWrote:\\n  {out_uk}\\n  {out_econ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL-7Zsc0898L",
        "outputId": "f229fa34-b344-47bb-b993-8e433101388f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2006: attempting 12 monthly files\n",
            "Processing: 200601.csv\n",
            "Processing: 200602.csv\n",
            "Processing: 200603.csv\n",
            "Processing: 200604.csv\n",
            "Processing: 200605.csv\n",
            "Processing: 200606.csv\n",
            "Processing: 200607.csv\n",
            "Processing: 200608.csv\n",
            "Processing: 200609.csv\n",
            "Processing: 200610.csv\n",
            "Processing: 200611.csv\n",
            "Processing: 200612.csv\n",
            "\n",
            "--- Done ---\n",
            "UK-all monthly:\n",
            "     Month  Docs  AvgTone\n",
            "2006-01-01 17312 5.221570\n",
            "2006-02-01 15091 5.228871\n",
            "2006-03-01 28846 5.577158\n",
            "2006-04-01 17763 5.630909\n",
            "2006-05-01 23968 5.630888\n",
            "2006-06-01 27549 5.632453\n",
            "2006-07-01 28312 5.433701\n",
            "2006-08-01 39512 5.021931\n",
            "2006-09-01 37393 5.587386\n",
            "2006-10-01 25470 5.655227\n",
            "2006-11-01 42019 5.496018\n",
            "2006-12-01 40233 5.241839\n",
            "\n",
            "UK-economic monthly (EventRootCode '04*'):\n",
            "     Month  Docs  AvgTone\n",
            "2006-01-01  5670 5.345079\n",
            "2006-02-01  3695 5.678923\n",
            "2006-03-01  8627 5.792521\n",
            "2006-04-01  5070 5.805373\n",
            "2006-05-01  6969 5.884226\n",
            "2006-06-01  7556 5.987236\n",
            "2006-07-01  7818 5.436749\n",
            "2006-08-01  8647 5.173487\n",
            "2006-09-01  9348 5.740292\n",
            "2006-10-01  6987 5.724813\n",
            "2006-11-01 10407 5.603168\n",
            "2006-12-01 10216 5.244667\n",
            "\n",
            "Wrote:\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2006.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2006.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2007"
      ],
      "metadata": {
        "id": "9A5tIOu5-LSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import io, zipfile, requests, pandas as pd, numpy as np, math, re\n",
        "from pathlib import Path\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "YEAR = 2007\n",
        "BASE = \"http://data.gdeltproject.org/events\"   # index lists 200601.zip, 200602.zip, ... for 2006\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt_events_1p0\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UK_FIPS = \"UK\"    # ActionGeo_* uses FIPS-2\n",
        "UK_ISO3 = \"GBR\"   # Actor*CountryCode uses ISO-3\n",
        "CHUNK = 200_000\n",
        "\n",
        "# Column positions in Events 1.0 (57 cols total: 0..56)\n",
        "# 1=SQLDATE, 7=Actor1CountryCode, 17=Actor2CountryCode, 51=ActionGeo_CountryCode,\n",
        "# 34=AvgTone, 28=EventRootCode\n",
        "COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT = 1, 7, 17, 51, 34, 28\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-events-monthly/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def monthly_urls(year: int):\n",
        "    # 2006–2012 → 12 months; 2013 → months 1..3\n",
        "    last_m = 3 if year == 2013 else 12\n",
        "    return [f\"{BASE}/{year}{m:02d}.zip\" for m in range(1, last_m+1)]\n",
        "\n",
        "def stream_month(url: str):\n",
        "    \"\"\"Yield a (ZipFile, inner_csv_name) pair for a monthly ZIP, or None if 404.\"\"\"\n",
        "    r = session().get(url, timeout=180)\n",
        "    if not r.ok:\n",
        "        return None\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    inner = z.namelist()[0]  # e.g., '200601.csv'\n",
        "    return z, inner\n",
        "\n",
        "def add_chunk(df, counts, tone_sum, tone_n):\n",
        "    # UK if ActionGeo FIPS==UK OR Actor1/2 ISO3==GBR\n",
        "    m_uk = (df[COL_ACTC].eq(UK_FIPS)) | (df[COL_A1C].eq(UK_ISO3)) | (df[COL_A2C].eq(UK_ISO3))\n",
        "    if not m_uk.any():\n",
        "        return\n",
        "    sub = df.loc[m_uk, [COL_SQLDATE, COL_TONE]].copy()\n",
        "    sub[\"Month\"] = (\n",
        "        pd.to_datetime(sub[COL_SQLDATE], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "          .dt.to_period(\"M\")\n",
        "          .dt.to_timestamp()     # month start\n",
        "    )\n",
        "    sub[\"AvgTone\"] = pd.to_numeric(sub[COL_TONE], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[\"Month\"])\n",
        "    if sub.empty:\n",
        "        return\n",
        "\n",
        "    # Vectorized per-chunk aggregation\n",
        "    docs = sub.groupby(\"Month\", as_index=False).size().rename(columns={\"size\": \"Docs\"})\n",
        "    tsum = sub.dropna(subset=[\"AvgTone\"]).groupby(\"Month\", as_index=False)[\"AvgTone\"].sum()\\\n",
        "              .rename(columns={\"AvgTone\": \"ToneSum\"})\n",
        "    tnum = sub[\"AvgTone\"].notna().groupby(sub[\"Month\"]).sum().reset_index(name=\"ToneN\")\n",
        "    tmp = docs.merge(tsum, on=\"Month\", how=\"left\").merge(tnum, on=\"Month\", how=\"left\").fillna(0)\n",
        "\n",
        "    for _, r in tmp.iterrows():\n",
        "        m = r[\"Month\"]\n",
        "        counts[m]   += int(r[\"Docs\"])\n",
        "        tone_sum[m] += float(r[\"ToneSum\"])\n",
        "        tone_n[m]   += int(r[\"ToneN\"])\n",
        "\n",
        "def add_chunk_econ(df, counts, tone_sum, tone_n):\n",
        "    # Economic family: EventRootCode starts with '04'\n",
        "    m_econ = df[COL_ROOT].astype(str).str.startswith(\"04\", na=False)\n",
        "    if m_econ.any():\n",
        "        add_chunk(df.loc[m_econ, :], counts, tone_sum, tone_n)\n",
        "\n",
        "def finalize(counts, tone_sum, tone_n, year:int):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        if m.year != year:\n",
        "            continue\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m] / tone_n[m]) if tone_n[m] else np.nan\n",
        "        rows.append({\"Month\": m, \"Docs\": n, \"AvgTone\": avg})\n",
        "    # reindex to full year months so any missing appear explicitly\n",
        "    df = pd.DataFrame(rows).sort_values(\"Month\").reset_index(drop=True)\n",
        "    allm = pd.date_range(f\"{year}-01-01\", f\"{year}-12-01\" if year != 2013 else f\"{year}-03-01\", freq=\"MS\")\n",
        "    df = (df.set_index(\"Month\")\n",
        "            .reindex(allm)\n",
        "            .rename_axis(\"Month\")\n",
        "            .reset_index())\n",
        "    return df\n",
        "\n",
        "# ---------- run ----------\n",
        "urls = monthly_urls(YEAR)\n",
        "print(f\"{YEAR}: attempting {len(urls)} monthly files\")\n",
        "uk_counts, uk_tsum, uk_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "ec_counts, ec_tsum, ec_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "for url in urls:\n",
        "    got = stream_month(url)\n",
        "    if got is None:\n",
        "        print(\"SKIP (not found):\", url)\n",
        "        continue\n",
        "    z, inner = got\n",
        "    print(\"Processing:\", inner)\n",
        "    reader = pd.read_csv(\n",
        "        z.open(inner),\n",
        "        sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "        usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "        dtype={COL_SQLDATE:str, COL_A1C:str, COL_A2C:str, COL_ACTC:str, COL_TONE:float, COL_ROOT:str}\n",
        "    )\n",
        "    rows = 0\n",
        "    for i, chunk in enumerate(reader, 1):\n",
        "        rows += len(chunk)\n",
        "        add_chunk(chunk,   uk_counts, uk_tsum, uk_tn)\n",
        "        add_chunk_econ(chunk, ec_counts, ec_tsum, ec_tn)\n",
        "        if i % 5 == 0:\n",
        "            print(f\"  read ~{rows:,} rows…\")\n",
        "\n",
        "# finalize\n",
        "df_uk   = finalize(uk_counts, uk_tsum, uk_tn, YEAR)\n",
        "df_econ = finalize(ec_counts, ec_tsum, ec_tn, YEAR)\n",
        "\n",
        "# save\n",
        "out_uk   = OUT_DIR / f\"events_uk_monthly_{YEAR}.csv\"\n",
        "out_econ = OUT_DIR / f\"events_uk_economic_monthly_{YEAR}.csv\"\n",
        "df_uk.to_csv(out_uk, index=False, date_format=\"%Y-%m-%d\")\n",
        "df_econ.to_csv(out_econ, index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n",
        "print(\"UK-all monthly:\")\n",
        "print(df_uk.to_string(index=False))\n",
        "print(\"\\nUK-economic monthly (EventRootCode '04*'):\")\n",
        "print(df_econ.to_string(index=False))\n",
        "print(f\"\\nWrote:\\n  {out_uk}\\n  {out_econ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbesp9PhAwvw",
        "outputId": "6b555755-50e8-49ef-d085-143eef18ca42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2007: attempting 12 monthly files\n",
            "Processing: 200701.csv\n",
            "Processing: 200702.csv\n",
            "Processing: 200703.csv\n",
            "  read ~903,257 rows…\n",
            "Processing: 200704.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200705.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200706.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200707.csv\n",
            "  read ~960,901 rows…\n",
            "Processing: 200708.csv\n",
            "Processing: 200709.csv\n",
            "  read ~957,387 rows…\n",
            "Processing: 200710.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200711.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200712.csv\n",
            "  read ~940,702 rows…\n",
            "\n",
            "--- Done ---\n",
            "UK-all monthly:\n",
            "     Month  Docs  AvgTone\n",
            "2007-01-01 38932 5.650727\n",
            "2007-02-01 42950 5.515068\n",
            "2007-03-01 69455 5.518773\n",
            "2007-04-01 77598 5.511486\n",
            "2007-05-01 86161 5.475945\n",
            "2007-06-01 81884 5.518685\n",
            "2007-07-01 90549 5.063248\n",
            "2007-08-01 43332 5.276237\n",
            "2007-09-01 60090 5.528454\n",
            "2007-10-01 66685 5.700373\n",
            "2007-11-01 67144 5.483070\n",
            "2007-12-01 63852 5.469246\n",
            "\n",
            "UK-economic monthly (EventRootCode '04*'):\n",
            "     Month  Docs  AvgTone\n",
            "2007-01-01  8799 5.725413\n",
            "2007-02-01 10238 5.847031\n",
            "2007-03-01 15814 5.662984\n",
            "2007-04-01 17897 5.716621\n",
            "2007-05-01 23474 5.624303\n",
            "2007-06-01 19309 5.639841\n",
            "2007-07-01 19736 5.467247\n",
            "2007-08-01  9327 5.458635\n",
            "2007-09-01 16102 5.664197\n",
            "2007-10-01 17139 5.768694\n",
            "2007-11-01 16027 5.671963\n",
            "2007-12-01 16462 5.591290\n",
            "\n",
            "Wrote:\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2007.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2007.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2008"
      ],
      "metadata": {
        "id": "mp_TIv2VDKKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import io, zipfile, requests, pandas as pd, numpy as np, math, re\n",
        "from pathlib import Path\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "YEAR = 2008\n",
        "BASE = \"http://data.gdeltproject.org/events\"   # index lists 200601.zip, 200602.zip, ... for 2006\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt_events_1p0\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UK_FIPS = \"UK\"    # ActionGeo_* uses FIPS-2\n",
        "UK_ISO3 = \"GBR\"   # Actor*CountryCode uses ISO-3\n",
        "CHUNK = 200_000\n",
        "\n",
        "# Column positions in Events 1.0 (57 cols total: 0..56)\n",
        "# 1=SQLDATE, 7=Actor1CountryCode, 17=Actor2CountryCode, 51=ActionGeo_CountryCode,\n",
        "# 34=AvgTone, 28=EventRootCode\n",
        "COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT = 1, 7, 17, 51, 34, 28\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-events-monthly/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def monthly_urls(year: int):\n",
        "    # 2006–2012 → 12 months; 2013 → months 1..3\n",
        "    last_m = 3 if year == 2013 else 12\n",
        "    return [f\"{BASE}/{year}{m:02d}.zip\" for m in range(1, last_m+1)]\n",
        "\n",
        "def stream_month(url: str):\n",
        "    \"\"\"Yield a (ZipFile, inner_csv_name) pair for a monthly ZIP, or None if 404.\"\"\"\n",
        "    r = session().get(url, timeout=180)\n",
        "    if not r.ok:\n",
        "        return None\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    inner = z.namelist()[0]  # e.g., '200601.csv'\n",
        "    return z, inner\n",
        "\n",
        "def add_chunk(df, counts, tone_sum, tone_n):\n",
        "    # UK if ActionGeo FIPS==UK OR Actor1/2 ISO3==GBR\n",
        "    m_uk = (df[COL_ACTC].eq(UK_FIPS)) | (df[COL_A1C].eq(UK_ISO3)) | (df[COL_A2C].eq(UK_ISO3))\n",
        "    if not m_uk.any():\n",
        "        return\n",
        "    sub = df.loc[m_uk, [COL_SQLDATE, COL_TONE]].copy()\n",
        "    sub[\"Month\"] = (\n",
        "        pd.to_datetime(sub[COL_SQLDATE], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "          .dt.to_period(\"M\")\n",
        "          .dt.to_timestamp()     # month start\n",
        "    )\n",
        "    sub[\"AvgTone\"] = pd.to_numeric(sub[COL_TONE], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[\"Month\"])\n",
        "    if sub.empty:\n",
        "        return\n",
        "\n",
        "    # Vectorized per-chunk aggregation\n",
        "    docs = sub.groupby(\"Month\", as_index=False).size().rename(columns={\"size\": \"Docs\"})\n",
        "    tsum = sub.dropna(subset=[\"AvgTone\"]).groupby(\"Month\", as_index=False)[\"AvgTone\"].sum()\\\n",
        "              .rename(columns={\"AvgTone\": \"ToneSum\"})\n",
        "    tnum = sub[\"AvgTone\"].notna().groupby(sub[\"Month\"]).sum().reset_index(name=\"ToneN\")\n",
        "    tmp = docs.merge(tsum, on=\"Month\", how=\"left\").merge(tnum, on=\"Month\", how=\"left\").fillna(0)\n",
        "\n",
        "    for _, r in tmp.iterrows():\n",
        "        m = r[\"Month\"]\n",
        "        counts[m]   += int(r[\"Docs\"])\n",
        "        tone_sum[m] += float(r[\"ToneSum\"])\n",
        "        tone_n[m]   += int(r[\"ToneN\"])\n",
        "\n",
        "def add_chunk_econ(df, counts, tone_sum, tone_n):\n",
        "    # Economic family: EventRootCode starts with '04'\n",
        "    m_econ = df[COL_ROOT].astype(str).str.startswith(\"04\", na=False)\n",
        "    if m_econ.any():\n",
        "        add_chunk(df.loc[m_econ, :], counts, tone_sum, tone_n)\n",
        "\n",
        "def finalize(counts, tone_sum, tone_n, year:int):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        if m.year != year:\n",
        "            continue\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m] / tone_n[m]) if tone_n[m] else np.nan\n",
        "        rows.append({\"Month\": m, \"Docs\": n, \"AvgTone\": avg})\n",
        "    # reindex to full year months so any missing appear explicitly\n",
        "    df = pd.DataFrame(rows).sort_values(\"Month\").reset_index(drop=True)\n",
        "    allm = pd.date_range(f\"{year}-01-01\", f\"{year}-12-01\" if year != 2013 else f\"{year}-03-01\", freq=\"MS\")\n",
        "    df = (df.set_index(\"Month\")\n",
        "            .reindex(allm)\n",
        "            .rename_axis(\"Month\")\n",
        "            .reset_index())\n",
        "    return df\n",
        "\n",
        "# ---------- run ----------\n",
        "urls = monthly_urls(YEAR)\n",
        "print(f\"{YEAR}: attempting {len(urls)} monthly files\")\n",
        "uk_counts, uk_tsum, uk_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "ec_counts, ec_tsum, ec_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "for url in urls:\n",
        "    got = stream_month(url)\n",
        "    if got is None:\n",
        "        print(\"SKIP (not found):\", url)\n",
        "        continue\n",
        "    z, inner = got\n",
        "    print(\"Processing:\", inner)\n",
        "    reader = pd.read_csv(\n",
        "        z.open(inner),\n",
        "        sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "        usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "        dtype={COL_SQLDATE:str, COL_A1C:str, COL_A2C:str, COL_ACTC:str, COL_TONE:float, COL_ROOT:str}\n",
        "    )\n",
        "    rows = 0\n",
        "    for i, chunk in enumerate(reader, 1):\n",
        "        rows += len(chunk)\n",
        "        add_chunk(chunk,   uk_counts, uk_tsum, uk_tn)\n",
        "        add_chunk_econ(chunk, ec_counts, ec_tsum, ec_tn)\n",
        "        if i % 5 == 0:\n",
        "            print(f\"  read ~{rows:,} rows…\")\n",
        "\n",
        "# finalize\n",
        "df_uk   = finalize(uk_counts, uk_tsum, uk_tn, YEAR)\n",
        "df_econ = finalize(ec_counts, ec_tsum, ec_tn, YEAR)\n",
        "\n",
        "# save\n",
        "out_uk   = OUT_DIR / f\"events_uk_monthly_{YEAR}.csv\"\n",
        "out_econ = OUT_DIR / f\"events_uk_economic_monthly_{YEAR}.csv\"\n",
        "df_uk.to_csv(out_uk, index=False, date_format=\"%Y-%m-%d\")\n",
        "df_econ.to_csv(out_econ, index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n",
        "print(\"UK-all monthly:\")\n",
        "print(df_uk.to_string(index=False))\n",
        "print(\"\\nUK-economic monthly (EventRootCode '04*'):\")\n",
        "print(df_econ.to_string(index=False))\n",
        "print(f\"\\nWrote:\\n  {out_uk}\\n  {out_econ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds5Sret6AxsO",
        "outputId": "00a28ead-72b0-4119-96a2-18537098daad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2008: attempting 12 monthly files\n",
            "Processing: 200801.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200802.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200803.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200804.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200805.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200806.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200807.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200808.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200809.csv\n",
            "  read ~912,791 rows…\n",
            "Processing: 200810.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200811.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200812.csv\n",
            "  read ~1,000,000 rows…\n",
            "\n",
            "--- Done ---\n",
            "UK-all monthly:\n",
            "     Month  Docs  AvgTone\n",
            "2008-01-01 75043 5.551900\n",
            "2008-02-01 77358 5.573736\n",
            "2008-03-01 65256 5.635886\n",
            "2008-04-01 67284 5.646853\n",
            "2008-05-01 70249 5.763192\n",
            "2008-06-01 80523 5.596329\n",
            "2008-07-01 85633 5.616669\n",
            "2008-08-01 55390 5.580945\n",
            "2008-09-01 51242 5.878708\n",
            "2008-10-01 75640 5.825013\n",
            "2008-11-01 77855 5.696601\n",
            "2008-12-01 75121 5.574498\n",
            "\n",
            "UK-economic monthly (EventRootCode '04*'):\n",
            "     Month  Docs  AvgTone\n",
            "2008-01-01 19571 5.739710\n",
            "2008-02-01 18318 5.746198\n",
            "2008-03-01 15791 5.833972\n",
            "2008-04-01 17338 5.768374\n",
            "2008-05-01 18000 5.851488\n",
            "2008-06-01 20096 5.610849\n",
            "2008-07-01 24149 5.711628\n",
            "2008-08-01 12356 5.625138\n",
            "2008-09-01 12196 5.892837\n",
            "2008-10-01 18127 5.930972\n",
            "2008-11-01 19881 5.893839\n",
            "2008-12-01 16702 5.686230\n",
            "\n",
            "Wrote:\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2008.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2008.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2009"
      ],
      "metadata": {
        "id": "rLvhxJ_oHhgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import io, zipfile, requests, pandas as pd, numpy as np, math, re\n",
        "from pathlib import Path\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "YEAR = 2009\n",
        "BASE = \"http://data.gdeltproject.org/events\"   # index lists 200601.zip, 200602.zip, ... for 2006\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt_events_1p0\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UK_FIPS = \"UK\"    # ActionGeo_* uses FIPS-2\n",
        "UK_ISO3 = \"GBR\"   # Actor*CountryCode uses ISO-3\n",
        "CHUNK = 200_000\n",
        "\n",
        "# Column positions in Events 1.0 (57 cols total: 0..56)\n",
        "# 1=SQLDATE, 7=Actor1CountryCode, 17=Actor2CountryCode, 51=ActionGeo_CountryCode,\n",
        "# 34=AvgTone, 28=EventRootCode\n",
        "COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT = 1, 7, 17, 51, 34, 28\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-events-monthly/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def monthly_urls(year: int):\n",
        "    # 2006–2012 → 12 months; 2013 → months 1..3\n",
        "    last_m = 3 if year == 2013 else 12\n",
        "    return [f\"{BASE}/{year}{m:02d}.zip\" for m in range(1, last_m+1)]\n",
        "\n",
        "def stream_month(url: str):\n",
        "    \"\"\"Yield a (ZipFile, inner_csv_name) pair for a monthly ZIP, or None if 404.\"\"\"\n",
        "    r = session().get(url, timeout=180)\n",
        "    if not r.ok:\n",
        "        return None\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    inner = z.namelist()[0]  # e.g., '200601.csv'\n",
        "    return z, inner\n",
        "\n",
        "def add_chunk(df, counts, tone_sum, tone_n):\n",
        "    # UK if ActionGeo FIPS==UK OR Actor1/2 ISO3==GBR\n",
        "    m_uk = (df[COL_ACTC].eq(UK_FIPS)) | (df[COL_A1C].eq(UK_ISO3)) | (df[COL_A2C].eq(UK_ISO3))\n",
        "    if not m_uk.any():\n",
        "        return\n",
        "    sub = df.loc[m_uk, [COL_SQLDATE, COL_TONE]].copy()\n",
        "    sub[\"Month\"] = (\n",
        "        pd.to_datetime(sub[COL_SQLDATE], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "          .dt.to_period(\"M\")\n",
        "          .dt.to_timestamp()     # month start\n",
        "    )\n",
        "    sub[\"AvgTone\"] = pd.to_numeric(sub[COL_TONE], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[\"Month\"])\n",
        "    if sub.empty:\n",
        "        return\n",
        "\n",
        "    # Vectorized per-chunk aggregation\n",
        "    docs = sub.groupby(\"Month\", as_index=False).size().rename(columns={\"size\": \"Docs\"})\n",
        "    tsum = sub.dropna(subset=[\"AvgTone\"]).groupby(\"Month\", as_index=False)[\"AvgTone\"].sum()\\\n",
        "              .rename(columns={\"AvgTone\": \"ToneSum\"})\n",
        "    tnum = sub[\"AvgTone\"].notna().groupby(sub[\"Month\"]).sum().reset_index(name=\"ToneN\")\n",
        "    tmp = docs.merge(tsum, on=\"Month\", how=\"left\").merge(tnum, on=\"Month\", how=\"left\").fillna(0)\n",
        "\n",
        "    for _, r in tmp.iterrows():\n",
        "        m = r[\"Month\"]\n",
        "        counts[m]   += int(r[\"Docs\"])\n",
        "        tone_sum[m] += float(r[\"ToneSum\"])\n",
        "        tone_n[m]   += int(r[\"ToneN\"])\n",
        "\n",
        "def add_chunk_econ(df, counts, tone_sum, tone_n):\n",
        "    # Economic family: EventRootCode starts with '04'\n",
        "    m_econ = df[COL_ROOT].astype(str).str.startswith(\"04\", na=False)\n",
        "    if m_econ.any():\n",
        "        add_chunk(df.loc[m_econ, :], counts, tone_sum, tone_n)\n",
        "\n",
        "def finalize(counts, tone_sum, tone_n, year:int):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        if m.year != year:\n",
        "            continue\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m] / tone_n[m]) if tone_n[m] else np.nan\n",
        "        rows.append({\"Month\": m, \"Docs\": n, \"AvgTone\": avg})\n",
        "    # reindex to full year months so any missing appear explicitly\n",
        "    df = pd.DataFrame(rows).sort_values(\"Month\").reset_index(drop=True)\n",
        "    allm = pd.date_range(f\"{year}-01-01\", f\"{year}-12-01\" if year != 2013 else f\"{year}-03-01\", freq=\"MS\")\n",
        "    df = (df.set_index(\"Month\")\n",
        "            .reindex(allm)\n",
        "            .rename_axis(\"Month\")\n",
        "            .reset_index())\n",
        "    return df\n",
        "\n",
        "# ---------- run ----------\n",
        "urls = monthly_urls(YEAR)\n",
        "print(f\"{YEAR}: attempting {len(urls)} monthly files\")\n",
        "uk_counts, uk_tsum, uk_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "ec_counts, ec_tsum, ec_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "for url in urls:\n",
        "    got = stream_month(url)\n",
        "    if got is None:\n",
        "        print(\"SKIP (not found):\", url)\n",
        "        continue\n",
        "    z, inner = got\n",
        "    print(\"Processing:\", inner)\n",
        "    reader = pd.read_csv(\n",
        "        z.open(inner),\n",
        "        sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "        usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "        dtype={COL_SQLDATE:str, COL_A1C:str, COL_A2C:str, COL_ACTC:str, COL_TONE:float, COL_ROOT:str}\n",
        "    )\n",
        "    rows = 0\n",
        "    for i, chunk in enumerate(reader, 1):\n",
        "        rows += len(chunk)\n",
        "        add_chunk(chunk,   uk_counts, uk_tsum, uk_tn)\n",
        "        add_chunk_econ(chunk, ec_counts, ec_tsum, ec_tn)\n",
        "        if i % 5 == 0:\n",
        "            print(f\"  read ~{rows:,} rows…\")\n",
        "\n",
        "# finalize\n",
        "df_uk   = finalize(uk_counts, uk_tsum, uk_tn, YEAR)\n",
        "df_econ = finalize(ec_counts, ec_tsum, ec_tn, YEAR)\n",
        "\n",
        "# save\n",
        "out_uk   = OUT_DIR / f\"events_uk_monthly_{YEAR}.csv\"\n",
        "out_econ = OUT_DIR / f\"events_uk_economic_monthly_{YEAR}.csv\"\n",
        "df_uk.to_csv(out_uk, index=False, date_format=\"%Y-%m-%d\")\n",
        "df_econ.to_csv(out_econ, index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n",
        "print(\"UK-all monthly:\")\n",
        "print(df_uk.to_string(index=False))\n",
        "print(\"\\nUK-economic monthly (EventRootCode '04*'):\")\n",
        "print(df_econ.to_string(index=False))\n",
        "print(f\"\\nWrote:\\n  {out_uk}\\n  {out_econ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8sw_cspHi52",
        "outputId": "9023c124-2fc1-4895-d433-cfbeed08b06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2009: attempting 12 monthly files\n",
            "Processing: 200901.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~1,971,575 rows…\n",
            "Processing: 200902.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200903.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200904.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~1,910,335 rows…\n",
            "Processing: 200905.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~1,917,261 rows…\n",
            "Processing: 200906.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~1,808,376 rows…\n",
            "Processing: 200907.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 200908.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 200909.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 200910.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 200911.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~1,893,590 rows…\n",
            "Processing: 200912.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "\n",
            "--- Done ---\n",
            "UK-all monthly:\n",
            "     Month   Docs  AvgTone\n",
            "2009-01-01  99103 5.599559\n",
            "2009-02-01  99910 5.554991\n",
            "2009-03-01 109427 5.576604\n",
            "2009-04-01 111652 5.625235\n",
            "2009-05-01  98546 5.672658\n",
            "2009-06-01 111020 5.602716\n",
            "2009-07-01  99586 5.550419\n",
            "2009-08-01 129756 5.778115\n",
            "2009-09-01 141935 5.754052\n",
            "2009-10-01 127707 5.807638\n",
            "2009-11-01 112552 5.790218\n",
            "2009-12-01 125901 5.582183\n",
            "\n",
            "UK-economic monthly (EventRootCode '04*'):\n",
            "     Month  Docs  AvgTone\n",
            "2009-01-01 25719 5.592004\n",
            "2009-02-01 27269 5.618076\n",
            "2009-03-01 28277 5.713109\n",
            "2009-04-01 34073 5.855136\n",
            "2009-05-01 26065 5.786144\n",
            "2009-06-01 28319 5.762501\n",
            "2009-07-01 24327 5.827318\n",
            "2009-08-01 31623 5.798037\n",
            "2009-09-01 37955 5.871977\n",
            "2009-10-01 34476 5.862122\n",
            "2009-11-01 28492 5.933693\n",
            "2009-12-01 30508 5.680389\n",
            "\n",
            "Wrote:\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2009.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2009.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2010"
      ],
      "metadata": {
        "id": "n3AAG_FgqUqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import io, zipfile, requests, pandas as pd, numpy as np, math, re\n",
        "from pathlib import Path\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "YEAR = 2010\n",
        "BASE = \"http://data.gdeltproject.org/events\"   # index lists 200601.zip, 200602.zip, ... for 2006\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt_events_1p0\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UK_FIPS = \"UK\"    # ActionGeo_* uses FIPS-2\n",
        "UK_ISO3 = \"GBR\"   # Actor*CountryCode uses ISO-3\n",
        "CHUNK = 200_000\n",
        "\n",
        "# Column positions in Events 1.0 (57 cols total: 0..56)\n",
        "# 1=SQLDATE, 7=Actor1CountryCode, 17=Actor2CountryCode, 51=ActionGeo_CountryCode,\n",
        "# 34=AvgTone, 28=EventRootCode\n",
        "COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT = 1, 7, 17, 51, 34, 28\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-events-monthly/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def monthly_urls(year: int):\n",
        "    # 2006–2012 → 12 months; 2013 → months 1..3\n",
        "    last_m = 3 if year == 2013 else 12\n",
        "    return [f\"{BASE}/{year}{m:02d}.zip\" for m in range(1, last_m+1)]\n",
        "\n",
        "def stream_month(url: str):\n",
        "    \"\"\"Yield a (ZipFile, inner_csv_name) pair for a monthly ZIP, or None if 404.\"\"\"\n",
        "    r = session().get(url, timeout=180)\n",
        "    if not r.ok:\n",
        "        return None\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    inner = z.namelist()[0]  # e.g., '200601.csv'\n",
        "    return z, inner\n",
        "\n",
        "def add_chunk(df, counts, tone_sum, tone_n):\n",
        "    # UK if ActionGeo FIPS==UK OR Actor1/2 ISO3==GBR\n",
        "    m_uk = (df[COL_ACTC].eq(UK_FIPS)) | (df[COL_A1C].eq(UK_ISO3)) | (df[COL_A2C].eq(UK_ISO3))\n",
        "    if not m_uk.any():\n",
        "        return\n",
        "    sub = df.loc[m_uk, [COL_SQLDATE, COL_TONE]].copy()\n",
        "    sub[\"Month\"] = (\n",
        "        pd.to_datetime(sub[COL_SQLDATE], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "          .dt.to_period(\"M\")\n",
        "          .dt.to_timestamp()     # month start\n",
        "    )\n",
        "    sub[\"AvgTone\"] = pd.to_numeric(sub[COL_TONE], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[\"Month\"])\n",
        "    if sub.empty:\n",
        "        return\n",
        "\n",
        "    # Vectorized per-chunk aggregation\n",
        "    docs = sub.groupby(\"Month\", as_index=False).size().rename(columns={\"size\": \"Docs\"})\n",
        "    tsum = sub.dropna(subset=[\"AvgTone\"]).groupby(\"Month\", as_index=False)[\"AvgTone\"].sum()\\\n",
        "              .rename(columns={\"AvgTone\": \"ToneSum\"})\n",
        "    tnum = sub[\"AvgTone\"].notna().groupby(sub[\"Month\"]).sum().reset_index(name=\"ToneN\")\n",
        "    tmp = docs.merge(tsum, on=\"Month\", how=\"left\").merge(tnum, on=\"Month\", how=\"left\").fillna(0)\n",
        "\n",
        "    for _, r in tmp.iterrows():\n",
        "        m = r[\"Month\"]\n",
        "        counts[m]   += int(r[\"Docs\"])\n",
        "        tone_sum[m] += float(r[\"ToneSum\"])\n",
        "        tone_n[m]   += int(r[\"ToneN\"])\n",
        "\n",
        "def add_chunk_econ(df, counts, tone_sum, tone_n):\n",
        "    # Economic family: EventRootCode starts with '04'\n",
        "    m_econ = df[COL_ROOT].astype(str).str.startswith(\"04\", na=False)\n",
        "    if m_econ.any():\n",
        "        add_chunk(df.loc[m_econ, :], counts, tone_sum, tone_n)\n",
        "\n",
        "def finalize(counts, tone_sum, tone_n, year:int):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        if m.year != year:\n",
        "            continue\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m] / tone_n[m]) if tone_n[m] else np.nan\n",
        "        rows.append({\"Month\": m, \"Docs\": n, \"AvgTone\": avg})\n",
        "    # reindex to full year months so any missing appear explicitly\n",
        "    df = pd.DataFrame(rows).sort_values(\"Month\").reset_index(drop=True)\n",
        "    allm = pd.date_range(f\"{year}-01-01\", f\"{year}-12-01\" if year != 2013 else f\"{year}-03-01\", freq=\"MS\")\n",
        "    df = (df.set_index(\"Month\")\n",
        "            .reindex(allm)\n",
        "            .rename_axis(\"Month\")\n",
        "            .reset_index())\n",
        "    return df\n",
        "\n",
        "# ---------- run ----------\n",
        "urls = monthly_urls(YEAR)\n",
        "print(f\"{YEAR}: attempting {len(urls)} monthly files\")\n",
        "uk_counts, uk_tsum, uk_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "ec_counts, ec_tsum, ec_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "for url in urls:\n",
        "    got = stream_month(url)\n",
        "    if got is None:\n",
        "        print(\"SKIP (not found):\", url)\n",
        "        continue\n",
        "    z, inner = got\n",
        "    print(\"Processing:\", inner)\n",
        "    reader = pd.read_csv(\n",
        "        z.open(inner),\n",
        "        sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "        usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "        dtype={COL_SQLDATE:str, COL_A1C:str, COL_A2C:str, COL_ACTC:str, COL_TONE:float, COL_ROOT:str}\n",
        "    )\n",
        "    rows = 0\n",
        "    for i, chunk in enumerate(reader, 1):\n",
        "        rows += len(chunk)\n",
        "        add_chunk(chunk,   uk_counts, uk_tsum, uk_tn)\n",
        "        add_chunk_econ(chunk, ec_counts, ec_tsum, ec_tn)\n",
        "        if i % 5 == 0:\n",
        "            print(f\"  read ~{rows:,} rows…\")\n",
        "\n",
        "# finalize\n",
        "df_uk   = finalize(uk_counts, uk_tsum, uk_tn, YEAR)\n",
        "df_econ = finalize(ec_counts, ec_tsum, ec_tn, YEAR)\n",
        "\n",
        "# save\n",
        "out_uk   = OUT_DIR / f\"events_uk_monthly_{YEAR}.csv\"\n",
        "out_econ = OUT_DIR / f\"events_uk_economic_monthly_{YEAR}.csv\"\n",
        "df_uk.to_csv(out_uk, index=False, date_format=\"%Y-%m-%d\")\n",
        "df_econ.to_csv(out_econ, index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n",
        "print(\"UK-all monthly:\")\n",
        "print(df_uk.to_string(index=False))\n",
        "print(\"\\nUK-economic monthly (EventRootCode '04*'):\")\n",
        "print(df_econ.to_string(index=False))\n",
        "print(f\"\\nWrote:\\n  {out_uk}\\n  {out_econ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuMcnMGSqT9G",
        "outputId": "be335e25-4f74-4332-9f4f-a8f1c0db3b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2010: attempting 12 monthly files\n",
            "Processing: 201001.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201002.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201003.csv\n",
            "Processing: 201004.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 201005.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 201006.csv\n",
            "  read ~967,769 rows…\n",
            "Processing: 201007.csv\n",
            "  read ~1,000,000 rows…\n",
            "Processing: 201008.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201009.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "  read ~2,870,049 rows…\n",
            "Processing: 201010.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201011.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201012.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "\n",
            "--- Done ---\n",
            "UK-all monthly:\n",
            "     Month   Docs  AvgTone\n",
            "2010-01-01 145272 5.568387\n",
            "2010-02-01 116537 5.786885\n",
            "2010-03-01  35491 5.740809\n",
            "2010-04-01  62650 5.560080\n",
            "2010-05-01  48977 5.929850\n",
            "2010-06-01  43186 5.709821\n",
            "2010-07-01  66995 5.778580\n",
            "2010-08-01 118341 5.801101\n",
            "2010-09-01 138955 5.950679\n",
            "2010-10-01 110151 5.959748\n",
            "2010-11-01 136515 6.029133\n",
            "2010-12-01 148732 5.535400\n",
            "\n",
            "UK-economic monthly (EventRootCode '04*'):\n",
            "     Month  Docs  AvgTone\n",
            "2010-01-01 40168 5.732365\n",
            "2010-02-01 28770 5.945553\n",
            "2010-03-01  9038 5.978280\n",
            "2010-04-01 17387 5.582283\n",
            "2010-05-01 13015 6.126058\n",
            "2010-06-01 11449 5.913676\n",
            "2010-07-01 16942 6.049803\n",
            "2010-08-01 31830 5.904252\n",
            "2010-09-01 38582 6.037061\n",
            "2010-10-01 28094 6.076266\n",
            "2010-11-01 35380 6.309344\n",
            "2010-12-01 32865 5.796792\n",
            "\n",
            "Wrote:\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2010.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2010.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2011"
      ],
      "metadata": {
        "id": "nnQcWzYPwgDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import io, zipfile, requests, pandas as pd, numpy as np, math, re\n",
        "from pathlib import Path\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "YEAR = 2011\n",
        "BASE = \"http://data.gdeltproject.org/events\"   # index lists 200601.zip, 200602.zip, ... for 2006\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt_events_1p0\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UK_FIPS = \"UK\"    # ActionGeo_* uses FIPS-2\n",
        "UK_ISO3 = \"GBR\"   # Actor*CountryCode uses ISO-3\n",
        "CHUNK = 200_000\n",
        "\n",
        "# Column positions in Events 1.0 (57 cols total: 0..56)\n",
        "# 1=SQLDATE, 7=Actor1CountryCode, 17=Actor2CountryCode, 51=ActionGeo_CountryCode,\n",
        "# 34=AvgTone, 28=EventRootCode\n",
        "COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT = 1, 7, 17, 51, 34, 28\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-events-monthly/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def monthly_urls(year: int):\n",
        "    # 2006–2012 → 12 months; 2013 → months 1..3\n",
        "    last_m = 3 if year == 2013 else 12\n",
        "    return [f\"{BASE}/{year}{m:02d}.zip\" for m in range(1, last_m+1)]\n",
        "\n",
        "def stream_month(url: str):\n",
        "    \"\"\"Yield a (ZipFile, inner_csv_name) pair for a monthly ZIP, or None if 404.\"\"\"\n",
        "    r = session().get(url, timeout=180)\n",
        "    if not r.ok:\n",
        "        return None\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    inner = z.namelist()[0]  # e.g., '200601.csv'\n",
        "    return z, inner\n",
        "\n",
        "def add_chunk(df, counts, tone_sum, tone_n):\n",
        "    # UK if ActionGeo FIPS==UK OR Actor1/2 ISO3==GBR\n",
        "    m_uk = (df[COL_ACTC].eq(UK_FIPS)) | (df[COL_A1C].eq(UK_ISO3)) | (df[COL_A2C].eq(UK_ISO3))\n",
        "    if not m_uk.any():\n",
        "        return\n",
        "    sub = df.loc[m_uk, [COL_SQLDATE, COL_TONE]].copy()\n",
        "    sub[\"Month\"] = (\n",
        "        pd.to_datetime(sub[COL_SQLDATE], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "          .dt.to_period(\"M\")\n",
        "          .dt.to_timestamp()     # month start\n",
        "    )\n",
        "    sub[\"AvgTone\"] = pd.to_numeric(sub[COL_TONE], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[\"Month\"])\n",
        "    if sub.empty:\n",
        "        return\n",
        "\n",
        "    # Vectorized per-chunk aggregation\n",
        "    docs = sub.groupby(\"Month\", as_index=False).size().rename(columns={\"size\": \"Docs\"})\n",
        "    tsum = sub.dropna(subset=[\"AvgTone\"]).groupby(\"Month\", as_index=False)[\"AvgTone\"].sum()\\\n",
        "              .rename(columns={\"AvgTone\": \"ToneSum\"})\n",
        "    tnum = sub[\"AvgTone\"].notna().groupby(sub[\"Month\"]).sum().reset_index(name=\"ToneN\")\n",
        "    tmp = docs.merge(tsum, on=\"Month\", how=\"left\").merge(tnum, on=\"Month\", how=\"left\").fillna(0)\n",
        "\n",
        "    for _, r in tmp.iterrows():\n",
        "        m = r[\"Month\"]\n",
        "        counts[m]   += int(r[\"Docs\"])\n",
        "        tone_sum[m] += float(r[\"ToneSum\"])\n",
        "        tone_n[m]   += int(r[\"ToneN\"])\n",
        "\n",
        "def add_chunk_econ(df, counts, tone_sum, tone_n):\n",
        "    # Economic family: EventRootCode starts with '04'\n",
        "    m_econ = df[COL_ROOT].astype(str).str.startswith(\"04\", na=False)\n",
        "    if m_econ.any():\n",
        "        add_chunk(df.loc[m_econ, :], counts, tone_sum, tone_n)\n",
        "\n",
        "def finalize(counts, tone_sum, tone_n, year:int):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        if m.year != year:\n",
        "            continue\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m] / tone_n[m]) if tone_n[m] else np.nan\n",
        "        rows.append({\"Month\": m, \"Docs\": n, \"AvgTone\": avg})\n",
        "    # reindex to full year months so any missing appear explicitly\n",
        "    df = pd.DataFrame(rows).sort_values(\"Month\").reset_index(drop=True)\n",
        "    allm = pd.date_range(f\"{year}-01-01\", f\"{year}-12-01\" if year != 2013 else f\"{year}-03-01\", freq=\"MS\")\n",
        "    df = (df.set_index(\"Month\")\n",
        "            .reindex(allm)\n",
        "            .rename_axis(\"Month\")\n",
        "            .reset_index())\n",
        "    return df\n",
        "\n",
        "# ---------- run ----------\n",
        "urls = monthly_urls(YEAR)\n",
        "print(f\"{YEAR}: attempting {len(urls)} monthly files\")\n",
        "uk_counts, uk_tsum, uk_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "ec_counts, ec_tsum, ec_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "for url in urls:\n",
        "    got = stream_month(url)\n",
        "    if got is None:\n",
        "        print(\"SKIP (not found):\", url)\n",
        "        continue\n",
        "    z, inner = got\n",
        "    print(\"Processing:\", inner)\n",
        "    reader = pd.read_csv(\n",
        "        z.open(inner),\n",
        "        sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "        usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "        dtype={COL_SQLDATE:str, COL_A1C:str, COL_A2C:str, COL_ACTC:str, COL_TONE:float, COL_ROOT:str}\n",
        "    )\n",
        "    rows = 0\n",
        "    for i, chunk in enumerate(reader, 1):\n",
        "        rows += len(chunk)\n",
        "        add_chunk(chunk,   uk_counts, uk_tsum, uk_tn)\n",
        "        add_chunk_econ(chunk, ec_counts, ec_tsum, ec_tn)\n",
        "        if i % 5 == 0:\n",
        "            print(f\"  read ~{rows:,} rows…\")\n",
        "\n",
        "# finalize\n",
        "df_uk   = finalize(uk_counts, uk_tsum, uk_tn, YEAR)\n",
        "df_econ = finalize(ec_counts, ec_tsum, ec_tn, YEAR)\n",
        "\n",
        "# save\n",
        "out_uk   = OUT_DIR / f\"events_uk_monthly_{YEAR}.csv\"\n",
        "out_econ = OUT_DIR / f\"events_uk_economic_monthly_{YEAR}.csv\"\n",
        "df_uk.to_csv(out_uk, index=False, date_format=\"%Y-%m-%d\")\n",
        "df_econ.to_csv(out_econ, index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n",
        "print(\"UK-all monthly:\")\n",
        "print(df_uk.to_string(index=False))\n",
        "print(\"\\nUK-economic monthly (EventRootCode '04*'):\")\n",
        "print(df_econ.to_string(index=False))\n",
        "print(f\"\\nWrote:\\n  {out_uk}\\n  {out_econ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSNGzFXrwhOb",
        "outputId": "97b0b7f4-bbf3-424e-b96b-eb72fee08204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2011: attempting 12 monthly files\n",
            "Processing: 201101.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201102.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201103.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "  read ~3,000,000 rows…\n",
            "Processing: 201104.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "  read ~2,947,005 rows…\n",
            "Processing: 201105.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201106.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201107.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201108.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "  read ~2,854,375 rows…\n",
            "Processing: 201109.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "  read ~2,819,542 rows…\n",
            "Processing: 201110.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201111.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201112.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~1,881,592 rows…\n",
            "\n",
            "--- Done ---\n",
            "UK-all monthly:\n",
            "     Month   Docs  AvgTone\n",
            "2011-01-01 114382 5.868613\n",
            "2011-02-01 125672 5.734325\n",
            "2011-03-01 161123 5.823057\n",
            "2011-04-01 145731 6.103139\n",
            "2011-05-01 117843 6.005576\n",
            "2011-06-01 114844 5.886537\n",
            "2011-07-01 144327 5.526316\n",
            "2011-08-01 134605 5.457779\n",
            "2011-09-01 126927 5.847363\n",
            "2011-10-01 104376 5.829063\n",
            "2011-11-01 113509 5.813387\n",
            "2011-12-01  87374 5.647928\n",
            "\n",
            "UK-economic monthly (EventRootCode '04*'):\n",
            "     Month  Docs  AvgTone\n",
            "2011-01-01 29172 6.196800\n",
            "2011-02-01 31403 5.812310\n",
            "2011-03-01 41818 5.955905\n",
            "2011-04-01 38924 6.254299\n",
            "2011-05-01 32272 6.265659\n",
            "2011-06-01 28151 6.186302\n",
            "2011-07-01 35622 5.877829\n",
            "2011-08-01 30299 5.700682\n",
            "2011-09-01 31978 6.086146\n",
            "2011-10-01 25826 6.026894\n",
            "2011-11-01 29393 6.107396\n",
            "2011-12-01 20440 5.893213\n",
            "\n",
            "Wrote:\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2011.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2011.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2012"
      ],
      "metadata": {
        "id": "vLZD6WiVxgQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import io, zipfile, requests, pandas as pd, numpy as np, math, re\n",
        "from pathlib import Path\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "YEAR = 2012\n",
        "BASE = \"http://data.gdeltproject.org/events\"   # index lists 200601.zip, 200602.zip, ... for 2006\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt_events_1p0\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UK_FIPS = \"UK\"    # ActionGeo_* uses FIPS-2\n",
        "UK_ISO3 = \"GBR\"   # Actor*CountryCode uses ISO-3\n",
        "CHUNK = 200_000\n",
        "\n",
        "# Column positions in Events 1.0 (57 cols total: 0..56)\n",
        "# 1=SQLDATE, 7=Actor1CountryCode, 17=Actor2CountryCode, 51=ActionGeo_CountryCode,\n",
        "# 34=AvgTone, 28=EventRootCode\n",
        "COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT = 1, 7, 17, 51, 34, 28\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-events-monthly/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def monthly_urls(year: int):\n",
        "    # 2006–2012 → 12 months; 2013 → months 1..3\n",
        "    last_m = 3 if year == 2013 else 12\n",
        "    return [f\"{BASE}/{year}{m:02d}.zip\" for m in range(1, last_m+1)]\n",
        "\n",
        "def stream_month(url: str):\n",
        "    \"\"\"Yield a (ZipFile, inner_csv_name) pair for a monthly ZIP, or None if 404.\"\"\"\n",
        "    r = session().get(url, timeout=180)\n",
        "    if not r.ok:\n",
        "        return None\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    inner = z.namelist()[0]  # e.g., '200601.csv'\n",
        "    return z, inner\n",
        "\n",
        "def add_chunk(df, counts, tone_sum, tone_n):\n",
        "    # UK if ActionGeo FIPS==UK OR Actor1/2 ISO3==GBR\n",
        "    m_uk = (df[COL_ACTC].eq(UK_FIPS)) | (df[COL_A1C].eq(UK_ISO3)) | (df[COL_A2C].eq(UK_ISO3))\n",
        "    if not m_uk.any():\n",
        "        return\n",
        "    sub = df.loc[m_uk, [COL_SQLDATE, COL_TONE]].copy()\n",
        "    sub[\"Month\"] = (\n",
        "        pd.to_datetime(sub[COL_SQLDATE], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "          .dt.to_period(\"M\")\n",
        "          .dt.to_timestamp()     # month start\n",
        "    )\n",
        "    sub[\"AvgTone\"] = pd.to_numeric(sub[COL_TONE], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[\"Month\"])\n",
        "    if sub.empty:\n",
        "        return\n",
        "\n",
        "    # Vectorized per-chunk aggregation\n",
        "    docs = sub.groupby(\"Month\", as_index=False).size().rename(columns={\"size\": \"Docs\"})\n",
        "    tsum = sub.dropna(subset=[\"AvgTone\"]).groupby(\"Month\", as_index=False)[\"AvgTone\"].sum()\\\n",
        "              .rename(columns={\"AvgTone\": \"ToneSum\"})\n",
        "    tnum = sub[\"AvgTone\"].notna().groupby(sub[\"Month\"]).sum().reset_index(name=\"ToneN\")\n",
        "    tmp = docs.merge(tsum, on=\"Month\", how=\"left\").merge(tnum, on=\"Month\", how=\"left\").fillna(0)\n",
        "\n",
        "    for _, r in tmp.iterrows():\n",
        "        m = r[\"Month\"]\n",
        "        counts[m]   += int(r[\"Docs\"])\n",
        "        tone_sum[m] += float(r[\"ToneSum\"])\n",
        "        tone_n[m]   += int(r[\"ToneN\"])\n",
        "\n",
        "def add_chunk_econ(df, counts, tone_sum, tone_n):\n",
        "    # Economic family: EventRootCode starts with '04'\n",
        "    m_econ = df[COL_ROOT].astype(str).str.startswith(\"04\", na=False)\n",
        "    if m_econ.any():\n",
        "        add_chunk(df.loc[m_econ, :], counts, tone_sum, tone_n)\n",
        "\n",
        "def finalize(counts, tone_sum, tone_n, year:int):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        if m.year != year:\n",
        "            continue\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m] / tone_n[m]) if tone_n[m] else np.nan\n",
        "        rows.append({\"Month\": m, \"Docs\": n, \"AvgTone\": avg})\n",
        "    # reindex to full year months so any missing appear explicitly\n",
        "    df = pd.DataFrame(rows).sort_values(\"Month\").reset_index(drop=True)\n",
        "    allm = pd.date_range(f\"{year}-01-01\", f\"{year}-12-01\" if year != 2013 else f\"{year}-03-01\", freq=\"MS\")\n",
        "    df = (df.set_index(\"Month\")\n",
        "            .reindex(allm)\n",
        "            .rename_axis(\"Month\")\n",
        "            .reset_index())\n",
        "    return df\n",
        "\n",
        "# ---------- run ----------\n",
        "urls = monthly_urls(YEAR)\n",
        "print(f\"{YEAR}: attempting {len(urls)} monthly files\")\n",
        "uk_counts, uk_tsum, uk_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "ec_counts, ec_tsum, ec_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "for url in urls:\n",
        "    got = stream_month(url)\n",
        "    if got is None:\n",
        "        print(\"SKIP (not found):\", url)\n",
        "        continue\n",
        "    z, inner = got\n",
        "    print(\"Processing:\", inner)\n",
        "    reader = pd.read_csv(\n",
        "        z.open(inner),\n",
        "        sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "        usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "        dtype={COL_SQLDATE:str, COL_A1C:str, COL_A2C:str, COL_ACTC:str, COL_TONE:float, COL_ROOT:str}\n",
        "    )\n",
        "    rows = 0\n",
        "    for i, chunk in enumerate(reader, 1):\n",
        "        rows += len(chunk)\n",
        "        add_chunk(chunk,   uk_counts, uk_tsum, uk_tn)\n",
        "        add_chunk_econ(chunk, ec_counts, ec_tsum, ec_tn)\n",
        "        if i % 5 == 0:\n",
        "            print(f\"  read ~{rows:,} rows…\")\n",
        "\n",
        "# finalize\n",
        "df_uk   = finalize(uk_counts, uk_tsum, uk_tn, YEAR)\n",
        "df_econ = finalize(ec_counts, ec_tsum, ec_tn, YEAR)\n",
        "\n",
        "# save\n",
        "out_uk   = OUT_DIR / f\"events_uk_monthly_{YEAR}.csv\"\n",
        "out_econ = OUT_DIR / f\"events_uk_economic_monthly_{YEAR}.csv\"\n",
        "df_uk.to_csv(out_uk, index=False, date_format=\"%Y-%m-%d\")\n",
        "df_econ.to_csv(out_econ, index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n",
        "print(\"UK-all monthly:\")\n",
        "print(df_uk.to_string(index=False))\n",
        "print(\"\\nUK-economic monthly (EventRootCode '04*'):\")\n",
        "print(df_econ.to_string(index=False))\n",
        "print(f\"\\nWrote:\\n  {out_uk}\\n  {out_econ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmVAzGzhxhY5",
        "outputId": "e083ebef-2e39-4a41-c8bd-0d7cfcee78bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2012: attempting 12 monthly files\n",
            "Processing: 201201.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201202.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "  read ~3,000,000 rows…\n",
            "Processing: 201203.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "  read ~3,000,000 rows…\n",
            "Processing: 201204.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "  read ~3,000,000 rows…\n",
            "Processing: 201205.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "  read ~3,000,000 rows…\n",
            "Processing: 201206.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "  read ~2,824,395 rows…\n",
            "Processing: 201207.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201208.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201209.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201210.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201211.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing: 201212.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "\n",
            "--- Done ---\n",
            "UK-all monthly:\n",
            "     Month   Docs  AvgTone\n",
            "2012-01-01 120132 5.806493\n",
            "2012-02-01 163064 5.708025\n",
            "2012-03-01 156369 5.713234\n",
            "2012-04-01 145284 5.638722\n",
            "2012-05-01 149291 5.821468\n",
            "2012-06-01 144762 6.000214\n",
            "2012-07-01 132718 6.183330\n",
            "2012-08-01 125275 6.125585\n",
            "2012-09-01 123476 6.072576\n",
            "2012-10-01 121303 6.105137\n",
            "2012-11-01 123751 2.967310\n",
            "2012-12-01 107267 2.667022\n",
            "\n",
            "UK-economic monthly (EventRootCode '04*'):\n",
            "     Month  Docs  AvgTone\n",
            "2012-01-01 30652 5.969752\n",
            "2012-02-01 42063 5.961819\n",
            "2012-03-01 40837 6.002810\n",
            "2012-04-01 37448 5.871764\n",
            "2012-05-01 41408 6.056408\n",
            "2012-06-01 42814 6.140011\n",
            "2012-07-01 36333 6.332878\n",
            "2012-08-01 28950 6.418421\n",
            "2012-09-01 30464 6.208529\n",
            "2012-10-01 29704 6.272340\n",
            "2012-11-01 31564 3.098871\n",
            "2012-12-01 27364 2.775960\n",
            "\n",
            "Wrote:\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2012.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2012.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2013"
      ],
      "metadata": {
        "id": "SKesnqHtyrZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= GDELT 1.0 EVENTS → 2013 monthly UK AvgTone =======================\n",
        "# Saves to Google Drive: /content/drive/MyDrive/msc_project/gdelt_events_1p0\n",
        "# Outputs:\n",
        "#   - events_uk_monthly_2013.csv\n",
        "#   - events_uk_economic_monthly_2013.csv\n",
        "# Notes:\n",
        "#   • 2013 has monthly ZIPs for Jan–Mar: 201301.zip, 201302.zip, 201303.zip\n",
        "#   • and daily ZIPs from 2013-04-01: YYYYMMDD.export.CSV.zip\n",
        "\n",
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import io, zipfile, requests, pandas as pd, numpy as np, math\n",
        "from pathlib import Path\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "YEAR = 2013\n",
        "BASE = \"http://data.gdeltproject.org/events\"\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt_events_1p0\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UK_FIPS = \"UK\"    # ActionGeo_* country code (FIPS-2)\n",
        "UK_ISO3 = \"GBR\"   # Actor*CountryCode (ISO-3/CAMEO)\n",
        "CHUNK = 200_000   # rows per chunk\n",
        "\n",
        "# Events 1.0 column positions (57 cols total: 0..56)\n",
        "# 1=SQLDATE, 7=Actor1CountryCode, 17=Actor2CountryCode, 51=ActionGeo_CountryCode,\n",
        "# 34=AvgTone, 28=EventRootCode\n",
        "COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT = 1, 7, 17, 51, 34, 28\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-events-2013/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def stream_zip(url: str):\n",
        "    \"\"\"Return (ZipFile, inner_csv_name) for a URL or None if not found.\"\"\"\n",
        "    r = session().get(url, timeout=180)\n",
        "    if not r.ok:\n",
        "        return None\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    inner = z.namelist()[0]\n",
        "    return z, inner\n",
        "\n",
        "def add_chunk(df, counts, tone_sum, tone_n):\n",
        "    # UK if ActionGeo FIPS==UK OR Actor1/2 ISO3==GBR\n",
        "    m_uk = (df[COL_ACTC].eq(UK_FIPS)) | (df[COL_A1C].eq(UK_ISO3)) | (df[COL_A2C].eq(UK_ISO3))\n",
        "    if not m_uk.any():\n",
        "        return\n",
        "    sub = df.loc[m_uk, [COL_SQLDATE, COL_TONE]].copy()\n",
        "    sub[\"Month\"] = (\n",
        "        pd.to_datetime(sub[COL_SQLDATE], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "          .dt.to_period(\"M\")\n",
        "          .dt.to_timestamp()     # month start\n",
        "    )\n",
        "    sub[\"AvgTone\"] = pd.to_numeric(sub[COL_TONE], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[\"Month\"])\n",
        "    if sub.empty:\n",
        "        return\n",
        "\n",
        "    # Vectorized per-chunk aggregation\n",
        "    docs = sub.groupby(\"Month\", as_index=False).size().rename(columns={\"size\": \"Docs\"})\n",
        "    tsum = sub.dropna(subset=[\"AvgTone\"]).groupby(\"Month\", as_index=False)[\"AvgTone\"].sum()\\\n",
        "              .rename(columns={\"AvgTone\": \"ToneSum\"})\n",
        "    tnum = sub[\"AvgTone\"].notna().groupby(sub[\"Month\"]).sum().reset_index(name=\"ToneN\")\n",
        "    tmp = docs.merge(tsum, on=\"Month\", how=\"left\").merge(tnum, on=\"Month\", how=\"left\").fillna(0)\n",
        "\n",
        "    for _, r in tmp.iterrows():\n",
        "        m = r[\"Month\"]\n",
        "        counts[m]   += int(r[\"Docs\"])\n",
        "        tone_sum[m] += float(r[\"ToneSum\"])\n",
        "        tone_n[m]   += int(r[\"ToneN\"])\n",
        "\n",
        "def add_chunk_econ(df, counts, tone_sum, tone_n):\n",
        "    # Economic family: EventRootCode starts with '04'\n",
        "    m_econ = df[COL_ROOT].astype(str).str.startswith(\"04\", na=False)\n",
        "    if m_econ.any():\n",
        "        add_chunk(df.loc[m_econ, :], counts, tone_sum, tone_n)\n",
        "\n",
        "def finalize(counts, tone_sum, tone_n, year:int):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        if m.year != year:\n",
        "            continue\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m] / tone_n[m]) if tone_n[m] else np.nan\n",
        "        rows.append({\"Month\": m, \"Docs\": n, \"AvgTone\": avg})\n",
        "    # Reindex to full-year months (Jan–Dec)\n",
        "    df = pd.DataFrame(rows).sort_values(\"Month\").reset_index(drop=True)\n",
        "    allm = pd.date_range(f\"{year}-01-01\", f\"{year}-12-01\", freq=\"MS\")\n",
        "    df = (df.set_index(\"Month\")\n",
        "            .reindex(allm)\n",
        "            .rename_axis(\"Month\")\n",
        "            .reset_index())\n",
        "    return df\n",
        "\n",
        "# ---------- run ----------\n",
        "uk_counts, uk_tsum, uk_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "ec_counts, ec_tsum, ec_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "# 1) Monthly ZIPs for Jan–Mar 2013\n",
        "for m in [1,2,3]:\n",
        "    url = f\"{BASE}/{YEAR}{m:02d}.zip\"   # e.g., http://.../201301.zip\n",
        "    got = stream_zip(url)\n",
        "    if got is None:\n",
        "        print(\"SKIP (not found):\", url)\n",
        "        continue\n",
        "    z, inner = got\n",
        "    print(\"Processing monthly:\", inner)\n",
        "    reader = pd.read_csv(\n",
        "        z.open(inner),\n",
        "        sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "        usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "        dtype={COL_SQLDATE:str, COL_A1C:str, COL_A2C:str, COL_ACTC:str, COL_TONE:float, COL_ROOT:str}\n",
        "    )\n",
        "    rows = 0\n",
        "    for i, chunk in enumerate(reader, 1):\n",
        "        rows += len(chunk)\n",
        "        add_chunk(chunk, uk_counts, uk_tsum, uk_tn)\n",
        "        add_chunk_econ(chunk, ec_counts, ec_tsum, ec_tn)\n",
        "        if i % 5 == 0:\n",
        "            print(f\"  read ~{rows:,} rows…\")\n",
        "\n",
        "# 2) Daily ZIPs from 2013-04-01 to 2013-12-31 (YYYYMMDD.export.CSV.zip)\n",
        "dates = pd.date_range(\"2013-04-01\", \"2013-12-31\", freq=\"D\")\n",
        "for d in dates:\n",
        "    url = f\"{BASE}/{d:%Y%m%d}.export.CSV.zip\"\n",
        "    got = stream_zip(url)\n",
        "    if got is None:\n",
        "        # some days can be missing; skip quietly\n",
        "        continue\n",
        "    z, inner = got\n",
        "    # inner is usually like '20130401.export.CSV'\n",
        "    # print(\"Processing daily:\", inner)  # uncomment if you want verbose logs\n",
        "    reader = pd.read_csv(\n",
        "        z.open(inner),\n",
        "        sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "        usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "        dtype={COL_SQLDATE:str, COL_A1C:str, COL_A2C:str, COL_ACTC:str, COL_TONE:float, COL_ROOT:str}\n",
        "    )\n",
        "    for chunk in reader:\n",
        "        add_chunk(chunk, uk_counts, uk_tsum, uk_tn)\n",
        "        add_chunk_econ(chunk, ec_counts, ec_tsum, ec_tn)\n",
        "\n",
        "# finalize + save\n",
        "df_uk   = finalize(uk_counts, uk_tsum, uk_tn, YEAR)\n",
        "df_econ = finalize(ec_counts, ec_tsum, ec_tn, YEAR)\n",
        "\n",
        "out_uk   = OUT_DIR / f\"events_uk_monthly_{YEAR}.csv\"\n",
        "out_econ = OUT_DIR / f\"events_uk_economic_monthly_{YEAR}.csv\"\n",
        "df_uk.to_csv(out_uk, index=False, date_format=\"%Y-%m-%d\")\n",
        "df_econ.to_csv(out_econ, index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n",
        "print(\"UK-all monthly:\")\n",
        "print(df_uk.to_string(index=False))\n",
        "print(\"\\nUK-economic monthly (EventRootCode '04*'):\")\n",
        "print(df_econ.to_string(index=False))\n",
        "print(f\"\\nWrote:\\n  {out_uk}\\n  {out_econ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdBFO7Uoyqg1",
        "outputId": "58755c20-63a7-4a64-ab87-13db5f406c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processing monthly: 201301.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing monthly: 201302.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "Processing monthly: 201303.csv\n",
            "  read ~1,000,000 rows…\n",
            "  read ~2,000,000 rows…\n",
            "  read ~2,834,527 rows…\n",
            "\n",
            "--- Done ---\n",
            "UK-all monthly:\n",
            "     Month   Docs  AvgTone\n",
            "2013-01-01 126048 2.425202\n",
            "2013-02-01 116075 2.565489\n",
            "2013-03-01 125238 2.609551\n",
            "2013-04-01  68333 2.500042\n",
            "2013-05-01  80872 2.296863\n",
            "2013-06-01 131886 2.576806\n",
            "2013-07-01 185674 2.691628\n",
            "2013-08-01 218965 2.447774\n",
            "2013-09-01 217858 2.529328\n",
            "2013-10-01 215743 2.679382\n",
            "2013-11-01 220830 2.679942\n",
            "2013-12-01 160578 2.708217\n",
            "\n",
            "UK-economic monthly (EventRootCode '04*'):\n",
            "     Month  Docs  AvgTone\n",
            "2013-01-01 32178 2.481781\n",
            "2013-02-01 33896 2.601469\n",
            "2013-03-01 31667 2.749431\n",
            "2013-04-01 17039 2.496610\n",
            "2013-05-01 20432 2.407361\n",
            "2013-06-01 37077 2.733151\n",
            "2013-07-01 45337 2.831854\n",
            "2013-08-01 56660 2.543029\n",
            "2013-09-01 59406 2.626215\n",
            "2013-10-01 60936 2.790972\n",
            "2013-11-01 64295 2.758263\n",
            "2013-12-01 41081 2.824925\n",
            "\n",
            "Wrote:\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2013.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2013.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2014"
      ],
      "metadata": {
        "id": "IvbK2FSz2Pgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= GDELT 1.0 EVENTS → 2014 monthly UK AvgTone =======================\n",
        "# Saves to Google Drive: /content/drive/MyDrive/msc_project/gdelt_events_1p0\n",
        "# Outputs:\n",
        "#   - events_uk_monthly_2014.csv\n",
        "#   - events_uk_economic_monthly_2014.csv\n",
        "\n",
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import io, zipfile, requests, pandas as pd, numpy as np, math\n",
        "from pathlib import Path\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "YEAR = 2014\n",
        "BASE = \"http://data.gdeltproject.org/events\"  # daily files: YYYYMMDD.export.CSV.zip\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt_events_1p0\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UK_FIPS = \"UK\"    # ActionGeo_* country code (FIPS-2)\n",
        "UK_ISO3 = \"GBR\"   # Actor*CountryCode (ISO-3/CAMEO)\n",
        "CHUNK = 200_000   # rows per chunk\n",
        "\n",
        "# Events 1.0 column positions (57 cols total: 0..56)\n",
        "# 1=SQLDATE, 7=Actor1CountryCode, 17=Actor2CountryCode, 51=ActionGeo_CountryCode,\n",
        "# 34=AvgTone, 28=EventRootCode\n",
        "COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT = 1, 7, 17, 51, 34, 28\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-events-2014/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def stream_zip(url: str):\n",
        "    \"\"\"Return (ZipFile, inner_csv_name) for a URL or None if not found.\"\"\"\n",
        "    r = session().get(url, timeout=180)\n",
        "    if not r.ok:\n",
        "        return None\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    inner = z.namelist()[0]\n",
        "    return z, inner\n",
        "\n",
        "def add_chunk(df, counts, tone_sum, tone_n):\n",
        "    # UK if ActionGeo FIPS==UK OR Actor1/2 ISO3==GBR\n",
        "    m_uk = (df[COL_ACTC].eq(UK_FIPS)) | (df[COL_A1C].eq(UK_ISO3)) | (df[COL_A2C].eq(UK_ISO3))\n",
        "    if not m_uk.any():\n",
        "        return\n",
        "    sub = df.loc[m_uk, [COL_SQLDATE, COL_TONE]].copy()\n",
        "    sub[\"Month\"] = (\n",
        "        pd.to_datetime(sub[COL_SQLDATE], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "          .dt.to_period(\"M\")\n",
        "          .dt.to_timestamp()     # month start\n",
        "    )\n",
        "    sub[\"AvgTone\"] = pd.to_numeric(sub[COL_TONE], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[\"Month\"])\n",
        "    if sub.empty:\n",
        "        return\n",
        "\n",
        "    # Vectorized per-chunk aggregation\n",
        "    docs = sub.groupby(\"Month\", as_index=False).size().rename(columns={\"size\": \"Docs\"})\n",
        "    tsum = sub.dropna(subset=[\"AvgTone\"]).groupby(\"Month\", as_index=False)[\"AvgTone\"].sum()\\\n",
        "              .rename(columns={\"AvgTone\": \"ToneSum\"})\n",
        "    tnum = sub[\"AvgTone\"].notna().groupby(sub[\"Month\"]).sum().reset_index(name=\"ToneN\")\n",
        "    tmp = docs.merge(tsum, on=\"Month\", how=\"left\").merge(tnum, on=\"Month\", how=\"left\").fillna(0)\n",
        "\n",
        "    for _, r in tmp.iterrows():\n",
        "        m = r[\"Month\"]\n",
        "        counts[m]   += int(r[\"Docs\"])\n",
        "        tone_sum[m] += float(r[\"ToneSum\"])\n",
        "        tone_n[m]   += int(r[\"ToneN\"])\n",
        "\n",
        "def add_chunk_econ(df, counts, tone_sum, tone_n):\n",
        "    # Economic family: EventRootCode starts with '04'\n",
        "    m_econ = df[COL_ROOT].astype(str).str.startswith(\"04\", na=False)\n",
        "    if m_econ.any():\n",
        "        add_chunk(df.loc[m_econ, :], counts, tone_sum, tone_n)\n",
        "\n",
        "def finalize(counts, tone_sum, tone_n, year:int):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        if m.year != year:\n",
        "            continue\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m] / tone_n[m]) if tone_n[m] else np.nan\n",
        "        rows.append({\"Month\": m, \"Docs\": n, \"AvgTone\": avg})\n",
        "    # Reindex to full-year months (Jan–Dec)\n",
        "    df = pd.DataFrame(rows).sort_values(\"Month\").reset_index(drop=True)\n",
        "    allm = pd.date_range(f\"{year}-01-01\", f\"{year}-12-01\", freq=\"MS\")\n",
        "    df = (df.set_index(\"Month\")\n",
        "            .reindex(allm)\n",
        "            .rename_axis(\"Month\")\n",
        "            .reset_index())\n",
        "    return df\n",
        "\n",
        "# ---------- run ----------\n",
        "uk_counts, uk_tsum, uk_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "ec_counts, ec_tsum, ec_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "dates = pd.date_range(\"2014-01-01\", \"2014-12-31\", freq=\"D\")\n",
        "miss = 0\n",
        "for j, d in enumerate(dates, 1):\n",
        "    url = f\"{BASE}/{d:%Y%m%d}.export.CSV.zip\"\n",
        "    got = stream_zip(url)\n",
        "    if got is None:\n",
        "        miss += 1\n",
        "        if j % 30 == 0:\n",
        "            print(f\"{j}/{len(dates)} days, missing so far: {miss}\")\n",
        "        continue\n",
        "    z, inner = got\n",
        "    # print(\"Processing:\", inner)  # uncomment for verbose logs\n",
        "    reader = pd.read_csv(\n",
        "        z.open(inner),\n",
        "        sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "        usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "        dtype={COL_SQLDATE:str, COL_A1C:str, COL_A2C:str, COL_ACTC:str, COL_TONE:float, COL_ROOT:str}\n",
        "    )\n",
        "    for chunk in reader:\n",
        "        add_chunk(chunk, uk_counts, uk_tsum, uk_tn)\n",
        "        add_chunk_econ(chunk, ec_counts, ec_tsum, ec_tn)\n",
        "    if j % 30 == 0:\n",
        "        print(f\"Processed {j}/{len(dates)} days… (missing so far: {miss})\")\n",
        "\n",
        "df_uk   = finalize(uk_counts, uk_tsum, uk_tn, YEAR)\n",
        "df_econ = finalize(ec_counts, ec_tsum, ec_tn, YEAR)\n",
        "\n",
        "out_uk   = OUT_DIR / f\"events_uk_monthly_{YEAR}.csv\"\n",
        "out_econ = OUT_DIR / f\"events_uk_economic_monthly_{YEAR}.csv\"\n",
        "df_uk.to_csv(out_uk, index=False, date_format=\"%Y-%m-%d\")\n",
        "df_econ.to_csv(out_econ, index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n",
        "print(\"UK-all monthly:\")\n",
        "print(df_uk.to_string(index=False))\n",
        "print(\"\\nUK-economic monthly (EventRootCode '04*'):\")\n",
        "print(df_econ.to_string(index=False))\n",
        "print(f\"\\nWrote:\\n  {out_uk}\\n  {out_econ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1Ln5mbx2Qd_",
        "outputId": "8585efa8-1405-4004-df97-6908423a8360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processed 30/365 days… (missing so far: 3)\n",
            "Processed 60/365 days… (missing so far: 3)\n",
            "Processed 90/365 days… (missing so far: 4)\n",
            "Processed 120/365 days… (missing so far: 4)\n",
            "Processed 150/365 days… (missing so far: 4)\n",
            "Processed 180/365 days… (missing so far: 4)\n",
            "Processed 210/365 days… (missing so far: 4)\n",
            "Processed 240/365 days… (missing so far: 4)\n",
            "Processed 270/365 days… (missing so far: 4)\n",
            "Processed 300/365 days… (missing so far: 4)\n",
            "Processed 330/365 days… (missing so far: 4)\n",
            "Processed 360/365 days… (missing so far: 4)\n",
            "\n",
            "--- Done ---\n",
            "UK-all monthly:\n",
            "     Month   Docs  AvgTone\n",
            "2014-01-01 139963 2.678215\n",
            "2014-02-01 166052 2.698572\n",
            "2014-03-01 178579 2.651045\n",
            "2014-04-01 184551 2.720858\n",
            "2014-05-01 195643 2.677188\n",
            "2014-06-01 191109 2.667295\n",
            "2014-07-01 214956 2.612385\n",
            "2014-08-01 205392 2.564620\n",
            "2014-09-01 278268 2.626609\n",
            "2014-10-01 265410 2.680726\n",
            "2014-11-01 243234 2.656559\n",
            "2014-12-01 198960 2.724392\n",
            "\n",
            "UK-economic monthly (EventRootCode '04*'):\n",
            "     Month  Docs  AvgTone\n",
            "2014-01-01 35665 2.769333\n",
            "2014-02-01 43668 2.859537\n",
            "2014-03-01 49400 2.715000\n",
            "2014-04-01 51001 2.881430\n",
            "2014-05-01 49443 2.832798\n",
            "2014-06-01 52893 2.755171\n",
            "2014-07-01 58283 2.700179\n",
            "2014-08-01 54274 2.678771\n",
            "2014-09-01 75431 2.690719\n",
            "2014-10-01 71706 2.814015\n",
            "2014-11-01 64486 2.790142\n",
            "2014-12-01 53564 2.859509\n",
            "\n",
            "Wrote:\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2014.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2014.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jan 2015"
      ],
      "metadata": {
        "id": "FjApmzaY5fQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= GDELT 1.0 EVENTS → Jan 2015 (monthly UK AvgTone) =======================\n",
        "# Saves to Google Drive: /content/drive/MyDrive/msc_project/gdelt_events_1p0\n",
        "# Outputs:\n",
        "#   - events_uk_monthly_2015_Jan.csv\n",
        "#   - events_uk_economic_monthly_2015_Jan.csv\n",
        "# Notes: GDELT 2.0 starts Feb 19, 2015. Jan 2015 is from GDELT 1.0 daily files: YYYYMMDD.export.CSV.zip\n",
        "\n",
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import io, zipfile, requests, pandas as pd, numpy as np, math\n",
        "from pathlib import Path\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "START_DATE = \"2015-01-01\"          # you can extend to \"2015-02-18\" if you also want early Feb 2015\n",
        "END_DATE   = \"2015-01-31\"\n",
        "BASE = \"http://data.gdeltproject.org/events\"   # daily: YYYYMMDD.export.CSV.zip\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt_events_1p0\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "UK_FIPS = \"UK\"    # ActionGeo_* country code (FIPS-2)\n",
        "UK_ISO3 = \"GBR\"   # Actor*CountryCode (ISO-3/CAMEO)\n",
        "CHUNK = 200_000   # rows per chunk\n",
        "\n",
        "# Events 1.0 column positions (57 cols total: 0..56)\n",
        "# 1=SQLDATE, 7=Actor1CountryCode, 17=Actor2CountryCode, 51=ActionGeo_CountryCode,\n",
        "# 34=AvgTone, 28=EventRootCode\n",
        "COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT = 1, 7, 17, 51, 34, 28\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-events-2015-Jan/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def stream_zip(url: str):\n",
        "    \"\"\"Return (ZipFile, inner_csv_name) if available; else None.\"\"\"\n",
        "    r = session().get(url, timeout=180)\n",
        "    if not r.ok:\n",
        "        return None\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    inner = z.namelist()[0]\n",
        "    return z, inner\n",
        "\n",
        "def add_chunk(df, counts, tone_sum, tone_n):\n",
        "    \"\"\"Aggregate UK-related rows into monthly Docs and AvgTone sums.\"\"\"\n",
        "    # UK if ActionGeo FIPS==UK OR Actor1/2 ISO3==GBR\n",
        "    m_uk = (df[COL_ACTC].eq(UK_FIPS)) | (df[COL_A1C].eq(UK_ISO3)) | (df[COL_A2C].eq(UK_ISO3))\n",
        "    if not m_uk.any():\n",
        "        return\n",
        "    sub = df.loc[m_uk, [COL_SQLDATE, COL_TONE]].copy()\n",
        "    sub[\"Month\"] = (\n",
        "        pd.to_datetime(sub[COL_SQLDATE], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "          .dt.to_period(\"M\")\n",
        "          .dt.to_timestamp()    # month start\n",
        "    )\n",
        "    sub[\"AvgTone\"] = pd.to_numeric(sub[COL_TONE], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[\"Month\"])\n",
        "    if sub.empty:\n",
        "        return\n",
        "\n",
        "    # Per-chunk aggregation (vectorized)\n",
        "    docs = sub.groupby(\"Month\", as_index=False).size().rename(columns={\"size\": \"Docs\"})\n",
        "    tsum = sub.dropna(subset=[\"AvgTone\"]).groupby(\"Month\", as_index=False)[\"AvgTone\"].sum()\\\n",
        "              .rename(columns={\"AvgTone\": \"ToneSum\"})\n",
        "    tnum = sub[\"AvgTone\"].notna().groupby(sub[\"Month\"]).sum().reset_index(name=\"ToneN\")\n",
        "    tmp = docs.merge(tsum, on=\"Month\", how=\"left\").merge(tnum, on=\"Month\", how=\"left\").fillna(0)\n",
        "\n",
        "    for _, r in tmp.iterrows():\n",
        "        m = r[\"Month\"]\n",
        "        counts[m]   += int(r[\"Docs\"])\n",
        "        tone_sum[m] += float(r[\"ToneSum\"])\n",
        "        tone_n[m]   += int(r[\"ToneN\"])\n",
        "\n",
        "def add_chunk_econ(df, counts, tone_sum, tone_n):\n",
        "    \"\"\"Economic family only: EventRootCode starts with '04'.\"\"\"\n",
        "    m_econ = df[COL_ROOT].astype(str).str.startswith(\"04\", na=False)\n",
        "    if m_econ.any():\n",
        "        add_chunk(df.loc[m_econ, :], counts, tone_sum, tone_n)\n",
        "\n",
        "def finalize(counts, tone_sum, tone_n, start_date: str, end_date: str):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        if m < pd.to_datetime(start_date).to_period(\"M\").to_timestamp() or \\\n",
        "           m > pd.to_datetime(end_date).to_period(\"M\").to_timestamp():\n",
        "            continue\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m] / tone_n[m]) if tone_n[m] else np.nan\n",
        "        rows.append({\"Month\": m, \"Docs\": n, \"AvgTone\": avg})\n",
        "    df = pd.DataFrame(rows).sort_values(\"Month\").reset_index(drop=True)\n",
        "\n",
        "    # Reindex to month(s) fully covered by the date window (here, just Jan 2015)\n",
        "    mstart = pd.to_datetime(start_date).to_period(\"M\").to_timestamp()\n",
        "    mend   = pd.to_datetime(end_date).to_period(\"M\").to_timestamp()\n",
        "    allm = pd.date_range(mstart, mend, freq=\"MS\")\n",
        "    df = (df.set_index(\"Month\")\n",
        "            .reindex(allm)\n",
        "            .rename_axis(\"Month\")\n",
        "            .reset_index())\n",
        "    return df\n",
        "\n",
        "# ---------- run ----------\n",
        "dates = pd.date_range(START_DATE, END_DATE, freq=\"D\")\n",
        "uk_counts, uk_tsum, uk_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "ec_counts, ec_tsum, ec_tn = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "missing = 0\n",
        "for j, d in enumerate(dates, 1):\n",
        "    url = f\"{BASE}/{d:%Y%m%d}.export.CSV.zip\"\n",
        "    got = stream_zip(url)\n",
        "    if got is None:\n",
        "        missing += 1\n",
        "        if j % 10 == 0:\n",
        "            print(f\"{j}/{len(dates)} days, missing so far: {missing}\")\n",
        "        continue\n",
        "    z, inner = got\n",
        "    # print(\"Processing:\", inner)  # uncomment for verbose logs\n",
        "\n",
        "    reader = pd.read_csv(\n",
        "        z.open(inner),\n",
        "        sep=\"\\t\", header=None, low_memory=False, chunksize=CHUNK,\n",
        "        usecols=[COL_SQLDATE, COL_A1C, COL_A2C, COL_ACTC, COL_TONE, COL_ROOT],\n",
        "        dtype={COL_SQLDATE:str, COL_A1C:str, COL_A2C:str, COL_ACTC:str, COL_TONE:float, COL_ROOT:str}\n",
        "    )\n",
        "    for chunk in reader:\n",
        "        add_chunk(chunk, uk_counts, uk_tsum, uk_tn)\n",
        "        add_chunk_econ(chunk, ec_counts, ec_tsum, ec_tn)\n",
        "    if j % 10 == 0:\n",
        "        print(f\"Processed {j}/{len(dates)} days… (missing so far: {missing})\")\n",
        "\n",
        "df_uk   = finalize(uk_counts, uk_tsum, uk_tn, START_DATE, END_DATE)\n",
        "df_econ = finalize(ec_counts, ec_tsum, ec_tn, START_DATE, END_DATE)\n",
        "\n",
        "out_suffix = \"2015_Jan\"\n",
        "df_uk.to_csv(OUT_DIR / f\"events_uk_monthly_{out_suffix}.csv\", index=False, date_format=\"%Y-%m-%d\")\n",
        "df_econ.to_csv(OUT_DIR / f\"events_uk_economic_monthly_{out_suffix}.csv\", index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n",
        "print(\"UK-all monthly:\")\n",
        "print(df_uk.to_string(index=False))\n",
        "print(\"\\nUK-economic monthly (EventRootCode '04*'):\")\n",
        "print(df_econ.to_string(index=False))\n",
        "print(\"\\nWrote:\")\n",
        "print(\" \", OUT_DIR / f\"events_uk_monthly_{out_suffix}.csv\")\n",
        "print(\" \", OUT_DIR / f\"events_uk_economic_monthly_{out_suffix}.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm7sAf8v5guQ",
        "outputId": "210cf70c-5ba9-4342-afcf-2e1726f24932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processed 10/31 days… (missing so far: 0)\n",
            "Processed 20/31 days… (missing so far: 0)\n",
            "Processed 30/31 days… (missing so far: 0)\n",
            "\n",
            "--- Done ---\n",
            "UK-all monthly:\n",
            "     Month   Docs  AvgTone\n",
            "2015-01-01 219962 2.650666\n",
            "\n",
            "UK-economic monthly (EventRootCode '04*'):\n",
            "     Month  Docs  AvgTone\n",
            "2015-01-01 60775 2.742955\n",
            "\n",
            "Wrote:\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2015_Jan.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2015_Jan.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging 2005 to Jan 2015"
      ],
      "metadata": {
        "id": "Ih8PoVVM78to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== Merge GDELT Events 1.0 monthly outputs (2005 → Jan 2015) =====================\n",
        "# Input folder (what you used earlier)\n",
        "BASE_DIR = \"/content/drive/MyDrive/msc_project/gdelt_events_1p0\"\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(BASE_DIR)\n",
        "\n",
        "def load_concat(pattern, label=None):\n",
        "    \"\"\"Load all CSVs matching pattern, normalize columns, concat, sort, dedupe by Month.\"\"\"\n",
        "    files = sorted([p for p in BASE.glob(pattern)])\n",
        "    if not files:\n",
        "        print(f\"No files found for pattern: {pattern}\")\n",
        "        return pd.DataFrame(columns=[\"Month\",\"Docs\",\"AvgTone\"])\n",
        "    frames = []\n",
        "    for p in files:\n",
        "        df = pd.read_csv(p)\n",
        "        # Normalize column names\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        # try both 'Month' and 'month'\n",
        "        if \"Month\" not in df.columns and \"month\" in cols:\n",
        "            df.rename(columns={\"month\":\"Month\"}, inplace=True)\n",
        "        if \"Docs\" not in df.columns and \"docs\" in cols:\n",
        "            df.rename(columns={\"docs\":\"Docs\"}, inplace=True)\n",
        "        if \"AvgTone\" not in df.columns and \"avg_tone\" in cols:\n",
        "            df.rename(columns={\"avg_tone\":\"AvgTone\"}, inplace=True)\n",
        "        # Parse types\n",
        "        df[\"Month\"] = pd.to_datetime(df[\"Month\"], errors=\"coerce\")\n",
        "        df[\"Docs\"] = pd.to_numeric(df[\"Docs\"], errors=\"coerce\")\n",
        "        df[\"AvgTone\"] = pd.to_numeric(df[\"AvgTone\"], errors=\"coerce\")\n",
        "        frames.append(df[[\"Month\",\"Docs\",\"AvgTone\"]])\n",
        "    out = pd.concat(frames, ignore_index=True).sort_values(\"Month\")\n",
        "    # De-duplicate months (keep last if the same month appears in multiple files)\n",
        "    out = out.drop_duplicates(subset=\"Month\", keep=\"last\").reset_index(drop=True)\n",
        "    # Clip to 2005-01 to 2015-01 (Jan-2015 only)\n",
        "    start = pd.Timestamp(\"2005-01-01\")\n",
        "    end   = pd.Timestamp(\"2015-01-01\")  # adjust to 2015-02-01 if you included early Feb 2015\n",
        "    out = out[(out[\"Month\"] >= start) & (out[\"Month\"] <= end)].copy()\n",
        "    out.sort_values(\"Month\", inplace=True)\n",
        "    out.reset_index(drop=True, inplace=True)\n",
        "    print(f\"{label or pattern}: merged {len(files)} files → {len(out)} rows\")\n",
        "    return out\n",
        "\n",
        "# 1) Merge the All-UK series (exclude 'economic' files)\n",
        "all_df  = load_concat(\"events_uk_monthly_*.csv\", label=\"All-UK\")\n",
        "# 2) Merge the Economic-only series\n",
        "econ_df = load_concat(\"events_uk_economic_monthly_*.csv\", label=\"Economic-only\")\n",
        "\n",
        "# 3) Save each separately (optional but handy)\n",
        "out_all  = BASE / \"events_uk_monthly_2005_to_2015Jan.csv\"\n",
        "out_econ = BASE / \"events_uk_economic_monthly_2005_to_2015Jan.csv\"\n",
        "all_df.to_csv(out_all, index=False, date_format=\"%Y-%m-%d\")\n",
        "econ_df.to_csv(out_econ, index=False, date_format=\"%Y-%m-%d\")\n",
        "print(\"Wrote:\", out_all)\n",
        "print(\"Wrote:\", out_econ)\n",
        "\n",
        "# 4) Also build ONE wide file with both series side-by-side\n",
        "wide = (\n",
        "    all_df.rename(columns={\"Docs\":\"Docs_All\",\"AvgTone\":\"AvgTone_All\"})\n",
        "      .merge(\n",
        "        econ_df.rename(columns={\"Docs\":\"Docs_Econ\",\"AvgTone\":\"AvgTone_Econ\"}),\n",
        "        on=\"Month\", how=\"outer\"\n",
        "      )\n",
        "      .sort_values(\"Month\")\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "out_wide = BASE / \"events_uk_monthly_2005_to_2015Jan_WIDE.csv\"\n",
        "wide.to_csv(out_wide, index=False, date_format=\"%Y-%m-%d\")\n",
        "print(\"Wrote:\", out_wide)\n",
        "\n",
        "# Quick peek\n",
        "print(\"\\nPreview (last 6 rows):\")\n",
        "print(wide.tail(6).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "hs4dxKal7_Nd",
        "outputId": "d2a11d97-f818-438c-8676-30393916d6d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All-UK: merged 11 files → 121 rows\n",
            "Economic-only: merged 11 files → 121 rows\n",
            "Wrote: /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2005_to_2015Jan.csv\n",
            "Wrote: /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2005_to_2015Jan.csv\n",
            "Wrote: /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2005_to_2015Jan_WIDE.csv\n",
            "\n",
            "Preview (last 6 rows):\n",
            "     Month  Docs_All  AvgTone_All  Docs_Econ  AvgTone_Econ\n",
            "2014-08-01    205392     2.564620      54274      2.678771\n",
            "2014-09-01    278268     2.626609      75431      2.690719\n",
            "2014-10-01    265410     2.680726      71706      2.814015\n",
            "2014-11-01    243234     2.656559      64486      2.790142\n",
            "2014-12-01    198960     2.724392      53564      2.859509\n",
            "2015-01-01    219962     2.650666      60775      2.742955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "merge"
      ],
      "metadata": {
        "id": "Lxublirs9iGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Merge GDELT 2005–Jan 2015 (Events 1.0) with Feb 2015–2025 (GKG 2.0) ==================\n",
        "# Inputs (adjust paths if yours differ):\n",
        "#   /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2005_to_2015Jan.csv\n",
        "#   /content/drive/MyDrive/msc_project/gdelt/gdelt_uk_housing_monthly_STITCHING.csv\n",
        "# Outputs:\n",
        "#   /content/drive/MyDrive/msc_project/gdelt/gdelt_tone_2005_2025_wide.csv\n",
        "#   /content/drive/MyDrive/msc_project/gdelt/gdelt_tone_2005_2025_stitched.csv\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive (no-op if already mounted)\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ---- paths ----\n",
        "EVENTS_PATH = \"/content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_monthly_2005_to_2015Jan.csv\"\n",
        "GKG_MASTER  = \"/content/drive/MyDrive/msc_project/gdelt/gdelt_uk_housing_monthly_STITCHING.csv\"\n",
        "OUT_DIR     = Path(\"/content/drive/MyDrive/msc_project/gdelt\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- load Events v1.0 merged (2005–2015-01) ----\n",
        "ev = pd.read_csv(EVENTS_PATH, parse_dates=[\"Month\"])\n",
        "ev = ev.sort_values(\"Month\").reset_index(drop=True)\n",
        "\n",
        "# ---- load GKG master (2015-02 onward) ----\n",
        "gkg = pd.read_csv(GKG_MASTER)\n",
        "# normalize columns\n",
        "if \"month\" in gkg.columns:\n",
        "    gkg.rename(columns={\"month\":\"Month\"}, inplace=True)\n",
        "if \"docs\" in gkg.columns:\n",
        "    gkg.rename(columns={\"docs\":\"Docs\"}, inplace=True)\n",
        "if \"avg_tone\" in gkg.columns:\n",
        "    gkg.rename(columns={\"avg_tone\":\"AvgTone\"}, inplace=True)\n",
        "\n",
        "# parse + clean types\n",
        "gkg[\"Month\"]   = pd.to_datetime(gkg[\"Month\"], errors=\"coerce\")\n",
        "gkg[\"Docs\"]    = pd.to_numeric(gkg[\"Docs\"], errors=\"coerce\")\n",
        "gkg[\"AvgTone\"] = pd.to_numeric(gkg[\"AvgTone\"], errors=\"coerce\")\n",
        "\n",
        "# ---- clip windows ----\n",
        "start = pd.Timestamp(\"2005-01-01\")\n",
        "cutover_last_events = pd.Timestamp(\"2015-01-01\")  # keep Events through Jan-2015\n",
        "cutover_first_gkg   = pd.Timestamp(\"2015-02-01\")  # use GKG from Feb-2015 onward\n",
        "end   = pd.Timestamp(\"2025-06-01\")                # set your project end\n",
        "\n",
        "ev = ev[(ev[\"Month\"] >= start) & (ev[\"Month\"] <= cutover_last_events)].copy()\n",
        "gkg = gkg[(gkg[\"Month\"] >= cutover_first_gkg) & (gkg[\"Month\"] <= end)].copy()\n",
        "\n",
        "# ---- make a complete monthly index and merge wide ----\n",
        "full_idx = pd.date_range(start, end, freq=\"MS\")\n",
        "ev_w  = ev.set_index(\"Month\").rename(columns={\"Docs\":\"Docs_Events\", \"AvgTone\":\"AvgTone_Events\"})\n",
        "gkg_w = gkg.set_index(\"Month\").rename(columns={\"Docs\":\"Docs_GKG\",   \"AvgTone\":\"AvgTone_GKG\"})\n",
        "\n",
        "wide = (\n",
        "    pd.DataFrame(index=full_idx)\n",
        "      .join(ev_w[[\"Docs_Events\",\"AvgTone_Events\"]], how=\"left\")\n",
        "      .join(gkg_w[[\"Docs_GKG\",\"AvgTone_GKG\"]],      how=\"left\")\n",
        "      .rename_axis(\"Month\")\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# ---- build a stitched single series (Events until 2015-01, then GKG) ----\n",
        "wide[\"AvgTone_Stitched\"] = np.where(\n",
        "    wide[\"Month\"] <= cutover_last_events, wide[\"AvgTone_Events\"], wide[\"AvgTone_GKG\"]\n",
        ")\n",
        "wide[\"Docs_Stitched\"] = np.where(\n",
        "    wide[\"Month\"] <= cutover_last_events, wide[\"Docs_Events\"], wide[\"Docs_GKG\"]\n",
        ")\n",
        "wide[\"Source\"] = np.where(\n",
        "    wide[\"Month\"] <= cutover_last_events, \"Events_v1_0\", \"GKG_v2_0_Housing\"\n",
        ")\n",
        "\n",
        "# ---- save outputs ----\n",
        "out_wide = OUT_DIR / \"gdelt_tone_2005_2025_wide.csv\"\n",
        "out_st   = OUT_DIR / \"gdelt_tone_2005_2025_stitched.csv\"\n",
        "\n",
        "wide.to_csv(out_wide, index=False, date_format=\"%Y-%m-%d\")\n",
        "\n",
        "wide[[\"Month\",\"AvgTone_Stitched\",\"Docs_Stitched\",\"Source\"]].to_csv(\n",
        "    out_st, index=False, date_format=\"%Y-%m-%d\"\n",
        ")\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" \", out_wide)\n",
        "print(\" \", out_st)\n",
        "\n",
        "# quick peek\n",
        "print(\"\\nPreview (tail):\")\n",
        "print(wide.tail(6).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuB8ZcbN9ksF",
        "outputId": "2fc63c5d-db07-48a5-eb31-4db1767c31bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Saved:\n",
            "  /content/drive/MyDrive/msc_project/gdelt/gdelt_tone_2005_2025_wide.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt/gdelt_tone_2005_2025_stitched.csv\n",
            "\n",
            "Preview (tail):\n",
            "     Month  Docs_Events  AvgTone_Events  Docs_GKG  AvgTone_GKG  AvgTone_Stitched  Docs_Stitched           Source\n",
            "2025-01-01          NaN             NaN   14180.0    -1.096796         -1.096796        14180.0 GKG_v2_0_Housing\n",
            "2025-02-01          NaN             NaN   16683.0    -0.884768         -0.884768        16683.0 GKG_v2_0_Housing\n",
            "2025-03-01          NaN             NaN   17850.0    -0.767261         -0.767261        17850.0 GKG_v2_0_Housing\n",
            "2025-04-01          NaN             NaN   14902.0    -1.109188         -1.109188        14902.0 GKG_v2_0_Housing\n",
            "2025-05-01          NaN             NaN   18253.0    -0.360499         -0.360499        18253.0 GKG_v2_0_Housing\n",
            "2025-06-01          NaN             NaN    8278.0    -0.826562         -0.826562         8278.0 GKG_v2_0_Housing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gMjmTdj508-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Merge GDELT economic-only (2005–Jan 2015) with GKG housing (Feb 2015–Jun 2025) ==================\n",
        "# Inputs (change paths if yours differ):\n",
        "#   /content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2005_to_2015Jan.csv\n",
        "#   /content/drive/MyDrive/msc_project/gdelt/gdelt_uk_housing_monthly_STITCHING.csv\n",
        "# Outputs:\n",
        "#   /content/drive/MyDrive/msc_project/gdelt/gdelt_tone_2005_2025_wide_ECONxGKG.csv\n",
        "#   /content/drive/MyDrive/msc_project/gdelt/gdelt_tone_2005_2025_stitched_ECONxGKG.csv\n",
        "\n",
        "!pip -q install pandas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ---------- paths ----------\n",
        "EVENTS_PATH = \"/content/drive/MyDrive/msc_project/gdelt_events_1p0/events_uk_economic_monthly_2005_to_2015Jan.csv\"  # ECON ONLY\n",
        "GKG_MASTER  = \"/content/drive/MyDrive/msc_project/gdelt/gdelt_uk_housing_monthly_STITCHING.csv\"                      # your GKG (housing) master\n",
        "OUT_DIR     = Path(\"/content/drive/MyDrive/msc_project/gdelt\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- window ----------\n",
        "start = pd.Timestamp(\"2005-01-01\")\n",
        "cutover_last_events = pd.Timestamp(\"2015-01-01\")  # use ECON-only events up to Jan 2015\n",
        "cutover_first_gkg   = pd.Timestamp(\"2015-02-01\")  # use GKG housing from Feb 2015\n",
        "end   = pd.Timestamp(\"2025-06-01\")                # adjust if needed\n",
        "\n",
        "# ---------- load ECON-only events ----------\n",
        "ev = pd.read_csv(EVENTS_PATH)\n",
        "\n",
        "# normalize columns just in case\n",
        "rename_map = {}\n",
        "if \"month\" in ev.columns: rename_map[\"month\"] = \"Month\"\n",
        "if \"docs\"  in ev.columns: rename_map[\"docs\"]  = \"Docs\"\n",
        "if \"avg_tone\" in ev.columns: rename_map[\"avg_tone\"] = \"AvgTone\"\n",
        "if rename_map: ev = ev.rename(columns=rename_map)\n",
        "\n",
        "ev[\"Month\"]   = pd.to_datetime(ev[\"Month\"], errors=\"coerce\")\n",
        "ev[\"Docs\"]    = pd.to_numeric(ev[\"Docs\"], errors=\"coerce\")\n",
        "ev[\"AvgTone\"] = pd.to_numeric(ev[\"AvgTone\"], errors=\"coerce\")\n",
        "ev = ev.dropna(subset=[\"Month\"]).sort_values(\"Month\").drop_duplicates(subset=\"Month\", keep=\"last\")\n",
        "\n",
        "# clip to window for safety\n",
        "ev = ev[(ev[\"Month\"] >= start) & (ev[\"Month\"] <= cutover_last_events)].copy()\n",
        "\n",
        "# ---------- load GKG master (housing filtered) ----------\n",
        "gkg = pd.read_csv(GKG_MASTER)\n",
        "\n",
        "gkg_ren = {}\n",
        "if \"month\" in gkg.columns: gkg_ren[\"month\"] = \"Month\"\n",
        "if \"docs\"  in gkg.columns: gkg_ren[\"docs\"]  = \"Docs\"\n",
        "if \"avg_tone\" in gkg.columns: gkg_ren[\"avg_tone\"] = \"AvgTone\"\n",
        "if gkg_ren: gkg = gkg.rename(columns=gkg_ren)\n",
        "\n",
        "gkg[\"Month\"]   = pd.to_datetime(gkg[\"Month\"], errors=\"coerce\")\n",
        "gkg[\"Docs\"]    = pd.to_numeric(gkg[\"Docs\"], errors=\"coerce\")\n",
        "gkg[\"AvgTone\"] = pd.to_numeric(gkg[\"AvgTone\"], errors=\"coerce\")\n",
        "gkg = gkg.dropna(subset=[\"Month\"]).sort_values(\"Month\").drop_duplicates(subset=\"Month\", keep=\"last\")\n",
        "\n",
        "# clip to Feb 2015 – Jun 2025\n",
        "gkg = gkg[(gkg[\"Month\"] >= cutover_first_gkg) & (gkg[\"Month\"] <= end)].copy()\n",
        "\n",
        "# ---------- build full index & merge wide ----------\n",
        "full_idx = pd.date_range(start, end, freq=\"MS\")\n",
        "\n",
        "ev_w  = ev.set_index(\"Month\").rename(columns={\"Docs\":\"Docs_Events_Econ\", \"AvgTone\":\"AvgTone_Events_Econ\"})\n",
        "gkg_w = gkg.set_index(\"Month\").rename(columns={\"Docs\":\"Docs_GKG\",        \"AvgTone\":\"AvgTone_GKG\"})\n",
        "\n",
        "wide = (\n",
        "    pd.DataFrame(index=full_idx)\n",
        "      .join(ev_w[[\"Docs_Events_Econ\",\"AvgTone_Events_Econ\"]], how=\"left\")\n",
        "      .join(gkg_w[[\"Docs_GKG\",\"AvgTone_GKG\"]],               how=\"left\")\n",
        "      .rename_axis(\"Month\")\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# ---------- stitched single series (ECON Events -> GKG) ----------\n",
        "wide[\"AvgTone_Stitched\"] = np.where(\n",
        "    wide[\"Month\"] <= cutover_last_events, wide[\"AvgTone_Events_Econ\"], wide[\"AvgTone_GKG\"]\n",
        ")\n",
        "wide[\"Docs_Stitched\"] = np.where(\n",
        "    wide[\"Month\"] <= cutover_last_events, wide[\"Docs_Events_Econ\"], wide[\"Docs_GKG\"]\n",
        ")\n",
        "wide[\"Source\"] = np.where(\n",
        "    wide[\"Month\"] <= cutover_last_events, \"Events_v1_0_ECON\", \"GKG_v2_0_Housing\"\n",
        ")\n",
        "\n",
        "# ---------- save ----------\n",
        "out_wide = OUT_DIR / \"gdelt_tone_2005_2025_wide_ECONxGKG.csv\"\n",
        "out_st   = OUT_DIR / \"gdelt_tone_2005_2025_stitched_ECONxGKG.csv\"\n",
        "\n",
        "wide.to_csv(out_wide, index=False, date_format=\"%Y-%m-%d\")\n",
        "wide[[\"Month\",\"AvgTone_Stitched\",\"Docs_Stitched\",\"Source\"]].to_csv(\n",
        "    out_st, index=False, date_format=\"%Y-%m-%d\"\n",
        ")\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" \", out_wide)\n",
        "print(\" \", out_st)\n",
        "\n",
        "# quick peek\n",
        "print(\"\\nPreview (tail):\")\n",
        "print(wide.tail(6).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhsWrPVK09KD",
        "outputId": "d5ff8659-c9ca-4d08-9d4f-7499bb955508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Saved:\n",
            "  /content/drive/MyDrive/msc_project/gdelt/gdelt_tone_2005_2025_wide_ECONxGKG.csv\n",
            "  /content/drive/MyDrive/msc_project/gdelt/gdelt_tone_2005_2025_stitched_ECONxGKG.csv\n",
            "\n",
            "Preview (tail):\n",
            "     Month  Docs_Events_Econ  AvgTone_Events_Econ  Docs_GKG  AvgTone_GKG  AvgTone_Stitched  Docs_Stitched           Source\n",
            "2025-01-01               NaN                  NaN   14180.0    -1.096796         -1.096796        14180.0 GKG_v2_0_Housing\n",
            "2025-02-01               NaN                  NaN   16683.0    -0.884768         -0.884768        16683.0 GKG_v2_0_Housing\n",
            "2025-03-01               NaN                  NaN   17850.0    -0.767261         -0.767261        17850.0 GKG_v2_0_Housing\n",
            "2025-04-01               NaN                  NaN   14902.0    -1.109188         -1.109188        14902.0 GKG_v2_0_Housing\n",
            "2025-05-01               NaN                  NaN   18253.0    -0.360499         -0.360499        18253.0 GKG_v2_0_Housing\n",
            "2025-06-01               NaN                  NaN    8278.0    -0.826562         -0.826562         8278.0 GKG_v2_0_Housing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge with 2018. 2019, 2020, 2021"
      ],
      "metadata": {
        "id": "lS94hPb-6goF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Update stitched series with 2019, 2020, 2021 (now present in /gdelt) ===\n",
        "!pip -q install pandas\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")\n",
        "\n",
        "# Prefer ECON×GKG stitched; fall back to plain stitched\n",
        "for cand in [\n",
        "    BASE_DIR / \"gdelt_tone_2005_2025_stitched_ECONxGKG.csv\",\n",
        "    BASE_DIR / \"gdelt_tone_2005_2025_stitched.csv\",\n",
        "]:\n",
        "    if cand.exists():\n",
        "        STITCHED = cand\n",
        "        break\n",
        "else:\n",
        "    raise FileNotFoundError(\"No stitched file found in: \" + str(BASE_DIR))\n",
        "\n",
        "print(\"Using stitched:\", STITCHED.name)\n",
        "\n",
        "YEARS = [2019, 2020, 2021]\n",
        "\n",
        "def find_year_file(year:int):\n",
        "    # look in root and in gdelt_run_<year>/\n",
        "    for p in [\n",
        "        BASE_DIR / f\"gdelt_uk_housing_monthly_{year}.csv\",\n",
        "        BASE_DIR / f\"gdelt_run_{year}\" / f\"gdelt_uk_housing_monthly_{year}.csv\",\n",
        "    ]:\n",
        "        if p.exists():\n",
        "            return p\n",
        "    # fallback: search anywhere under BASE_DIR\n",
        "    for p in BASE_DIR.rglob(f\"gdelt_uk_housing_monthly_{year}.csv\"):\n",
        "        return p\n",
        "    print(f\"!! Could not find a yearly CSV for {year}\")\n",
        "    return None\n",
        "\n",
        "def load_year(year:int) -> pd.DataFrame:\n",
        "    p = find_year_file(year)\n",
        "    if p is None:\n",
        "        return pd.DataFrame(columns=[\"Month\",\"AvgTone\",\"Docs\"])\n",
        "    df = pd.read_csv(p).rename(columns={\"month\":\"Month\",\"docs\":\"Docs\",\"avg_tone\":\"AvgTone\"})\n",
        "    df[\"Month\"]   = pd.to_datetime(df[\"Month\"], errors=\"coerce\")\n",
        "    df[\"AvgTone\"] = pd.to_numeric(df.get(\"AvgTone\"), errors=\"coerce\")\n",
        "    df[\"Docs\"]    = pd.to_numeric(df.get(\"Docs\"), errors=\"coerce\")\n",
        "    df = (df[df[\"Month\"].dt.year.eq(year)]\n",
        "          .dropna(subset=[\"Month\"])\n",
        "          .sort_values(\"Month\")\n",
        "          .drop_duplicates(subset=\"Month\", keep=\"last\"))\n",
        "    print(f\"Loaded {year}: {p.relative_to(BASE_DIR)} → {len(df)} rows\")\n",
        "    return df[[\"Month\",\"AvgTone\",\"Docs\"]]\n",
        "\n",
        "# Load stitched base\n",
        "base = pd.read_csv(STITCHED)\n",
        "if \"Month\" not in base.columns:\n",
        "    base.rename(columns={\"month\":\"Month\"}, inplace=True)\n",
        "for c in [\"AvgTone_Stitched\",\"Docs_Stitched\",\"Source\"]:\n",
        "    if c not in base.columns:\n",
        "        raise ValueError(f\"Expected column '{c}' in {STITCHED.name}\")\n",
        "\n",
        "base[\"Month\"]            = pd.to_datetime(base[\"Month\"], errors=\"coerce\")\n",
        "base[\"AvgTone_Stitched\"] = pd.to_numeric(base[\"AvgTone_Stitched\"], errors=\"coerce\")\n",
        "base[\"Docs_Stitched\"]    = pd.to_numeric(base[\"Docs_Stitched\"], errors=\"coerce\")\n",
        "base = (base.dropna(subset=[\"Month\"])\n",
        "             .sort_values(\"Month\")\n",
        "             .drop_duplicates(subset=\"Month\", keep=\"last\")\n",
        "             .set_index(\"Month\"))\n",
        "\n",
        "# Load the three years and combine\n",
        "parts = [load_year(y) for y in YEARS]\n",
        "new_df = pd.concat(parts, ignore_index=True).dropna(subset=[\"Month\"]).drop_duplicates(subset=\"Month\", keep=\"last\").set_index(\"Month\")\n",
        "\n",
        "if new_df.empty:\n",
        "    print(\"\\nNo yearly rows found; stitched file unchanged.\")\n",
        "else:\n",
        "    upd = pd.DataFrame(index=new_df.index)\n",
        "    upd[\"AvgTone_Stitched\"] = pd.to_numeric(new_df[\"AvgTone\"], errors=\"coerce\")\n",
        "    upd[\"Docs_Stitched\"]    = pd.to_numeric(new_df[\"Docs\"], errors=\"coerce\")\n",
        "    upd[\"Source\"]           = \"GKG_v2_0_Housing\"\n",
        "\n",
        "    to_overwrite = base.index.intersection(upd.index)\n",
        "    base.loc[to_overwrite, [\"AvgTone_Stitched\",\"Docs_Stitched\",\"Source\"]] = \\\n",
        "        upd.loc[to_overwrite, [\"AvgTone_Stitched\",\"Docs_Stitched\",\"Source\"]].values\n",
        "\n",
        "    print(f\"\\nOverwrote {len(to_overwrite)} months:\",\n",
        "          \", \".join(pd.Index(to_overwrite).strftime(\"%Y-%m\").tolist()[:24]) + (\" ...\" if len(to_overwrite) > 24 else \"\"))\n",
        "\n",
        "# Save updated stitched\n",
        "OUT_PATH = BASE_DIR / (STITCHED.stem + \"_UPDATED.csv\")\n",
        "base.reset_index().to_csv(OUT_PATH, index=False, date_format=\"%Y-%m-%d\")\n",
        "print(\"\\nSaved:\", OUT_PATH)\n",
        "\n",
        "# Quick peek for 2019–2021\n",
        "peek = base.loc[base.index.year.isin(YEARS)].sort_index()\n",
        "print(\"\\nUpdated span preview (tail):\")\n",
        "print(peek.tail(12))\n"
      ],
      "metadata": {
        "id": "AhM5cPW9JlDW",
        "outputId": "0d0c4fcd-6f94-49aa-d18b-6281213825d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using stitched: gdelt_tone_2005_2025_stitched_ECONxGKG.csv\n",
            "Loaded 2019: gdelt_uk_housing_monthly_2019.csv → 12 rows\n",
            "Loaded 2020: gdelt_uk_housing_monthly_2020.csv → 12 rows\n",
            "Loaded 2021: gdelt_uk_housing_monthly_2021.csv → 12 rows\n",
            "\n",
            "Overwrote 36 months: 2019-01, 2019-02, 2019-03, 2019-04, 2019-05, 2019-06, 2019-07, 2019-08, 2019-09, 2019-10, 2019-11, 2019-12, 2020-01, 2020-02, 2020-03, 2020-04, 2020-05, 2020-06, 2020-07, 2020-08, 2020-09, 2020-10, 2020-11, 2020-12 ...\n",
            "\n",
            "Saved: /content/drive/MyDrive/msc_project/gdelt/gdelt_tone_2005_2025_stitched_ECONxGKG_UPDATED.csv\n",
            "\n",
            "Updated span preview (tail):\n",
            "            AvgTone_Stitched  Docs_Stitched            Source\n",
            "Month                                                        \n",
            "2021-01-01         -1.772631        16649.0  GKG_v2_0_Housing\n",
            "2021-02-01         -1.397794        13488.0  GKG_v2_0_Housing\n",
            "2021-03-01         -1.176045        12649.0  GKG_v2_0_Housing\n",
            "2021-04-01         -1.049070        12321.0  GKG_v2_0_Housing\n",
            "2021-05-01         -1.219614        11832.0  GKG_v2_0_Housing\n",
            "2021-06-01         -0.978622        11735.0  GKG_v2_0_Housing\n",
            "2021-07-01         -1.476303         9401.0  GKG_v2_0_Housing\n",
            "2021-08-01         -2.268476        11375.0  GKG_v2_0_Housing\n",
            "2021-09-01         -1.064425        11785.0  GKG_v2_0_Housing\n",
            "2021-10-01         -1.190661        11336.0  GKG_v2_0_Housing\n",
            "2021-11-01         -1.237307        12425.0  GKG_v2_0_Housing\n",
            "2021-12-01         -1.581094        11988.0  GKG_v2_0_Housing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Merge 2018 into the ECON×GKG UPDATED stitched file ===\n",
        "!pip -q install pandas\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/msc_project/gdelt\")\n",
        "YEAR = 2018\n",
        "\n",
        "# Prefer to edit the UPDATED stitched file; fall back to base if needed\n",
        "CAND_IN = [\n",
        "    BASE / \"gdelt_tone_2005_2025_stitched_ECONxGKG_UPDATED.csv\",\n",
        "    BASE / \"gdelt_tone_2005_2025_stitched_ECONxGKG.csv\",\n",
        "]\n",
        "for p in CAND_IN:\n",
        "    if p.exists():\n",
        "        STITCHED = p\n",
        "        break\n",
        "else:\n",
        "    raise FileNotFoundError(\"Could not find stitched ECONxGKG file in \" + str(BASE))\n",
        "\n",
        "print(\"Editing stitched file:\", STITCHED.name)\n",
        "\n",
        "# --- load stitched (base) ---\n",
        "s = pd.read_csv(STITCHED)\n",
        "if \"Month\" not in s.columns:\n",
        "    s.rename(columns={\"month\":\"Month\"}, inplace=True)\n",
        "\n",
        "# enforce schema + types\n",
        "needed_cols = [\"Month\",\"AvgTone_Stitched\",\"Docs_Stitched\",\"Source\"]\n",
        "missing = [c for c in needed_cols if c not in s.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Stitched file missing columns: {missing}\")\n",
        "\n",
        "s[\"Month\"]            = pd.to_datetime(s[\"Month\"], errors=\"coerce\")\n",
        "s[\"AvgTone_Stitched\"] = pd.to_numeric(s[\"AvgTone_Stitched\"], errors=\"coerce\")\n",
        "s[\"Docs_Stitched\"]    = pd.to_numeric(s[\"Docs_Stitched\"], errors=\"coerce\")\n",
        "s = (s.dropna(subset=[\"Month\"])\n",
        "       .sort_values(\"Month\")\n",
        "       .drop_duplicates(subset=\"Month\", keep=\"last\")\n",
        "       .set_index(\"Month\"))\n",
        "\n",
        "# --- load 2018 year file ---\n",
        "year_path_opts = [\n",
        "    BASE / f\"gdelt_uk_housing_monthly_{YEAR}.csv\",\n",
        "    BASE / f\"gdelt_run_{YEAR}\" / f\"gdelt_uk_housing_monthly_{YEAR}.csv\",\n",
        "]\n",
        "for yp in year_path_opts:\n",
        "    if yp.exists():\n",
        "        YEAR_FILE = yp\n",
        "        break\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Could not find gdelt_uk_housing_monthly_{YEAR}.csv under {BASE}\")\n",
        "\n",
        "y = pd.read_csv(YEAR_FILE).rename(columns={\"month\":\"Month\",\"docs\":\"Docs\",\"avg_tone\":\"AvgTone\"})\n",
        "y[\"Month\"]   = pd.to_datetime(y[\"Month\"], errors=\"coerce\")\n",
        "y[\"AvgTone\"] = pd.to_numeric(y.get(\"AvgTone\"), errors=\"coerce\")\n",
        "y[\"Docs\"]    = pd.to_numeric(y.get(\"Docs\"), errors=\"coerce\")\n",
        "\n",
        "y = (y[y[\"Month\"].dt.year.eq(YEAR)]\n",
        "       .dropna(subset=[\"Month\"])\n",
        "       .sort_values(\"Month\")\n",
        "       .drop_duplicates(subset=\"Month\", keep=\"last\")\n",
        "       .set_index(\"Month\"))\n",
        "\n",
        "if y.empty:\n",
        "    raise ValueError(f\"No rows for {YEAR} in {YEAR_FILE.name}\")\n",
        "\n",
        "# --- prepare update frame and overwrite matching months ---\n",
        "upd = pd.DataFrame(index=y.index)\n",
        "upd[\"AvgTone_Stitched\"] = y[\"AvgTone\"]\n",
        "upd[\"Docs_Stitched\"]    = y[\"Docs\"]\n",
        "upd[\"Source\"]           = \"GKG_v2_0_Housing\"\n",
        "\n",
        "to_overwrite = s.index.intersection(upd.index)\n",
        "s.loc[to_overwrite, [\"AvgTone_Stitched\",\"Docs_Stitched\",\"Source\"]] = \\\n",
        "    upd.loc[to_overwrite, [\"AvgTone_Stitched\",\"Docs_Stitched\",\"Source\"]].values\n",
        "\n",
        "print(f\"Overwrote {len(to_overwrite)} months:\",\n",
        "      \", \".join(pd.Index(to_overwrite).strftime(\"%Y-%m\").tolist()))\n",
        "\n",
        "# --- save back to UPDATED file (overwrite-in-place if it exists) ---\n",
        "OUT = BASE / \"gdelt_tone_2005_2025_stitched_ECONxGKG_UPDATED.csv\"\n",
        "s.reset_index().to_csv(OUT, index=False, date_format=\"%Y-%m-%d\")\n",
        "print(\"Saved:\", OUT)\n",
        "\n",
        "# quick peek of 2018 after merge\n",
        "print(\"\\n2018 preview:\")\n",
        "print(s.loc[s.index.year.eq(YEAR)][[\"AvgTone_Stitched\",\"Docs_Stitched\",\"Source\"]]\n",
        "        .sort_index().to_string())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "38cILKavK6IJ",
        "outputId": "3a224fdb-d974-4cbb-8cdc-6ac54124870f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Editing stitched file: gdelt_tone_2005_2025_stitched_ECONxGKG_UPDATED.csv\n",
            "Overwrote 12 months: 2018-01, 2018-02, 2018-03, 2018-04, 2018-05, 2018-06, 2018-07, 2018-08, 2018-09, 2018-10, 2018-11, 2018-12\n",
            "Saved: /content/drive/MyDrive/msc_project/gdelt/gdelt_tone_2005_2025_stitched_ECONxGKG_UPDATED.csv\n",
            "\n",
            "2018 preview:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Index' object has no attribute 'eq'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1767486827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# quick peek of 2018 after merge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n2018 preview:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m print(s.loc[s.index.year.eq(YEAR)][[\"AvgTone_Stitched\",\"Docs_Stitched\",\"Source\"]]\n\u001b[0m\u001b[1;32m     93\u001b[0m         .sort_index().to_string())\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'eq'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/msc_project/gdelt\")\n",
        "PATH = BASE / \"gdelt_tone_2005_2025_stitched_ECONxGKG_UPDATED.csv\"\n",
        "\n",
        "s = (pd.read_csv(PATH, parse_dates=[\"Month\"])\n",
        "       .set_index(\"Month\"))\n",
        "\n",
        "cols = [\"AvgTone_Stitched\",\"Docs_Stitched\",\"Source\"]\n",
        "\n",
        "# 2018 preview\n",
        "mask_2018 = s.index.year == 2018\n",
        "print(\"2018 preview:\")\n",
        "print(s.loc[mask_2018, cols].sort_index().to_string())\n",
        "\n",
        "# (Optional) quick check for 2019–2021 too\n",
        "for yr in [2019, 2020, 2021]:\n",
        "    m = s.index.year == yr\n",
        "    print(f\"\\n{yr} ({m.sum()} months):\")\n",
        "    if m.sum():\n",
        "        print(s.loc[m, cols].sort_index().to_string())\n",
        "    else:\n",
        "        print(\"No rows in stitched file for this year yet.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emb07nx-Lxl0",
        "outputId": "08bd8be7-3a17-41bd-b78e-251ad8647108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018 preview:\n",
            "            AvgTone_Stitched  Docs_Stitched            Source\n",
            "Month                                                        \n",
            "2018-01-01         -1.257888        23140.0  GKG_v2_0_Housing\n",
            "2018-02-01         -1.336428        21142.0  GKG_v2_0_Housing\n",
            "2018-03-01         -1.652378        30247.0  GKG_v2_0_Housing\n",
            "2018-04-01         -1.951460        30498.0  GKG_v2_0_Housing\n",
            "2018-05-01         -1.159840        27642.0  GKG_v2_0_Housing\n",
            "2018-06-01         -1.025702        26249.0  GKG_v2_0_Housing\n",
            "2018-07-01         -1.313607        26933.0  GKG_v2_0_Housing\n",
            "2018-08-01         -1.576400        19712.0  GKG_v2_0_Housing\n",
            "2018-09-01         -1.553960        18445.0  GKG_v2_0_Housing\n",
            "2018-10-01         -1.314133        20027.0  GKG_v2_0_Housing\n",
            "2018-11-01         -1.401884        20941.0  GKG_v2_0_Housing\n",
            "2018-12-01         -1.599224        18113.0  GKG_v2_0_Housing\n",
            "\n",
            "2019 (12 months):\n",
            "            AvgTone_Stitched  Docs_Stitched            Source\n",
            "Month                                                        \n",
            "2019-01-01         -1.409459        17837.0  GKG_v2_0_Housing\n",
            "2019-02-01         -1.213982        19175.0  GKG_v2_0_Housing\n",
            "2019-03-01         -1.386139        19681.0  GKG_v2_0_Housing\n",
            "2019-04-01         -1.578743        18225.0  GKG_v2_0_Housing\n",
            "2019-05-01         -1.526026        22140.0  GKG_v2_0_Housing\n",
            "2019-06-01         -1.500186        22166.0  GKG_v2_0_Housing\n",
            "2019-07-01         -1.759659        21116.0  GKG_v2_0_Housing\n",
            "2019-08-01         -1.974748        20330.0  GKG_v2_0_Housing\n",
            "2019-09-01         -1.733146        19306.0  GKG_v2_0_Housing\n",
            "2019-10-01         -1.537151        19108.0  GKG_v2_0_Housing\n",
            "2019-11-01         -1.493249        17406.0  GKG_v2_0_Housing\n",
            "2019-12-01         -1.608837        17247.0  GKG_v2_0_Housing\n",
            "\n",
            "2020 (12 months):\n",
            "            AvgTone_Stitched  Docs_Stitched            Source\n",
            "Month                                                        \n",
            "2020-01-01         -2.071448        19100.0  GKG_v2_0_Housing\n",
            "2020-02-01         -1.701928        17058.0  GKG_v2_0_Housing\n",
            "2020-03-01         -2.201446        24376.0  GKG_v2_0_Housing\n",
            "2020-04-01         -2.290566        20912.0  GKG_v2_0_Housing\n",
            "2020-05-01         -1.932716        17790.0  GKG_v2_0_Housing\n",
            "2020-06-01         -2.055529        20187.0  GKG_v2_0_Housing\n",
            "2020-07-01         -1.747520        17778.0  GKG_v2_0_Housing\n",
            "2020-08-01         -1.714375        13975.0  GKG_v2_0_Housing\n",
            "2020-09-01         -1.599640        16551.0  GKG_v2_0_Housing\n",
            "2020-10-01         -1.462914        11798.0  GKG_v2_0_Housing\n",
            "2020-11-01         -1.147462         9923.0  GKG_v2_0_Housing\n",
            "2020-12-01         -1.257441        16197.0  GKG_v2_0_Housing\n",
            "\n",
            "2021 (12 months):\n",
            "            AvgTone_Stitched  Docs_Stitched            Source\n",
            "Month                                                        \n",
            "2021-01-01         -1.772631        16649.0  GKG_v2_0_Housing\n",
            "2021-02-01         -1.397794        13488.0  GKG_v2_0_Housing\n",
            "2021-03-01         -1.176045        12649.0  GKG_v2_0_Housing\n",
            "2021-04-01         -1.049070        12321.0  GKG_v2_0_Housing\n",
            "2021-05-01         -1.219614        11832.0  GKG_v2_0_Housing\n",
            "2021-06-01         -0.978622        11735.0  GKG_v2_0_Housing\n",
            "2021-07-01         -1.476303         9401.0  GKG_v2_0_Housing\n",
            "2021-08-01         -2.268476        11375.0  GKG_v2_0_Housing\n",
            "2021-09-01         -1.064425        11785.0  GKG_v2_0_Housing\n",
            "2021-10-01         -1.190661        11336.0  GKG_v2_0_Housing\n",
            "2021-11-01         -1.237307        12425.0  GKG_v2_0_Housing\n",
            "2021-12-01         -1.581094        11988.0  GKG_v2_0_Housing\n"
          ]
        }
      ]
    }
  ]
}