{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "2015"
      ],
      "metadata": {
        "id": "TjVr_Rs7G9GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install pandas requests\n",
        "\n",
        "# ================== CONFIG (set this per run) ==================\n",
        "YEAR = 2015\n",
        "\n",
        "HOURLY_ONLY = True\n",
        "CHECKPOINT_EVERY = 200\n",
        "PROGRESS_EVERY   = 100\n",
        "MAX_FILES = None\n",
        "\n",
        "# Filters (same as before)\n",
        "UK_REGEX = r'#(?:GBR|GB|UK|United Kingdom|England|Scotland|Wales|Northern Ireland)#'\n",
        "THEME_PATTERN = r'(?:HOUS|MORTGAG|REMORTGAG|RENT|TENAN|REAL[ _]?ESTATE|PROPERTY|HOME[ _]?PRIC|HOUSE[ _]?PRIC|RIGHTMOVE|ZOOPLA|LANDLORD|BUY[ -]?TO[ -]?LET)'\n",
        "\n",
        "# GKG v2.1 columns (0-based)\n",
        "DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I = 1, 8, 10, 15\n",
        "\n",
        "# Where to write for this YEAR\n",
        "from pathlib import Path\n",
        "OUT_DIR = Path(f\"/content/gdelt_run_{YEAR}\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "YEARLY_CSV  = OUT_DIR / f\"gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "STATE_PATH  = OUT_DIR / \"state.json\"\n",
        "\n",
        "MASTER_URL  = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
        "\n",
        "\n",
        "import re, io, zipfile, math, time, json, logging, requests, pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "\n",
        "def year_bounds(y:int):\n",
        "    if y == 2015:\n",
        "        return \"20150218000000\", \"20151231235959\"     # GKG v2 starts mid-Feb 2015\n",
        "    elif y == 2025:\n",
        "        return \"20250101000000\", \"20250630235959\"\n",
        "    else:\n",
        "        return f\"{y}0101000000\", f\"{y}1231235959\"\n",
        "\n",
        "START_TS, END_TS = year_bounds(YEAR)\n",
        "\n",
        "def session():\n",
        "    from urllib3.util.retry import Retry\n",
        "    from requests.adapters import HTTPAdapter\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-colab/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def get(url, timeout=60, s=None):\n",
        "    s = s or session()\n",
        "    if url.startswith(\"https://\"):  # force http to dodge cert issues\n",
        "        url = \"http://\" + url[len(\"https://\"):]\n",
        "    r = s.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "def build_gkg_urls():\n",
        "    txt = get(MASTER_URL, timeout=90).text\n",
        "    urls = []\n",
        "    for line in txt.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3: continue\n",
        "        url = parts[2]\n",
        "        m = re.search(r'/gdeltv2/(\\d{14})\\.gkg\\.csv\\.zip$', url)\n",
        "        if not m: continue\n",
        "        ts = m.group(1)\n",
        "        if START_TS <= ts <= END_TS:\n",
        "            if HOURLY_ONLY and ts[10:12] != \"00\":  # one per hour\n",
        "                continue\n",
        "            urls.append(url.replace(\"https://\", \"http://\"))\n",
        "    urls.sort()\n",
        "    return urls\n",
        "\n",
        "def month_from_dateint(dateint):\n",
        "    dt = datetime.strptime(str(dateint), \"%Y%m%d%H%M%S\")\n",
        "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "\n",
        "def doc_tone(v2tone):\n",
        "    try:\n",
        "        return float(str(v2tone).split(\",\")[0])  # first field is doc tone\n",
        "    except Exception:\n",
        "        return math.nan\n",
        "\n",
        "def process_df(df, counts, tone_sum, tone_n, uk_re, theme_re):\n",
        "    m_uk = df[V2LOCS_I].astype(str).str.contains(uk_re, na=False, regex=True)\n",
        "    m_th = df[V2THEMES_I].astype(str).str.contains(theme_re, na=False, regex=True)\n",
        "    sub = df.loc[m_uk & m_th, [DATE_I, V2TONE_I]]\n",
        "    if sub.empty: return 0\n",
        "    months = sub[DATE_I].astype(str).apply(month_from_dateint)\n",
        "    tones  = sub[V2TONE_I].apply(doc_tone)\n",
        "    kept = 0\n",
        "    for m, t in zip(months, tones):\n",
        "        counts[m] += 1\n",
        "        if not math.isnan(t):\n",
        "            tone_sum[m] += t\n",
        "            tone_n[m] += 1\n",
        "        kept += 1\n",
        "    return kept\n",
        "\n",
        "def write_year_output(processed_count, processed_tail, counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m]/tone_n[m]) if tone_n[m] else None\n",
        "        rows.append({\"month\": f\"{m}-01\", \"docs\": n, \"avg_tone\": avg})\n",
        "    monthly = pd.DataFrame(rows, columns=[\"month\",\"docs\",\"avg_tone\"]).sort_values(\"month\")\n",
        "    monthly.to_csv(YEARLY_CSV, index=False)\n",
        "    json.dump({\n",
        "        \"processed_count\": processed_count,\n",
        "        \"processed_urls_tail\": processed_tail[-20000:],\n",
        "        \"counts\": counts, \"tone_sum\": tone_sum, \"tone_n\": tone_n\n",
        "    }, open(STATE_PATH, \"w\"))\n",
        "    print(f\"Saved → {YEARLY_CSV}\")\n",
        "\n",
        "# ---------- plan + resume for this YEAR ----------\n",
        "s = session()\n",
        "urls_all = build_gkg_urls()\n",
        "print(f\"[{YEAR}] planned files: {len(urls_all):,}\")\n",
        "\n",
        "if STATE_PATH.exists():\n",
        "    state = json.load(open(STATE_PATH))\n",
        "    processed_count      = state.get(\"processed_count\", 0)\n",
        "    processed_urls_tail  = state.get(\"processed_urls_tail\", [])\n",
        "    counts   = defaultdict(int,   state.get(\"counts\", {}))\n",
        "    tone_sum = defaultdict(float, state.get(\"tone_sum\", {}))\n",
        "    tone_n   = defaultdict(int,   state.get(\"tone_n\", {}))\n",
        "    print(f\"Resuming: {processed_count} files done previously.\")\n",
        "else:\n",
        "    processed_count, processed_urls_tail = 0, []\n",
        "    counts, tone_sum, tone_n = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "already = set(processed_urls_tail)\n",
        "todo = [u for u in urls_all if u not in already]\n",
        "# MAX_FILES=None -> no slicing; process all remaining files\n",
        "if MAX_FILES is not None:\n",
        "    todo = todo[:MAX_FILES]\n",
        "print(f\"[{YEAR}] remaining this run: {len(todo):,}\")\n",
        "\n",
        "uk_re = re.compile(UK_REGEX, re.I)\n",
        "theme_re = re.compile(THEME_PATTERN, re.I)\n",
        "\n",
        "# ---------------- main loop ----------------\n",
        "t0 = time.time()\n",
        "docs_seen = 0\n",
        "\n",
        "for i, url in enumerate(todo, 1):\n",
        "    try:\n",
        "        z = zipfile.ZipFile(io.BytesIO(get(url, timeout=90, s=s).content))\n",
        "        name = z.namelist()[0]\n",
        "        df = pd.read_csv(\n",
        "            z.open(name),\n",
        "            sep=\"\\t\", header=None, quoting=3, low_memory=False,\n",
        "            usecols=[DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I],\n",
        "            dtype={DATE_I:str, V2THEMES_I:str, V2LOCS_I:str, V2TONE_I:str}\n",
        "        )\n",
        "        kept = process_df(df, counts, tone_sum, tone_n, uk_re, theme_re)\n",
        "        docs_seen += kept\n",
        "        processed_count += 1\n",
        "        processed_urls_tail.append(url)\n",
        "        if len(processed_urls_tail) > 20000:\n",
        "            processed_urls_tail = processed_urls_tail[-20000:]\n",
        "    except Exception:\n",
        "        # skip bad zips quietly\n",
        "        pass\n",
        "\n",
        "    if i % PROGRESS_EVERY == 0:\n",
        "        elapsed = max(1e-6, time.time()-t0)\n",
        "        rate = i / elapsed\n",
        "        eta_min = (len(todo)-i) / max(1e-6, rate) / 60\n",
        "        print(f\"[{YEAR}] {i}/{len(todo)} | {rate:.2f} files/s | ETA ≈ {eta_min:.1f} min | docs_kept={docs_seen}\")\n",
        "\n",
        "    if i % CHECKPOINT_EVERY == 0:\n",
        "        write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "\n",
        "# final write for the year\n",
        "write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "print(f\"[{YEAR}] Done. This session processed: {len(todo)} | Total processed: {processed_count} | Docs kept: {docs_seen}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h96EhvBG_LS",
        "outputId": "d79af11f-8d4c-43af-835d-73fe487a9245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2015] planned files: 7,536\n",
            "Resuming: 997 files done previously.\n",
            "[2015] remaining this run: 6,539\n",
            "[2015] 100/6539 | 1.02 files/s | ETA ≈ 105.7 min | docs_kept=1215\n",
            "[2015] 200/6539 | 0.95 files/s | ETA ≈ 111.5 min | docs_kept=2911\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 300/6539 | 0.98 files/s | ETA ≈ 106.3 min | docs_kept=4174\n",
            "[2015] 400/6539 | 0.94 files/s | ETA ≈ 109.3 min | docs_kept=6214\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 500/6539 | 0.95 files/s | ETA ≈ 105.5 min | docs_kept=7456\n",
            "[2015] 600/6539 | 0.94 files/s | ETA ≈ 105.5 min | docs_kept=9238\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 700/6539 | 0.91 files/s | ETA ≈ 106.4 min | docs_kept=12145\n",
            "[2015] 800/6539 | 0.92 files/s | ETA ≈ 104.0 min | docs_kept=13953\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 900/6539 | 0.90 files/s | ETA ≈ 104.3 min | docs_kept=17574\n",
            "[2015] 1000/6539 | 0.91 files/s | ETA ≈ 101.6 min | docs_kept=19796\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 1100/6539 | 0.91 files/s | ETA ≈ 100.1 min | docs_kept=22309\n",
            "[2015] 1200/6539 | 0.90 files/s | ETA ≈ 98.7 min | docs_kept=24797\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 1300/6539 | 0.91 files/s | ETA ≈ 96.2 min | docs_kept=26888\n",
            "[2015] 1400/6539 | 0.90 files/s | ETA ≈ 94.7 min | docs_kept=29218\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 1500/6539 | 0.91 files/s | ETA ≈ 92.2 min | docs_kept=31127\n",
            "[2015] 1600/6539 | 0.91 files/s | ETA ≈ 90.3 min | docs_kept=33283\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 1700/6539 | 0.91 files/s | ETA ≈ 88.6 min | docs_kept=36410\n",
            "[2015] 1800/6539 | 0.91 files/s | ETA ≈ 86.4 min | docs_kept=38358\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 1900/6539 | 0.91 files/s | ETA ≈ 85.3 min | docs_kept=41438\n",
            "[2015] 2000/6539 | 0.91 files/s | ETA ≈ 83.1 min | docs_kept=43246\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 2100/6539 | 0.91 files/s | ETA ≈ 81.6 min | docs_kept=46243\n",
            "[2015] 2200/6539 | 0.90 files/s | ETA ≈ 79.9 min | docs_kept=49374\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 2300/6539 | 0.91 files/s | ETA ≈ 78.0 min | docs_kept=52312\n",
            "[2015] 2400/6539 | 0.88 files/s | ETA ≈ 78.2 min | docs_kept=56701\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 2500/6539 | 0.87 files/s | ETA ≈ 77.5 min | docs_kept=60147\n",
            "[2015] 2600/6539 | 0.85 files/s | ETA ≈ 76.8 min | docs_kept=63298\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 2700/6539 | 0.85 files/s | ETA ≈ 75.1 min | docs_kept=66150\n",
            "[2015] 2800/6539 | 0.85 files/s | ETA ≈ 73.2 min | docs_kept=68500\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 2900/6539 | 0.84 files/s | ETA ≈ 71.8 min | docs_kept=71721\n",
            "[2015] 3000/6539 | 0.85 files/s | ETA ≈ 69.8 min | docs_kept=73915\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 3100/6539 | 0.84 files/s | ETA ≈ 68.2 min | docs_kept=76589\n",
            "[2015] 3200/6539 | 0.84 files/s | ETA ≈ 66.3 min | docs_kept=78916\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 3300/6539 | 0.84 files/s | ETA ≈ 64.2 min | docs_kept=81251\n",
            "[2015] 3400/6539 | 0.84 files/s | ETA ≈ 62.6 min | docs_kept=84307\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 3500/6539 | 0.84 files/s | ETA ≈ 60.6 min | docs_kept=86539\n",
            "[2015] 3600/6539 | 0.83 files/s | ETA ≈ 58.9 min | docs_kept=89458\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 3700/6539 | 0.83 files/s | ETA ≈ 56.9 min | docs_kept=92061\n",
            "[2015] 3800/6539 | 0.83 files/s | ETA ≈ 54.9 min | docs_kept=94228\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 3900/6539 | 0.83 files/s | ETA ≈ 53.0 min | docs_kept=97498\n",
            "[2015] 4000/6539 | 0.83 files/s | ETA ≈ 51.0 min | docs_kept=99881\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 4100/6539 | 0.83 files/s | ETA ≈ 49.2 min | docs_kept=103210\n",
            "[2015] 4200/6539 | 0.83 files/s | ETA ≈ 47.2 min | docs_kept=105852\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 4300/6539 | 0.83 files/s | ETA ≈ 45.2 min | docs_kept=108233\n",
            "[2015] 4400/6539 | 0.82 files/s | ETA ≈ 43.4 min | docs_kept=111717\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 4500/6539 | 0.82 files/s | ETA ≈ 41.3 min | docs_kept=114230\n",
            "[2015] 4600/6539 | 0.82 files/s | ETA ≈ 39.4 min | docs_kept=117406\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 4700/6539 | 0.82 files/s | ETA ≈ 37.4 min | docs_kept=119988\n",
            "[2015] 4800/6539 | 0.82 files/s | ETA ≈ 35.4 min | docs_kept=122315\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 4900/6539 | 0.82 files/s | ETA ≈ 33.4 min | docs_kept=125159\n",
            "[2015] 5000/6539 | 0.82 files/s | ETA ≈ 31.4 min | docs_kept=127862\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 5100/6539 | 0.82 files/s | ETA ≈ 29.4 min | docs_kept=130226\n",
            "[2015] 5200/6539 | 0.81 files/s | ETA ≈ 27.4 min | docs_kept=133367\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 5300/6539 | 0.81 files/s | ETA ≈ 25.3 min | docs_kept=135826\n",
            "[2015] 5400/6539 | 0.81 files/s | ETA ≈ 23.4 min | docs_kept=139471\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 5500/6539 | 0.81 files/s | ETA ≈ 21.3 min | docs_kept=142994\n",
            "[2015] 5600/6539 | 0.81 files/s | ETA ≈ 19.3 min | docs_kept=146026\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 5700/6539 | 0.81 files/s | ETA ≈ 17.3 min | docs_kept=150171\n",
            "[2015] 5800/6539 | 0.81 files/s | ETA ≈ 15.2 min | docs_kept=152715\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 5900/6539 | 0.81 files/s | ETA ≈ 13.2 min | docs_kept=156673\n",
            "[2015] 6000/6539 | 0.81 files/s | ETA ≈ 11.1 min | docs_kept=160176\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 6100/6539 | 0.81 files/s | ETA ≈ 9.0 min | docs_kept=163281\n",
            "[2015] 6200/6539 | 0.81 files/s | ETA ≈ 7.0 min | docs_kept=166730\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 6300/6539 | 0.81 files/s | ETA ≈ 4.9 min | docs_kept=169470\n",
            "[2015] 6400/6539 | 0.81 files/s | ETA ≈ 2.9 min | docs_kept=172076\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] 6500/6539 | 0.81 files/s | ETA ≈ 0.8 min | docs_kept=174004\n",
            "Saved → /content/gdelt_run_2015/gdelt_uk_housing_monthly_2015.csv\n",
            "[2015] Done. This session processed: 6539 | Total processed: 7484 | Docs kept: 175091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "YEAR = 2015  # change per year you just processed\n",
        "\n",
        "df = pd.read_csv(f\"/content/gdelt_run_{YEAR}/gdelt_uk_housing_monthly_{YEAR}.csv\",\n",
        "                 parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "\n",
        "# Reindex to full year so you see empty months explicitly\n",
        "full = (df.set_index(\"month\")\n",
        "          .reindex(pd.date_range(f\"{YEAR}-01-01\", f\"{YEAR}-12-01\", freq=\"MS\"))\n",
        "          .rename_axis(\"month\")\n",
        "          .reset_index())\n",
        "\n",
        "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.3f}\")\n",
        "print(full.rename(columns={\"month\":\"Month\",\"docs\":\"Docs\",\"avg_tone\":\"AvgTone\"})\n",
        "          .to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azs22B_7lxWY",
        "outputId": "1bf2b11f-804a-48f2-f104-dc499b106655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Month       Docs  AvgTone\n",
            "2015-01-01        NaN      NaN\n",
            "2015-02-01  1,594.000   -0.542\n",
            "2015-03-01 13,219.000   -0.226\n",
            "2015-04-01 12,775.000   -0.327\n",
            "2015-05-01 17,417.000   -0.999\n",
            "2015-06-01 18,734.000   -1.570\n",
            "2015-07-01 23,645.000   -1.352\n",
            "2015-08-01 18,591.000   -1.222\n",
            "2015-09-01 20,365.000   -1.225\n",
            "2015-10-01 18,921.000   -0.969\n",
            "2015-11-01 22,705.000   -1.871\n",
            "2015-12-01 22,439.000   -1.730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = Path(\"/content\")  # Corrected path to where the files are located\n",
        "YEAR = 2015\n",
        "\n",
        "year_file   = BASE_DIR / f\"gdelt_run_{YEAR}/gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "master_file = BASE_DIR / \"gdelt_uk_housing_monthly_STITCHING.csv\"  # running master\n",
        "\n",
        "y = pd.read_csv(year_file, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "\n",
        "if master_file.exists():\n",
        "    m = pd.read_csv(master_file, parse_dates=[\"month\"])\n",
        "    m = (pd.concat([m, y], ignore_index=True)\n",
        "           .drop_duplicates(subset=\"month\", keep=\"last\")\n",
        "           .sort_values(\"month\")\n",
        "           .reset_index(drop=True))\n",
        "else:\n",
        "    m = y\n",
        "\n",
        "m.to_csv(master_file, index=False)\n",
        "print(\"Saved master:\", master_file, m.shape)\n",
        "print(m.tail(12).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3c4K7qnmV5D",
        "outputId": "a77e776f-3652-4b70-e61f-152db1566532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved master: /content/gdelt_uk_housing_monthly_STITCHING.csv (11, 3)\n",
            "     month  docs  avg_tone\n",
            "2015-02-01  1594    -0.542\n",
            "2015-03-01 13219    -0.226\n",
            "2015-04-01 12775    -0.327\n",
            "2015-05-01 17417    -0.999\n",
            "2015-06-01 18734    -1.570\n",
            "2015-07-01 23645    -1.352\n",
            "2015-08-01 18591    -1.222\n",
            "2015-09-01 20365    -1.225\n",
            "2015-10-01 18921    -0.969\n",
            "2015-11-01 22705    -1.871\n",
            "2015-12-01 22439    -1.730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import shutil, os\n",
        "\n",
        "# 1) Confirm mount\n",
        "print(\"Drive mounted:\", os.path.ismount(\"/content/drive\"))\n",
        "\n",
        "# 2) Your Drive workspace\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 3) Move the master file from /content -> Drive (if it exists there)\n",
        "src = Path(\"/content/gdelt_uk_housing_monthly_STITCHING.csv\")\n",
        "dst = BASE_DIR / src.name\n",
        "if src.exists():\n",
        "    shutil.move(str(src), str(dst))\n",
        "    print(\"Moved master to:\", dst)\n",
        "else:\n",
        "    print(\"No master file found in /content; nothing to move.\")\n",
        "\n",
        "# 4) Always use this path for the master file\n",
        "MASTER_FILE = BASE_DIR / \"gdelt_uk_housing_monthly_STITCHING.csv\"\n",
        "print(\"Master path to use:\", MASTER_FILE)\n",
        "\n",
        "# 5) Quick listing\n",
        "!ls -lh \"/content/drive/MyDrive/msc_project/gdelt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRJd4oVTpz0-",
        "outputId": "e2891891-5037-4c34-eeac-c72c42aef5f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive mounted: True\n",
            "Moved master to: /content/drive/MyDrive/msc_project/gdelt/gdelt_uk_housing_monthly_STITCHING.csv\n",
            "Master path to use: /content/drive/MyDrive/msc_project/gdelt/gdelt_uk_housing_monthly_STITCHING.csv\n",
            "total 512\n",
            "-rw------- 1 root root 423 Oct  5 13:40 gdelt_uk_housing_monthly_STITCHING.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2016"
      ],
      "metadata": {
        "id": "Zp15QyIqwRA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== GDELT GKG v2.1 — UK Housing — YEAR 2016 =====================\n",
        "# Runs in chunks with resume; writes to Google Drive; auto-appends to master.\n",
        "\n",
        "!pip -q install pandas requests\n",
        "\n",
        "# --------- Mount Drive & set project folder ---------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")  # <- change if you prefer\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ================== CONFIG (2016) ==================\n",
        "YEAR = 2016          # <- this cell is for 2016\n",
        "MAX_FILES = None     # <- chunk size per run; set None to process ALL remaining files this year\n",
        "HOURLY_ONLY = True   # take top-of-hour files only (lighter than every 15 min)\n",
        "CHECKPOINT_EVERY = 200\n",
        "PROGRESS_EVERY   = 100\n",
        "\n",
        "# Filters\n",
        "UK_REGEX = r'#(?:GBR|GB|UK|United Kingdom|England|Scotland|Wales|Northern Ireland)#'\n",
        "THEME_PATTERN = r'(?:HOUS|MORTGAG|REMORTGAG|RENT|TENAN|REAL[ _]?ESTATE|PROPERTY|HOME[ _]?PRIC|HOUSE[ _]?PRIC|RIGHTMOVE|ZOOPLA|LANDLORD|BUY[ -]?TO[ -]?LET)'\n",
        "\n",
        "# GKG v2.1 columns (0-based indices)\n",
        "DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I = 1, 8, 10, 15\n",
        "\n",
        "# Paths (on Drive)\n",
        "OUT_DIR = BASE_DIR / f\"gdelt_run_{YEAR}\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "YEARLY_CSV = OUT_DIR / f\"gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "STATE_PATH = OUT_DIR / \"state.json\"\n",
        "MASTER_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
        "\n",
        "# Year bounds for 2016 (full calendar year)\n",
        "START_TS, END_TS = \"20160101000000\", \"20161231235959\"\n",
        "\n",
        "# ----------------- Imports & setup -----------------\n",
        "import re, io, zipfile, math, time, json, logging, requests, pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-colab/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def get(url, timeout=60, s=None):\n",
        "    s = s or session()\n",
        "    if url.startswith(\"https://\"):  # avoid SSL hiccups in Colab\n",
        "        url = \"http://\" + url[len(\"https://\"):]\n",
        "    r = s.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "def build_gkg_urls():\n",
        "    txt = get(MASTER_URL, timeout=90).text\n",
        "    urls = []\n",
        "    for line in txt.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3: continue\n",
        "        url = parts[2]\n",
        "        m = re.search(r'/gdeltv2/(\\d{14})\\.gkg\\.csv\\.zip$', url)\n",
        "        if not m: continue\n",
        "        ts = m.group(1)\n",
        "        if START_TS <= ts <= END_TS:\n",
        "            if HOURLY_ONLY and ts[10:12] != \"00\":  # one per hour\n",
        "                continue\n",
        "            urls.append(url.replace(\"https://\",\"http://\"))\n",
        "    urls.sort()\n",
        "    return urls\n",
        "\n",
        "def month_from_dateint(dateint):\n",
        "    dt = datetime.strptime(str(dateint), \"%Y%m%d%H%M%S\")\n",
        "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "\n",
        "def doc_tone(v2tone):\n",
        "    try:\n",
        "        return float(str(v2tone).split(\",\")[0])  # first item is doc tone\n",
        "    except Exception:\n",
        "        return math.nan\n",
        "\n",
        "def process_df(df, counts, tone_sum, tone_n, uk_re, theme_re):\n",
        "    m_uk = df[V2LOCS_I].astype(str).str.contains(uk_re, na=False, regex=True)\n",
        "    m_th = df[V2THEMES_I].astype(str).str.contains(theme_re, na=False, regex=True)\n",
        "    sub = df.loc[m_uk & m_th, [DATE_I, V2TONE_I]]\n",
        "    if sub.empty: return 0\n",
        "    months = sub[DATE_I].astype(str).apply(month_from_dateint)\n",
        "    tones  = sub[V2TONE_I].apply(doc_tone)\n",
        "    kept = 0\n",
        "    for m, t in zip(months, tones):\n",
        "        counts[m] += 1\n",
        "        if not math.isnan(t):\n",
        "            tone_sum[m] += t\n",
        "            tone_n[m] += 1\n",
        "        kept += 1\n",
        "    return kept\n",
        "\n",
        "def write_year_output(processed_count, processed_tail, counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m]/tone_n[m]) if tone_n[m] else None\n",
        "        rows.append({\"month\": f\"{m}-01\", \"docs\": n, \"avg_tone\": avg})\n",
        "    monthly = pd.DataFrame(rows, columns=[\"month\",\"docs\",\"avg_tone\"]).sort_values(\"month\")\n",
        "    monthly.to_csv(YEARLY_CSV, index=False)\n",
        "    json.dump({\n",
        "        \"processed_count\": processed_count,\n",
        "        \"processed_urls_tail\": processed_tail[-20000:],\n",
        "        \"counts\": counts, \"tone_sum\": tone_sum, \"tone_n\": tone_n\n",
        "    }, open(STATE_PATH, \"w\"))\n",
        "    print(f\"Saved → {YEARLY_CSV}\")\n",
        "\n",
        "# ----------------- Plan + resume -----------------\n",
        "s = session()\n",
        "urls_all = build_gkg_urls()\n",
        "print(f\"[{YEAR}] planned hourly files: {len(urls_all):,}\")\n",
        "\n",
        "if STATE_PATH.exists():\n",
        "    state = json.load(open(STATE_PATH))\n",
        "    processed_count     = int(state.get(\"processed_count\", 0))\n",
        "    processed_urls_tail = state.get(\"processed_urls_tail\", [])\n",
        "    counts   = defaultdict(int,   state.get(\"counts\", {}))\n",
        "    tone_sum = defaultdict(float, state.get(\"tone_sum\", {}))\n",
        "    tone_n   = defaultdict(int,   state.get(\"tone_n\", {}))\n",
        "    print(f\"Resuming: {processed_count:,} files done previously.\")\n",
        "else:\n",
        "    processed_count, processed_urls_tail = 0, []\n",
        "    counts, tone_sum, tone_n = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "already = set(processed_urls_tail)\n",
        "todo = [u for u in urls_all if u not in already]\n",
        "if MAX_FILES is not None:\n",
        "    todo = todo[:MAX_FILES]\n",
        "print(f\"[{YEAR}] processing this session: {len(todo):,} files\")\n",
        "\n",
        "uk_re = re.compile(UK_REGEX, re.I)\n",
        "theme_re = re.compile(THEME_PATTERN, re.I)\n",
        "\n",
        "# ----------------- Main loop -----------------\n",
        "t0 = time.time()\n",
        "docs_seen = 0\n",
        "\n",
        "for i, url in enumerate(todo, 1):\n",
        "    try:\n",
        "        z = zipfile.ZipFile(io.BytesIO(get(url, timeout=90, s=s).content))\n",
        "        name = z.namelist()[0]\n",
        "        df = pd.read_csv(\n",
        "            z.open(name),\n",
        "            sep=\"\\t\", header=None, quoting=3, low_memory=False,\n",
        "            usecols=[DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I],\n",
        "            dtype={DATE_I:str, V2THEMES_I:str, V2LOCS_I:str, V2TONE_I:str}\n",
        "        )\n",
        "        docs_seen += process_df(df, counts, tone_sum, tone_n, uk_re, theme_re)\n",
        "        processed_count += 1\n",
        "        processed_urls_tail.append(url)\n",
        "        if len(processed_urls_tail) > 20000:\n",
        "            processed_urls_tail = processed_urls_tail[-20000:]\n",
        "    except Exception:\n",
        "        # skip bad zips quietly\n",
        "        pass\n",
        "\n",
        "    if i % PROGRESS_EVERY == 0:\n",
        "        elapsed = max(1e-6, time.time()-t0)\n",
        "        rate = i / elapsed\n",
        "        eta_min = (len(todo)-i) / max(1e-6, rate) / 60\n",
        "        print(f\"[{YEAR}] {i}/{len(todo)} | {rate:.2f} files/s | ETA≈{eta_min:.1f}m | docs_kept={docs_seen}\")\n",
        "\n",
        "    if i % CHECKPOINT_EVERY == 0:\n",
        "        write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "\n",
        "# Final write for the year\n",
        "write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "print(f\"[{YEAR}] Done. This session processed: {len(todo):,} | Total processed so far: {processed_count:,} | Docs kept: {docs_seen:,}\")\n",
        "\n",
        "# ----------------- Auto-append to Drive master -----------------\n",
        "try:\n",
        "    master_file = BASE_DIR / \"gdelt_uk_housing_monthly_STITCHING.csv\"\n",
        "    y = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    if master_file.exists():\n",
        "        m = pd.read_csv(master_file, parse_dates=[\"month\"])\n",
        "        m = (pd.concat([m, y], ignore_index=True)\n",
        "               .drop_duplicates(subset=\"month\", keep=\"last\")\n",
        "               .sort_values(\"month\")\n",
        "               .reset_index(drop=True))\n",
        "    else:\n",
        "        m = y\n",
        "    m.to_csv(master_file, index=False)\n",
        "    print(\"Appended to master:\", master_file, m.shape)\n",
        "except Exception as e:\n",
        "    print(\"Append-to-master skipped:\", e)\n",
        "\n",
        "# ----------------- Pretty print monthly (quick check) -----------------\n",
        "try:\n",
        "    df = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    full = (df.set_index(\"month\")\n",
        "              .reindex(pd.date_range(f\"{YEAR}-01-01\", f\"{YEAR}-12-01\", freq=\"MS\"))\n",
        "              .rename_axis(\"Month\").reset_index())\n",
        "    full[\"Docs\"]    = pd.to_numeric(full[\"docs\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "    full[\"AvgTone\"] = pd.to_numeric(full[\"avg_tone\"], errors=\"coerce\")\n",
        "    print(full[[\"Month\",\"Docs\",\"AvgTone\"]].to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Preview skipped:\", e)\n",
        "# ===================== end of cell =====================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z7fdoF7wSO-",
        "outputId": "7a9232e3-2e71-45ab-e442-6c76ff73962e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[2016] planned hourly files: 8,774\n",
            "[2016] processing this session: 8,774 files\n",
            "[2016] 100/8774 | 1.01 files/s | ETA≈143.0m | docs_kept=1979\n",
            "[2016] 200/8774 | 0.81 files/s | ETA≈177.0m | docs_kept=5754\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 300/8774 | 0.81 files/s | ETA≈174.0m | docs_kept=8347\n",
            "[2016] 400/8774 | 0.79 files/s | ETA≈175.6m | docs_kept=11532\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 500/8774 | 0.77 files/s | ETA≈178.2m | docs_kept=15409\n",
            "[2016] 600/8774 | 0.78 files/s | ETA≈175.0m | docs_kept=17834\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 700/8774 | 0.76 files/s | ETA≈177.8m | docs_kept=21314\n",
            "[2016] 800/8774 | 0.76 files/s | ETA≈174.1m | docs_kept=23675\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 900/8774 | 0.76 files/s | ETA≈173.5m | docs_kept=26735\n",
            "[2016] 1000/8774 | 0.75 files/s | ETA≈173.5m | docs_kept=30111\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 1100/8774 | 0.75 files/s | ETA≈171.0m | docs_kept=32463\n",
            "[2016] 1200/8774 | 0.73 files/s | ETA≈172.0m | docs_kept=35882\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 1300/8774 | 0.74 files/s | ETA≈169.2m | docs_kept=38241\n",
            "[2016] 1400/8774 | 0.73 files/s | ETA≈167.8m | docs_kept=41194\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 1500/8774 | 0.73 files/s | ETA≈166.6m | docs_kept=44244\n",
            "[2016] 1600/8774 | 0.73 files/s | ETA≈163.8m | docs_kept=46685\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 1700/8774 | 0.72 files/s | ETA≈163.3m | docs_kept=50293\n",
            "[2016] 1800/8774 | 0.72 files/s | ETA≈160.6m | docs_kept=52997\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 1900/8774 | 0.72 files/s | ETA≈158.4m | docs_kept=55915\n",
            "[2016] 2000/8774 | 0.72 files/s | ETA≈156.5m | docs_kept=59702\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 2100/8774 | 0.73 files/s | ETA≈153.2m | docs_kept=62229\n",
            "[2016] 2200/8774 | 0.72 files/s | ETA≈152.1m | docs_kept=65871\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 2300/8774 | 0.72 files/s | ETA≈149.4m | docs_kept=68445\n",
            "[2016] 2400/8774 | 0.72 files/s | ETA≈147.4m | docs_kept=71685\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 2500/8774 | 0.72 files/s | ETA≈145.3m | docs_kept=74955\n",
            "[2016] 2600/8774 | 0.72 files/s | ETA≈142.5m | docs_kept=77366\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 2700/8774 | 0.71 files/s | ETA≈141.6m | docs_kept=82160\n",
            "[2016] 2800/8774 | 0.71 files/s | ETA≈139.7m | docs_kept=87142\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 2900/8774 | 0.71 files/s | ETA≈138.8m | docs_kept=91064\n",
            "[2016] 3000/8774 | 0.70 files/s | ETA≈137.2m | docs_kept=94906\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 3100/8774 | 0.70 files/s | ETA≈135.2m | docs_kept=98332\n",
            "[2016] 3200/8774 | 0.69 files/s | ETA≈134.2m | docs_kept=103206\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 3300/8774 | 0.69 files/s | ETA≈131.7m | docs_kept=106620\n",
            "[2016] 3400/8774 | 0.69 files/s | ETA≈130.3m | docs_kept=111456\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 3500/8774 | 0.69 files/s | ETA≈128.3m | docs_kept=115542\n",
            "[2016] 3600/8774 | 0.69 files/s | ETA≈125.8m | docs_kept=118751\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 3700/8774 | 0.68 files/s | ETA≈123.9m | docs_kept=122542\n",
            "[2016] 3800/8774 | 0.68 files/s | ETA≈121.5m | docs_kept=126068\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 3900/8774 | 0.68 files/s | ETA≈119.7m | docs_kept=130717\n",
            "[2016] 4000/8774 | 0.68 files/s | ETA≈117.5m | docs_kept=134444\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 4100/8774 | 0.68 files/s | ETA≈115.0m | docs_kept=138697\n",
            "[2016] 4200/8774 | 0.67 files/s | ETA≈113.1m | docs_kept=144426\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 4300/8774 | 0.67 files/s | ETA≈110.5m | docs_kept=151690\n",
            "[2016] 4400/8774 | 0.67 files/s | ETA≈108.6m | docs_kept=158768\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 4500/8774 | 0.67 files/s | ETA≈105.9m | docs_kept=163607\n",
            "[2016] 4600/8774 | 0.67 files/s | ETA≈103.5m | docs_kept=168928\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 4700/8774 | 0.67 files/s | ETA≈101.5m | docs_kept=173977\n",
            "[2016] 4800/8774 | 0.67 files/s | ETA≈99.0m | docs_kept=177694\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 4900/8774 | 0.67 files/s | ETA≈96.8m | docs_kept=182611\n",
            "[2016] 5000/8774 | 0.67 files/s | ETA≈94.3m | docs_kept=186467\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 5100/8774 | 0.67 files/s | ETA≈91.8m | docs_kept=189962\n",
            "[2016] 5200/8774 | 0.67 files/s | ETA≈89.5m | docs_kept=194476\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 5300/8774 | 0.67 files/s | ETA≈86.8m | docs_kept=197418\n",
            "[2016] 5400/8774 | 0.67 files/s | ETA≈84.5m | docs_kept=201573\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 5500/8774 | 0.67 files/s | ETA≈81.9m | docs_kept=204535\n",
            "[2016] 5600/8774 | 0.67 files/s | ETA≈79.5m | docs_kept=208245\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 5700/8774 | 0.66 files/s | ETA≈77.2m | docs_kept=213437\n",
            "[2016] 5800/8774 | 0.66 files/s | ETA≈74.7m | docs_kept=216473\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 5900/8774 | 0.66 files/s | ETA≈72.4m | docs_kept=220887\n",
            "[2016] 6000/8774 | 0.66 files/s | ETA≈69.7m | docs_kept=224555\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 6100/8774 | 0.66 files/s | ETA≈67.2m | docs_kept=228146\n",
            "[2016] 6200/8774 | 0.66 files/s | ETA≈64.8m | docs_kept=232127\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 6300/8774 | 0.66 files/s | ETA≈62.2m | docs_kept=235860\n",
            "[2016] 6400/8774 | 0.66 files/s | ETA≈59.9m | docs_kept=240982\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 6500/8774 | 0.66 files/s | ETA≈57.3m | docs_kept=244255\n",
            "[2016] 6600/8774 | 0.66 files/s | ETA≈54.8m | docs_kept=248275\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 6700/8774 | 0.66 files/s | ETA≈52.4m | docs_kept=252767\n",
            "[2016] 6800/8774 | 0.66 files/s | ETA≈49.8m | docs_kept=256031\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 6900/8774 | 0.66 files/s | ETA≈47.4m | docs_kept=260466\n",
            "[2016] 7000/8774 | 0.66 files/s | ETA≈44.8m | docs_kept=264321\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 7100/8774 | 0.66 files/s | ETA≈42.3m | docs_kept=268410\n",
            "[2016] 7200/8774 | 0.66 files/s | ETA≈39.8m | docs_kept=272137\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 7300/8774 | 0.66 files/s | ETA≈37.3m | docs_kept=275662\n",
            "[2016] 7400/8774 | 0.66 files/s | ETA≈34.9m | docs_kept=281040\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 7500/8774 | 0.66 files/s | ETA≈32.3m | docs_kept=285315\n",
            "[2016] 7600/8774 | 0.66 files/s | ETA≈29.8m | docs_kept=292463\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 7700/8774 | 0.66 files/s | ETA≈27.3m | docs_kept=296925\n",
            "[2016] 7800/8774 | 0.66 files/s | ETA≈24.7m | docs_kept=300897\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 7900/8774 | 0.66 files/s | ETA≈22.2m | docs_kept=305970\n",
            "[2016] 8000/8774 | 0.66 files/s | ETA≈19.6m | docs_kept=309538\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 8100/8774 | 0.66 files/s | ETA≈17.1m | docs_kept=314247\n",
            "[2016] 8200/8774 | 0.66 files/s | ETA≈14.6m | docs_kept=318564\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 8300/8774 | 0.66 files/s | ETA≈12.0m | docs_kept=322007\n",
            "[2016] 8400/8774 | 0.66 files/s | ETA≈9.5m | docs_kept=326571\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 8500/8774 | 0.66 files/s | ETA≈7.0m | docs_kept=329490\n",
            "[2016] 8600/8774 | 0.66 files/s | ETA≈4.4m | docs_kept=333839\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] 8700/8774 | 0.66 files/s | ETA≈1.9m | docs_kept=336586\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2016/gdelt_uk_housing_monthly_2016.csv\n",
            "[2016] Done. This session processed: 8,774 | Total processed so far: 8,723 | Docs kept: 339,512\n",
            "Appended to master: /content/drive/MyDrive/msc_project/gdelt/gdelt_uk_housing_monthly_STITCHING.csv (23, 3)\n",
            "     Month  Docs  AvgTone\n",
            "2016-01-01 22027   -1.389\n",
            "2016-02-01 20296   -1.338\n",
            "2016-03-01 22789   -1.673\n",
            "2016-04-01 25980   -0.962\n",
            "2016-05-01 29200   -1.126\n",
            "2016-06-01 36768   -1.287\n",
            "2016-07-01 32968   -1.572\n",
            "2016-08-01 28466   -1.123\n",
            "2016-09-01 29185   -1.016\n",
            "2016-10-01 28596   -1.091\n",
            "2016-11-01 34808   -0.758\n",
            "2016-12-01 28429   -1.180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2017"
      ],
      "metadata": {
        "id": "04Oub01iq8lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================= GDELT GKG (UK housing) → monthly (docs + avg tone) =========================\n",
        "# Runs year-by-year. Set YEAR below (e.g., 2017). Saves to Google Drive and appends to a master CSV.\n",
        "\n",
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import re, io, zipfile, math, time, json, logging, requests, pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "# ------------------------ CONFIG ------------------------\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")  # change if you like\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "YEAR = 2017                     # <<< set the year you want to process\n",
        "RESET_STATE = True              # <<< set True if you previously ran the wrong year; else False\n",
        "MAX_FILES = None                # cap files processed this run; None = process all remaining\n",
        "HOURLY_ONLY = True              # process top-of-hour files only (lighter than every 15 minutes)\n",
        "CHECKPOINT_EVERY = 200\n",
        "PROGRESS_EVERY   = 100\n",
        "\n",
        "# Date window from YEAR (dynamic!)\n",
        "START_TS = f\"{YEAR}0101000000\"  # YYYY-01-01 00:00:00\n",
        "END_TS   = f\"{YEAR}1231235959\"  # YYYY-12-31 23:59:59\n",
        "# Filters\n",
        "UK_REGEX = r'#(?:GBR|GB|UK|United Kingdom|England|Scotland|Wales|Northern Ireland)#'\n",
        "THEME_PATTERN = r'(?:HOUS|MORTGAG|REMORTGAG|RENT|TENAN|REAL[ _]?ESTATE|PROPERTY|HOME[ _]?PRIC|HOUSE[ _]?PRIC|RIGHTMOVE|ZOOPLA|LANDLORD|BUY[ -]?TO[ -]?LET)'\n",
        "\n",
        "# GKG v2.1 columns (0-based indices)\n",
        "DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I = 1, 8, 10, 15\n",
        "\n",
        "# Paths\n",
        "OUT_DIR = BASE_DIR / f\"gdelt_run_{YEAR}\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "YEARLY_CSV = OUT_DIR / f\"gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "STATE_PATH = OUT_DIR / \"state.json\"\n",
        "MASTER_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
        "MASTER_CSV = BASE_DIR / \"gdelt_uk_housing_monthly_STITCHING.csv\"\n",
        "\n",
        "# ------------------------ Networking helpers ------------------------\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-colab/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def get(url, timeout=60, s=None):\n",
        "    s = s or session()\n",
        "    if url.startswith(\"https://\"):  # avoid SSL hiccups in Colab\n",
        "        url = \"http://\" + url[len(\"https://\"):]\n",
        "    r = s.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "# ------------------------ GDELT plumbing ------------------------\n",
        "def build_gkg_urls():\n",
        "    \"\"\"Collect GKG v2.1 .zip URLs within the YEAR bounds (top-of-hour if HOURLY_ONLY).\"\"\"\n",
        "    txt = get(MASTER_URL, timeout=90).text\n",
        "    urls = []\n",
        "    for line in txt.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3:\n",
        "            continue\n",
        "        url = parts[2]\n",
        "        m = re.search(r'/gdeltv2/(\\d{14})\\.gkg\\.csv\\.zip$', url)\n",
        "        if not m:\n",
        "            continue\n",
        "        ts = m.group(1)\n",
        "        if START_TS <= ts <= END_TS:\n",
        "            if HOURLY_ONLY and ts[10:12] != \"00\":  # only top-of-hour files\n",
        "                continue\n",
        "            urls.append(url.replace(\"https://\", \"http://\"))\n",
        "    urls.sort()\n",
        "    return urls\n",
        "\n",
        "def month_from_dateint(dateint):\n",
        "    dt = datetime.strptime(str(dateint), \"%Y%m%d%H%M%S\")\n",
        "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "\n",
        "def doc_tone(v2tone):\n",
        "    try:\n",
        "        # V2Tone = \"docTone, pos, neg, polarity, activityRefDensity, selfGroupRefDensity, wordCount\"\n",
        "        return float(str(v2tone).split(\",\")[0])\n",
        "    except Exception:\n",
        "        return math.nan\n",
        "\n",
        "def process_df(df, counts, tone_sum, tone_n, uk_re, theme_re):\n",
        "    m_uk = df[V2LOCS_I].astype(str).str.contains(uk_re, na=False, regex=True)\n",
        "    m_th = df[V2THEMES_I].astype(str).str.contains(theme_re, na=False, regex=True)\n",
        "    sub = df.loc[m_uk & m_th, [DATE_I, V2TONE_I]]\n",
        "    if sub.empty:\n",
        "        return 0\n",
        "    months = sub[DATE_I].astype(str).apply(month_from_dateint)\n",
        "    tones  = sub[V2TONE_I].apply(doc_tone)\n",
        "    kept = 0\n",
        "    for m, t in zip(months, tones):\n",
        "        counts[m] += 1\n",
        "        if not math.isnan(t):\n",
        "            tone_sum[m] += t\n",
        "            tone_n[m] += 1\n",
        "        kept += 1\n",
        "    return kept\n",
        "\n",
        "def write_year_output(processed_count, processed_tail, counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m]/tone_n[m]) if tone_n[m] else None\n",
        "        rows.append({\"month\": f\"{m}-01\", \"docs\": n, \"avg_tone\": avg})\n",
        "    monthly = pd.DataFrame(rows, columns=[\"month\",\"docs\",\"avg_tone\"]).sort_values(\"month\")\n",
        "    monthly.to_csv(YEARLY_CSV, index=False)\n",
        "    json.dump({\n",
        "        \"processed_count\": processed_count,\n",
        "        \"processed_urls_tail\": processed_tail[-20000:],  # retain a long tail for resume\n",
        "        \"counts\": counts, \"tone_sum\": tone_sum, \"tone_n\": tone_n\n",
        "    }, open(STATE_PATH, \"w\"))\n",
        "    print(f\"Saved → {YEARLY_CSV}\")\n",
        "\n",
        "# ------------------------ Reset (if previously wrong year) ------------------------\n",
        "if RESET_STATE:\n",
        "    if STATE_PATH.exists():\n",
        "        STATE_PATH.unlink()\n",
        "    if YEARLY_CSV.exists():\n",
        "        YEARLY_CSV.unlink()\n",
        "\n",
        "# ------------------------ Plan + resume ------------------------\n",
        "s = session()\n",
        "urls_all = build_gkg_urls()\n",
        "print(f\"[{YEAR}] planned hourly files: {len(urls_all):,}\")\n",
        "\n",
        "if STATE_PATH.exists():\n",
        "    state = json.load(open(STATE_PATH))\n",
        "    processed_count     = int(state.get(\"processed_count\", 0))\n",
        "    processed_urls_tail = state.get(\"processed_urls_tail\", [])\n",
        "    counts   = defaultdict(int,   state.get(\"counts\", {}))\n",
        "    tone_sum = defaultdict(float, state.get(\"tone_sum\", {}))\n",
        "    tone_n   = defaultdict(int,   state.get(\"tone_n\", {}))\n",
        "    print(f\"Resuming: {processed_count:,} files done previously.\")\n",
        "else:\n",
        "    processed_count, processed_urls_tail = 0, []\n",
        "    counts, tone_sum, tone_n = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "already = set(processed_urls_tail)\n",
        "todo = [u for u in urls_all if u not in already]\n",
        "if MAX_FILES is not None:\n",
        "    todo = todo[:MAX_FILES]\n",
        "print(f\"[{YEAR}] processing this session: {len(todo):,} files\")\n",
        "\n",
        "uk_re = re.compile(UK_REGEX, re.I)\n",
        "theme_re = re.compile(THEME_PATTERN, re.I)\n",
        "\n",
        "# ------------------------ Main loop ------------------------\n",
        "t0 = time.time()\n",
        "docs_seen = 0\n",
        "\n",
        "for i, url in enumerate(todo, 1):\n",
        "    try:\n",
        "        z = zipfile.ZipFile(io.BytesIO(get(url, timeout=90, s=s).content))\n",
        "        name = z.namelist()[0]\n",
        "        df = pd.read_csv(\n",
        "            z.open(name),\n",
        "            sep=\"\\t\", header=None, quoting=3, low_memory=False,\n",
        "            usecols=[DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I],\n",
        "            dtype={DATE_I:str, V2THEMES_I:str, V2LOCS_I:str, V2TONE_I:str}\n",
        "        )\n",
        "        docs_seen += process_df(df, counts, tone_sum, tone_n, uk_re, theme_re)\n",
        "        processed_count += 1\n",
        "        processed_urls_tail.append(url)\n",
        "        if len(processed_urls_tail) > 20000:\n",
        "            processed_urls_tail = processed_urls_tail[-20000:]\n",
        "    except Exception:\n",
        "        # skip bad zips quietly\n",
        "        pass\n",
        "\n",
        "    if i % PROGRESS_EVERY == 0:\n",
        "        elapsed = max(1e-6, time.time()-t0)\n",
        "        rate = i / elapsed\n",
        "        eta_min = (len(todo)-i) / max(1e-6, rate) / 60\n",
        "        print(f\"[{YEAR}] {i}/{len(todo)} | {rate:.2f} files/s | ETA≈{eta_min:.1f}m | docs_kept={docs_seen:,}\")\n",
        "\n",
        "    if i % CHECKPOINT_EVERY == 0:\n",
        "        write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "\n",
        "# final write for the year\n",
        "write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "print(f\"[{YEAR}] Done. This session processed: {len(todo):,} | Total processed so far: {processed_count:,} | Docs kept: {docs_seen:,}\")\n",
        "\n",
        "# ------------------------ Append to master ------------------------\n",
        "try:\n",
        "    ydf = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # sanity: ensure we really wrote YEAR\n",
        "    years_found = set(ydf[\"month\"].dt.year.dropna().unique().tolist())\n",
        "    if years_found != {YEAR}:\n",
        "        raise ValueError(f\"Year mismatch: expected {YEAR}, found {sorted(years_found)} in {YEARLY_CSV}\")\n",
        "\n",
        "    if MASTER_CSV.exists():\n",
        "        m = pd.read_csv(MASTER_CSV, parse_dates=[\"month\"])\n",
        "        m = (pd.concat([m, ydf], ignore_index=True)\n",
        "               .drop_duplicates(subset=\"month\", keep=\"last\")\n",
        "               .sort_values(\"month\")\n",
        "               .reset_index(drop=True))\n",
        "    else:\n",
        "        m = ydf\n",
        "    m.to_csv(MASTER_CSV, index=False)\n",
        "    print(\"Appended to master:\", MASTER_CSV, m.shape)\n",
        "except Exception as e:\n",
        "    print(\"Append-to-master skipped:\", e)\n",
        "\n",
        "# ------------------------ Pretty print monthly for YEAR ------------------------\n",
        "try:\n",
        "    df = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # build a complete monthly index for YEAR so missing months show as NaN\n",
        "    full = (df.set_index(\"month\")\n",
        "              .reindex(pd.date_range(f\"{YEAR}-01-01\", f\"{YEAR}-12-01\", freq=\"MS\"))\n",
        "              .rename_axis(\"Month\").reset_index())\n",
        "    full[\"Docs\"]    = pd.to_numeric(full[\"docs\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "    full[\"AvgTone\"] = pd.to_numeric(full[\"avg_tone\"], errors=\"coerce\")\n",
        "    print(full[[\"Month\",\"Docs\",\"AvgTone\"]].to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Preview skipped:\", e)\n",
        "# ========================= end =========================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO1Oyr88hRJm",
        "outputId": "0fbf8360-21ab-432d-edc3-eea9bcdd0105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[2017] planned hourly files: 8,503\n",
            "[2017] processing this session: 8,503 files\n",
            "[2017] 100/8503 | 0.78 files/s | ETA≈180.2m | docs_kept=3,422\n",
            "[2017] 200/8503 | 0.76 files/s | ETA≈182.9m | docs_kept=7,183\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 300/8503 | 0.69 files/s | ETA≈197.5m | docs_kept=12,752\n",
            "[2017] 400/8503 | 0.70 files/s | ETA≈191.6m | docs_kept=17,049\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 500/8503 | 0.67 files/s | ETA≈198.1m | docs_kept=24,366\n",
            "[2017] 600/8503 | 0.67 files/s | ETA≈197.4m | docs_kept=31,267\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 700/8503 | 0.67 files/s | ETA≈194.4m | docs_kept=38,433\n",
            "[2017] 800/8503 | 0.65 files/s | ETA≈197.8m | docs_kept=45,617\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 900/8503 | 0.66 files/s | ETA≈193.1m | docs_kept=49,991\n",
            "[2017] 1000/8503 | 0.65 files/s | ETA≈193.2m | docs_kept=55,000\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 1100/8503 | 0.65 files/s | ETA≈190.1m | docs_kept=59,500\n",
            "[2017] 1200/8503 | 0.65 files/s | ETA≈186.7m | docs_kept=63,385\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 1300/8503 | 0.65 files/s | ETA≈185.6m | docs_kept=68,459\n",
            "[2017] 1400/8503 | 0.65 files/s | ETA≈181.5m | docs_kept=72,282\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 1500/8503 | 0.65 files/s | ETA≈180.4m | docs_kept=77,627\n",
            "[2017] 1600/8503 | 0.65 files/s | ETA≈176.8m | docs_kept=82,128\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 1700/8503 | 0.65 files/s | ETA≈173.6m | docs_kept=86,157\n",
            "[2017] 1800/8503 | 0.65 files/s | ETA≈171.6m | docs_kept=90,851\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 1900/8503 | 0.66 files/s | ETA≈167.4m | docs_kept=96,186\n",
            "[2017] 2000/8503 | 0.65 files/s | ETA≈165.5m | docs_kept=104,438\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 2100/8503 | 0.66 files/s | ETA≈162.0m | docs_kept=109,371\n",
            "[2017] 2200/8503 | 0.66 files/s | ETA≈158.3m | docs_kept=113,757\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 2300/8503 | 0.66 files/s | ETA≈156.5m | docs_kept=119,387\n",
            "[2017] 2400/8503 | 0.67 files/s | ETA≈152.8m | docs_kept=124,151\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 2500/8503 | 0.67 files/s | ETA≈150.2m | docs_kept=128,043\n",
            "[2017] 2600/8503 | 0.67 files/s | ETA≈146.6m | docs_kept=130,973\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 2700/8503 | 0.67 files/s | ETA≈143.6m | docs_kept=135,171\n",
            "[2017] 2800/8503 | 0.67 files/s | ETA≈141.5m | docs_kept=140,058\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 2900/8503 | 0.68 files/s | ETA≈138.1m | docs_kept=142,970\n",
            "[2017] 3000/8503 | 0.68 files/s | ETA≈135.6m | docs_kept=146,053\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 3100/8503 | 0.68 files/s | ETA≈132.7m | docs_kept=149,598\n",
            "[2017] 3200/8503 | 0.68 files/s | ETA≈129.6m | docs_kept=153,581\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 3300/8503 | 0.68 files/s | ETA≈127.5m | docs_kept=159,675\n",
            "[2017] 3400/8503 | 0.68 files/s | ETA≈124.3m | docs_kept=164,063\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 3500/8503 | 0.68 files/s | ETA≈121.9m | docs_kept=171,967\n",
            "[2017] 3600/8503 | 0.69 files/s | ETA≈118.8m | docs_kept=175,731\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 3700/8503 | 0.69 files/s | ETA≈115.8m | docs_kept=182,759\n",
            "[2017] 3800/8503 | 0.69 files/s | ETA≈113.7m | docs_kept=189,857\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 3900/8503 | 0.69 files/s | ETA≈110.7m | docs_kept=193,048\n",
            "[2017] 4000/8503 | 0.69 files/s | ETA≈108.1m | docs_kept=197,741\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 4100/8503 | 0.69 files/s | ETA≈105.6m | docs_kept=201,992\n",
            "[2017] 4200/8503 | 0.70 files/s | ETA≈102.8m | docs_kept=205,807\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 4300/8503 | 0.70 files/s | ETA≈100.2m | docs_kept=209,794\n",
            "[2017] 4400/8503 | 0.71 files/s | ETA≈96.9m | docs_kept=212,459\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 4500/8503 | 0.71 files/s | ETA≈93.9m | docs_kept=215,706\n",
            "[2017] 4600/8503 | 0.71 files/s | ETA≈91.5m | docs_kept=219,934\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 4700/8503 | 0.71 files/s | ETA≈88.7m | docs_kept=222,556\n",
            "[2017] 4800/8503 | 0.72 files/s | ETA≈86.0m | docs_kept=225,397\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 4900/8503 | 0.72 files/s | ETA≈83.5m | docs_kept=228,462\n",
            "[2017] 5000/8503 | 0.72 files/s | ETA≈80.8m | docs_kept=231,106\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 5100/8503 | 0.72 files/s | ETA≈78.4m | docs_kept=234,608\n",
            "[2017] 5200/8503 | 0.73 files/s | ETA≈75.8m | docs_kept=237,362\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 5300/8503 | 0.73 files/s | ETA≈73.2m | docs_kept=240,195\n",
            "[2017] 5400/8503 | 0.73 files/s | ETA≈70.8m | docs_kept=243,578\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 5500/8503 | 0.73 files/s | ETA≈68.3m | docs_kept=246,616\n",
            "[2017] 5600/8503 | 0.73 files/s | ETA≈66.0m | docs_kept=249,711\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 5700/8503 | 0.73 files/s | ETA≈63.6m | docs_kept=252,132\n",
            "[2017] 5800/8503 | 0.74 files/s | ETA≈61.2m | docs_kept=254,939\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 5900/8503 | 0.74 files/s | ETA≈58.8m | docs_kept=258,728\n",
            "[2017] 6000/8503 | 0.74 files/s | ETA≈56.4m | docs_kept=262,204\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 6100/8503 | 0.74 files/s | ETA≈54.1m | docs_kept=266,819\n",
            "[2017] 6200/8503 | 0.74 files/s | ETA≈51.8m | docs_kept=271,840\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 6300/8503 | 0.74 files/s | ETA≈49.4m | docs_kept=275,099\n",
            "[2017] 6400/8503 | 0.74 files/s | ETA≈47.1m | docs_kept=278,403\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 6500/8503 | 0.75 files/s | ETA≈44.7m | docs_kept=281,234\n",
            "[2017] 6600/8503 | 0.75 files/s | ETA≈42.3m | docs_kept=284,225\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 6700/8503 | 0.75 files/s | ETA≈40.1m | docs_kept=287,926\n",
            "[2017] 6800/8503 | 0.75 files/s | ETA≈37.8m | docs_kept=291,802\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 6900/8503 | 0.75 files/s | ETA≈35.6m | docs_kept=295,100\n",
            "[2017] 7000/8503 | 0.75 files/s | ETA≈33.3m | docs_kept=298,075\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 7100/8503 | 0.75 files/s | ETA≈31.0m | docs_kept=301,250\n",
            "[2017] 7200/8503 | 0.75 files/s | ETA≈28.9m | docs_kept=305,965\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 7300/8503 | 0.75 files/s | ETA≈26.6m | docs_kept=308,870\n",
            "[2017] 7400/8503 | 0.75 files/s | ETA≈24.5m | docs_kept=312,406\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 7500/8503 | 0.75 files/s | ETA≈22.2m | docs_kept=315,346\n",
            "[2017] 7600/8503 | 0.75 files/s | ETA≈20.0m | docs_kept=318,455\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 7700/8503 | 0.75 files/s | ETA≈17.8m | docs_kept=322,404\n",
            "[2017] 7800/8503 | 0.75 files/s | ETA≈15.5m | docs_kept=326,187\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 7900/8503 | 0.75 files/s | ETA≈13.3m | docs_kept=330,125\n",
            "[2017] 8000/8503 | 0.75 files/s | ETA≈11.1m | docs_kept=334,692\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 8100/8503 | 0.75 files/s | ETA≈8.9m | docs_kept=337,934\n",
            "[2017] 8200/8503 | 0.75 files/s | ETA≈6.7m | docs_kept=341,035\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 8300/8503 | 0.75 files/s | ETA≈4.5m | docs_kept=345,552\n",
            "[2017] 8400/8503 | 0.76 files/s | ETA≈2.3m | docs_kept=346,925\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] 8500/8503 | 0.76 files/s | ETA≈0.1m | docs_kept=349,014\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2017/gdelt_uk_housing_monthly_2017.csv\n",
            "[2017] Done. This session processed: 8,503 | Total processed so far: 8,475 | Docs kept: 349,060\n",
            "Appended to master: /content/drive/MyDrive/msc_project/gdelt/gdelt_uk_housing_monthly_STITCHING.csv (35, 3)\n",
            "     Month  Docs   AvgTone\n",
            "2017-01-01 42207 -1.250740\n",
            "2017-02-01 31044 -1.223141\n",
            "2017-03-01 39557 -1.559904\n",
            "2017-04-01 29243 -1.860913\n",
            "2017-05-01 33502 -1.969347\n",
            "2017-06-01 34080 -2.154123\n",
            "2017-07-01 21622 -1.268047\n",
            "2017-08-01 21880 -1.669011\n",
            "2017-09-01 25820 -2.080702\n",
            "2017-10-01 24245 -1.284773\n",
            "2017-11-01 24756 -1.270683\n",
            "2017-12-01 21104 -1.267863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2018"
      ],
      "metadata": {
        "id": "A7yyKxDKJrKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GDELT UK housing monthly for a single year (no auto-append to master) ---\n",
        "!pip -q install pandas requests\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import re, io, zipfile, math, time, json, logging, requests, pandas as pd\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "YEAR = 2018\n",
        "RESET_STATE = True\n",
        "MAX_FILES = None\n",
        "HOURLY_ONLY = True\n",
        "CHECKPOINT_EVERY = 200\n",
        "PROGRESS_EVERY   = 100\n",
        "APPEND_TO_MASTER = False\n",
        "\n",
        "START_TS = f\"{YEAR}0101000000\"\n",
        "END_TS   = f\"{YEAR}1231235959\"\n",
        "\n",
        "UK_REGEX = r'#(?:GBR|GB|UK|United Kingdom|England|Scotland|Wales|Northern Ireland)#'\n",
        "THEME_PATTERN = r'(?:HOUS|MORTGAG|REMORTGAG|RENT|TENAN|REAL[ _]?ESTATE|PROPERTY|HOME[ _]?PRIC|HOUSE[ _]?PRIC|RIGHTMOVE|ZOOPLA|LANDLORD|BUY[ -]?TO[ -]?LET)'\n",
        "\n",
        "DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I = 1, 8, 10, 15\n",
        "\n",
        "OUT_DIR = BASE_DIR / f\"gdelt_run_{YEAR}\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "YEARLY_CSV = OUT_DIR / f\"gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "STATE_PATH = OUT_DIR / \"state.json\"\n",
        "MASTER_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
        "\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-colab/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def get(url, timeout=60, s=None):\n",
        "    s = s or session()\n",
        "    if url.startswith(\"https://\"):\n",
        "        url = \"http://\" + url[len(\"https://\"):]\n",
        "    r = s.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "def build_gkg_urls():\n",
        "    txt = get(MASTER_URL, timeout=90).text\n",
        "    urls = []\n",
        "    for line in txt.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3: continue\n",
        "        url = parts[2]\n",
        "        m = re.search(r'/gdeltv2/(\\d{14})\\.gkg\\.csv\\.zip$', url)\n",
        "        if not m: continue\n",
        "        ts = m.group(1)\n",
        "        if START_TS <= ts <= END_TS:\n",
        "            if HOURLY_ONLY and ts[10:12] != \"00\":  # top-of-hour only\n",
        "                continue\n",
        "            urls.append(url.replace(\"https://\",\"http://\"))\n",
        "    urls.sort()\n",
        "    return urls\n",
        "\n",
        "def month_from_dateint(dateint):\n",
        "    dt = datetime.strptime(str(dateint), \"%Y%m%d%H%M%S\")\n",
        "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "\n",
        "def doc_tone(v2tone):\n",
        "    try:\n",
        "        return float(str(v2tone).split(\",\")[0])\n",
        "    except Exception:\n",
        "        return math.nan\n",
        "\n",
        "def process_df(df, counts, tone_sum, tone_n, uk_re, theme_re):\n",
        "    m_uk = df[V2LOCS_I].astype(str).str.contains(uk_re, na=False, regex=True)\n",
        "    m_th = df[V2THEMES_I].astype(str).str.contains(theme_re, na=False, regex=True)\n",
        "    sub = df.loc[m_uk & m_th, [DATE_I, V2TONE_I]]\n",
        "    if sub.empty: return 0\n",
        "    months = sub[DATE_I].astype(str).apply(month_from_dateint)\n",
        "    tones  = sub[V2TONE_I].apply(doc_tone)\n",
        "    kept = 0\n",
        "    for m, t in zip(months, tones):\n",
        "        counts[m] += 1\n",
        "        if not math.isnan(t):\n",
        "            tone_sum[m] += t\n",
        "            tone_n[m] += 1\n",
        "        kept += 1\n",
        "    return kept\n",
        "\n",
        "def write_year_output(processed_count, processed_tail, counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m]/tone_n[m]) if tone_n[m] else None\n",
        "        rows.append({\"month\": f\"{m}-01\", \"docs\": n, \"avg_tone\": avg})\n",
        "    monthly = pd.DataFrame(rows, columns=[\"month\",\"docs\",\"avg_tone\"]).sort_values(\"month\")\n",
        "    monthly.to_csv(YEARLY_CSV, index=False)\n",
        "    json.dump({\n",
        "        \"processed_count\": processed_count,\n",
        "        \"processed_urls_tail\": processed_tail[-20000:],\n",
        "        \"counts\": counts, \"tone_sum\": tone_sum, \"tone_n\": tone_n\n",
        "    }, open(STATE_PATH, \"w\"))\n",
        "    print(f\"Saved → {YEARLY_CSV}\")\n",
        "\n",
        "# Reset per-year state if needed\n",
        "if RESET_STATE:\n",
        "    if STATE_PATH.exists(): STATE_PATH.unlink()\n",
        "    if YEARLY_CSV.exists(): YEARLY_CSV.unlink()\n",
        "\n",
        "# Plan + resume\n",
        "s = session()\n",
        "urls_all = build_gkg_urls()\n",
        "print(f\"[{YEAR}] planned hourly files: {len(urls_all):,}\")\n",
        "\n",
        "if STATE_PATH.exists():\n",
        "    state = json.load(open(STATE_PATH))\n",
        "    processed_count     = int(state.get(\"processed_count\", 0))\n",
        "    processed_urls_tail = state.get(\"processed_urls_tail\", [])\n",
        "    counts   = defaultdict(int,   state.get(\"counts\", {}))\n",
        "    tone_sum = defaultdict(float, state.get(\"tone_sum\", {}))\n",
        "    tone_n   = defaultdict(int,   state.get(\"tone_n\", {}))\n",
        "    print(f\"Resuming: {processed_count:,} files done previously.\")\n",
        "else:\n",
        "    processed_count, processed_urls_tail = 0, []\n",
        "    counts, tone_sum, tone_n = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "already = set(processed_urls_tail)\n",
        "todo = [u for u in urls_all if u not in already]\n",
        "if MAX_FILES is not None:\n",
        "    todo = todo[:MAX_FILES]\n",
        "print(f\"[{YEAR}] processing this session: {len(todo):,} files\")\n",
        "\n",
        "uk_re = re.compile(UK_REGEX, re.I)\n",
        "theme_re = re.compile(THEME_PATTERN, re.I)\n",
        "\n",
        "# Main loop\n",
        "t0 = time.time()\n",
        "docs_seen = 0\n",
        "\n",
        "for i, url in enumerate(todo, 1):\n",
        "    try:\n",
        "        z = zipfile.ZipFile(io.BytesIO(get(url, timeout=90, s=s).content))\n",
        "        name = z.namelist()[0]\n",
        "        df = pd.read_csv(\n",
        "            z.open(name),\n",
        "            sep=\"\\t\", header=None, quoting=3, low_memory=False,\n",
        "            usecols=[DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I],\n",
        "            dtype={DATE_I:str, V2THEMES_I:str, V2LOCS_I:str, V2TONE_I:str}\n",
        "        )\n",
        "        docs_seen += process_df(df, counts, tone_sum, tone_n, uk_re, theme_re)\n",
        "        processed_count += 1\n",
        "        processed_urls_tail.append(url)\n",
        "        if len(processed_urls_tail) > 20000:\n",
        "            processed_urls_tail = processed_urls_tail[-20000:]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if i % PROGRESS_EVERY == 0:\n",
        "        elapsed = max(1e-6, time.time()-t0)\n",
        "        rate = i / elapsed\n",
        "        eta_min = (len(todo)-i) / max(1e-6, rate) / 60\n",
        "        print(f\"[{YEAR}] {i}/{len(todo)} | {rate:.2f} files/s | ETA≈{eta_min:.1f}m | docs_kept={docs_seen:,}\")\n",
        "\n",
        "    if i % CHECKPOINT_EVERY == 0:\n",
        "        write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "\n",
        "# Final write for the year\n",
        "write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "print(f\"[{YEAR}] Done. Processed this session: {len(todo):,} | Total processed: {processed_count:,} | Docs kept: {docs_seen:,}\")\n",
        "\n",
        "# Quick print for the year\n",
        "df = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "full = (df.set_index(\"month\")\n",
        "          .reindex(pd.date_range(f\"{YEAR}-01-01\", f\"{YEAR}-12-01\", freq=\"MS\"))\n",
        "          .rename_axis(\"Month\").reset_index())\n",
        "full[\"Docs\"]    = pd.to_numeric(full[\"docs\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "full[\"AvgTone\"] = pd.to_numeric(full[\"avg_tone\"], errors=\"coerce\")\n",
        "print(full[[\"Month\",\"Docs\",\"AvgTone\"]].to_string(index=False))\n"
      ],
      "metadata": {
        "id": "bKIc6qCCKlEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2019"
      ],
      "metadata": {
        "id": "fAtfegHdJsYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import re, io, zipfile, math, time, json, logging, requests, pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "# ------------------------ CONFIG ------------------------\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")  # change if you like\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "YEAR = 2019\n",
        "RESET_STATE = True\n",
        "MAX_FILES = None\n",
        "HOURLY_ONLY = True\n",
        "CHECKPOINT_EVERY = 200\n",
        "PROGRESS_EVERY   = 100\n",
        "\n",
        "# Date window from YEAR (dynamic!)\n",
        "START_TS = f\"{YEAR}0101000000\"\n",
        "END_TS   = f\"{YEAR}1231235959\"\n",
        "# Filters\n",
        "UK_REGEX = r'#(?:GBR|GB|UK|United Kingdom|England|Scotland|Wales|Northern Ireland)#'\n",
        "THEME_PATTERN = r'(?:HOUS|MORTGAG|REMORTGAG|RENT|TENAN|REAL[ _]?ESTATE|PROPERTY|HOME[ _]?PRIC|HOUSE[ _]?PRIC|RIGHTMOVE|ZOOPLA|LANDLORD|BUY[ -]?TO[ -]?LET)'\n",
        "\n",
        "# GKG v2.1 columns (0-based indices)\n",
        "DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I = 1, 8, 10, 15\n",
        "\n",
        "# Paths\n",
        "OUT_DIR = BASE_DIR / f\"gdelt_run_{YEAR}\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "YEARLY_CSV = OUT_DIR / f\"gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "STATE_PATH = OUT_DIR / \"state.json\"\n",
        "MASTER_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
        "MASTER_CSV = BASE_DIR / \"gdelt_uk_housing_monthly_STITCHING.csv\"\n",
        "\n",
        "# ------------------------ Networking helpers ------------------------\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-colab/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def get(url, timeout=60, s=None):\n",
        "    s = s or session()\n",
        "    if url.startswith(\"https://\"):  # avoid SSL hiccups in Colab\n",
        "        url = \"http://\" + url[len(\"https://\"):]\n",
        "    r = s.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "# ------------------------ GDELT plumbing ------------------------\n",
        "def build_gkg_urls():\n",
        "    \"\"\"Collect GKG v2.1 .zip URLs within the YEAR bounds (top-of-hour if HOURLY_ONLY).\"\"\"\n",
        "    txt = get(MASTER_URL, timeout=90).text\n",
        "    urls = []\n",
        "    for line in txt.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3:\n",
        "            continue\n",
        "        url = parts[2]\n",
        "        m = re.search(r'/gdeltv2/(\\d{14})\\.gkg\\.csv\\.zip$', url)\n",
        "        if not m:\n",
        "            continue\n",
        "        ts = m.group(1)\n",
        "        if START_TS <= ts <= END_TS:\n",
        "            if HOURLY_ONLY and ts[10:12] != \"00\":  # only top-of-hour files\n",
        "                continue\n",
        "            urls.append(url.replace(\"https://\", \"http://\"))\n",
        "    urls.sort()\n",
        "    return urls\n",
        "\n",
        "def month_from_dateint(dateint):\n",
        "    dt = datetime.strptime(str(dateint), \"%Y%m%d%H%M%S\")\n",
        "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "\n",
        "def doc_tone(v2tone):\n",
        "    try:\n",
        "        # V2Tone = \"docTone, pos, neg, polarity, activityRefDensity, selfGroupRefDensity, wordCount\"\n",
        "        return float(str(v2tone).split(\",\")[0])\n",
        "    except Exception:\n",
        "        return math.nan\n",
        "\n",
        "def process_df(df, counts, tone_sum, tone_n, uk_re, theme_re):\n",
        "    m_uk = df[V2LOCS_I].astype(str).str.contains(uk_re, na=False, regex=True)\n",
        "    m_th = df[V2THEMES_I].astype(str).str.contains(theme_re, na=False, regex=True)\n",
        "    sub = df.loc[m_uk & m_th, [DATE_I, V2TONE_I]]\n",
        "    if sub.empty:\n",
        "        return 0\n",
        "    months = sub[DATE_I].astype(str).apply(month_from_dateint)\n",
        "    tones  = sub[V2TONE_I].apply(doc_tone)\n",
        "    kept = 0\n",
        "    for m, t in zip(months, tones):\n",
        "        counts[m] += 1\n",
        "        if not math.isnan(t):\n",
        "            tone_sum[m] += t\n",
        "            tone_n[m] += 1\n",
        "        kept += 1\n",
        "    return kept\n",
        "\n",
        "def write_year_output(processed_count, processed_tail, counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m]/tone_n[m]) if tone_n[m] else None\n",
        "        rows.append({\"month\": f\"{m}-01\", \"docs\": n, \"avg_tone\": avg})\n",
        "    monthly = pd.DataFrame(rows, columns=[\"month\",\"docs\",\"avg_tone\"]).sort_values(\"month\")\n",
        "    monthly.to_csv(YEARLY_CSV, index=False)\n",
        "    json.dump({\n",
        "        \"processed_count\": processed_count,\n",
        "        \"processed_urls_tail\": processed_tail[-20000:],  # retain a long tail for resume\n",
        "        \"counts\": counts, \"tone_sum\": tone_sum, \"tone_n\": tone_n\n",
        "    }, open(STATE_PATH, \"w\"))\n",
        "    print(f\"Saved → {YEARLY_CSV}\")\n",
        "\n",
        "# ------------------------ Reset (if previously wrong year) ------------------------\n",
        "if RESET_STATE:\n",
        "    if STATE_PATH.exists():\n",
        "        STATE_PATH.unlink()\n",
        "    if YEARLY_CSV.exists():\n",
        "        YEARLY_CSV.unlink()\n",
        "\n",
        "# ------------------------ Plan + resume ------------------------\n",
        "s = session()\n",
        "urls_all = build_gkg_urls()\n",
        "print(f\"[{YEAR}] planned hourly files: {len(urls_all):,}\")\n",
        "\n",
        "if STATE_PATH.exists():\n",
        "    state = json.load(open(STATE_PATH))\n",
        "    processed_count     = int(state.get(\"processed_count\", 0))\n",
        "    processed_urls_tail = state.get(\"processed_urls_tail\", [])\n",
        "    counts   = defaultdict(int,   state.get(\"counts\", {}))\n",
        "    tone_sum = defaultdict(float, state.get(\"tone_sum\", {}))\n",
        "    tone_n   = defaultdict(int,   state.get(\"tone_n\", {}))\n",
        "    print(f\"Resuming: {processed_count:,} files done previously.\")\n",
        "else:\n",
        "    processed_count, processed_urls_tail = 0, []\n",
        "    counts, tone_sum, tone_n = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "already = set(processed_urls_tail)\n",
        "todo = [u for u in urls_all if u not in already]\n",
        "if MAX_FILES is not None:\n",
        "    todo = todo[:MAX_FILES]\n",
        "print(f\"[{YEAR}] processing this session: {len(todo):,} files\")\n",
        "\n",
        "uk_re = re.compile(UK_REGEX, re.I)\n",
        "theme_re = re.compile(THEME_PATTERN, re.I)\n",
        "\n",
        "# ------------------------ Main loop ------------------------\n",
        "t0 = time.time()\n",
        "docs_seen = 0\n",
        "\n",
        "for i, url in enumerate(todo, 1):\n",
        "    try:\n",
        "        z = zipfile.ZipFile(io.BytesIO(get(url, timeout=90, s=s).content))\n",
        "        name = z.namelist()[0]\n",
        "        df = pd.read_csv(\n",
        "            z.open(name),\n",
        "            sep=\"\\t\", header=None, quoting=3, low_memory=False,\n",
        "            usecols=[DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I],\n",
        "            dtype={DATE_I:str, V2THEMES_I:str, V2LOCS_I:str, V2TONE_I:str}\n",
        "        )\n",
        "        docs_seen += process_df(df, counts, tone_sum, tone_n, uk_re, theme_re)\n",
        "        processed_count += 1\n",
        "        processed_urls_tail.append(url)\n",
        "        if len(processed_urls_tail) > 20000:\n",
        "            processed_urls_tail = processed_urls_tail[-20000:]\n",
        "    except Exception:\n",
        "        # skip bad zips quietly\n",
        "        pass\n",
        "\n",
        "    if i % PROGRESS_EVERY == 0:\n",
        "        elapsed = max(1e-6, time.time()-t0)\n",
        "        rate = i / elapsed\n",
        "        eta_min = (len(todo)-i) / max(1e-6, rate) / 60\n",
        "        print(f\"[{YEAR}] {i}/{len(todo)} | {rate:.2f} files/s | ETA≈{eta_min:.1f}m | docs_kept={docs_seen:,}\")\n",
        "\n",
        "    if i % CHECKPOINT_EVERY == 0:\n",
        "        write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "\n",
        "# final write for the year\n",
        "write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "print(f\"[{YEAR}] Done. This session processed: {len(todo):,} | Total processed so far: {processed_count:,} | Docs kept: {docs_seen:,}\")\n",
        "\n",
        "# ------------------------ Append to master ------------------------\n",
        "try:\n",
        "    ydf = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # sanity: ensure we really wrote YEAR\n",
        "    years_found = set(ydf[\"month\"].dt.year.dropna().unique().tolist())\n",
        "    if years_found != {YEAR}:\n",
        "        raise ValueError(f\"Year mismatch: expected {YEAR}, found {sorted(years_found)} in {YEARLY_CSV}\")\n",
        "\n",
        "    if MASTER_CSV.exists():\n",
        "        m = pd.read_csv(MASTER_CSV, parse_dates=[\"month\"])\n",
        "        m = (pd.concat([m, ydf], ignore_index=True)\n",
        "               .drop_duplicates(subset=\"month\", keep=\"last\")\n",
        "               .sort_values(\"month\")\n",
        "               .reset_index(drop=True))\n",
        "    else:\n",
        "        m = ydf\n",
        "    m.to_csv(MASTER_CSV, index=False)\n",
        "    print(\"Appended to master:\", MASTER_CSV, m.shape)\n",
        "except Exception as e:\n",
        "    print(\"Append-to-master skipped:\", e)\n",
        "\n",
        "# ------------------------ Pretty print monthly for YEAR ------------------------\n",
        "try:\n",
        "    df = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # build a complete monthly index for YEAR so missing months show as NaN\n",
        "    full = (df.set_index(\"month\")\n",
        "              .reindex(pd.date_range(f\"{YEAR}-01-01\", f\"{YEAR}-12-01\", freq=\"MS\"))\n",
        "              .rename_axis(\"Month\").reset_index())\n",
        "    full[\"Docs\"]    = pd.to_numeric(full[\"docs\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "    full[\"AvgTone\"] = pd.to_numeric(full[\"avg_tone\"], errors=\"coerce\")\n",
        "    print(full[[\"Month\",\"Docs\",\"AvgTone\"]].to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Preview skipped:\", e)"
      ],
      "metadata": {
        "id": "3GcWdwowMUa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2020"
      ],
      "metadata": {
        "id": "DhKgAoVpKEnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import re, io, zipfile, math, time, json, logging, requests, pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "# ------------------------ CONFIG ------------------------\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")  # change if you like\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "YEAR = 2020\n",
        "RESET_STATE = True\n",
        "MAX_FILES = None\n",
        "HOURLY_ONLY = True\n",
        "CHECKPOINT_EVERY = 200\n",
        "PROGRESS_EVERY   = 100\n",
        "\n",
        "# Date window from YEAR (dynamic!)\n",
        "START_TS = f\"{YEAR}0101000000\"  # YYYY-01-01 00:00:00\n",
        "END_TS   = f\"{YEAR}1231235959\"  # YYYY-12-31 23:59:59\n",
        "# Filters\n",
        "UK_REGEX = r'#(?:GBR|GB|UK|United Kingdom|England|Scotland|Wales|Northern Ireland)#'\n",
        "THEME_PATTERN = r'(?:HOUS|MORTGAG|REMORTGAG|RENT|TENAN|REAL[ _]?ESTATE|PROPERTY|HOME[ _]?PRIC|HOUSE[ _]?PRIC|RIGHTMOVE|ZOOPLA|LANDLORD|BUY[ -]?TO[ -]?LET)'\n",
        "\n",
        "# GKG v2.1 columns (0-based indices)\n",
        "DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I = 1, 8, 10, 15\n",
        "\n",
        "# Paths\n",
        "OUT_DIR = BASE_DIR / f\"gdelt_run_{YEAR}\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "YEARLY_CSV = OUT_DIR / f\"gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "STATE_PATH = OUT_DIR / \"state.json\"\n",
        "MASTER_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
        "MASTER_CSV = BASE_DIR / \"gdelt_uk_housing_monthly_STITCHING.csv\"\n",
        "\n",
        "# ------------------------ Networking helpers ------------------------\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-colab/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def get(url, timeout=60, s=None):\n",
        "    s = s or session()\n",
        "    if url.startswith(\"https://\"):  # avoid SSL hiccups in Colab\n",
        "        url = \"http://\" + url[len(\"https://\"):]\n",
        "    r = s.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "# ------------------------ GDELT plumbing ------------------------\n",
        "def build_gkg_urls():\n",
        "    \"\"\"Collect GKG v2.1 .zip URLs within the YEAR bounds (top-of-hour if HOURLY_ONLY).\"\"\"\n",
        "    txt = get(MASTER_URL, timeout=90).text\n",
        "    urls = []\n",
        "    for line in txt.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3:\n",
        "            continue\n",
        "        url = parts[2]\n",
        "        m = re.search(r'/gdeltv2/(\\d{14})\\.gkg\\.csv\\.zip$', url)\n",
        "        if not m:\n",
        "            continue\n",
        "        ts = m.group(1)\n",
        "        if START_TS <= ts <= END_TS:\n",
        "            if HOURLY_ONLY and ts[10:12] != \"00\":  # only top-of-hour files\n",
        "                continue\n",
        "            urls.append(url.replace(\"https://\", \"http://\"))\n",
        "    urls.sort()\n",
        "    return urls\n",
        "\n",
        "def month_from_dateint(dateint):\n",
        "    dt = datetime.strptime(str(dateint), \"%Y%m%d%H%M%S\")\n",
        "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "\n",
        "def doc_tone(v2tone):\n",
        "    try:\n",
        "        # V2Tone = \"docTone, pos, neg, polarity, activityRefDensity, selfGroupRefDensity, wordCount\"\n",
        "        return float(str(v2tone).split(\",\")[0])\n",
        "    except Exception:\n",
        "        return math.nan\n",
        "\n",
        "def process_df(df, counts, tone_sum, tone_n, uk_re, theme_re):\n",
        "    m_uk = df[V2LOCS_I].astype(str).str.contains(uk_re, na=False, regex=True)\n",
        "    m_th = df[V2THEMES_I].astype(str).str.contains(theme_re, na=False, regex=True)\n",
        "    sub = df.loc[m_uk & m_th, [DATE_I, V2TONE_I]]\n",
        "    if sub.empty:\n",
        "        return 0\n",
        "    months = sub[DATE_I].astype(str).apply(month_from_dateint)\n",
        "    tones  = sub[V2TONE_I].apply(doc_tone)\n",
        "    kept = 0\n",
        "    for m, t in zip(months, tones):\n",
        "        counts[m] += 1\n",
        "        if not math.isnan(t):\n",
        "            tone_sum[m] += t\n",
        "            tone_n[m] += 1\n",
        "        kept += 1\n",
        "    return kept\n",
        "\n",
        "def write_year_output(processed_count, processed_tail, counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m]/tone_n[m]) if tone_n[m] else None\n",
        "        rows.append({\"month\": f\"{m}-01\", \"docs\": n, \"avg_tone\": avg})\n",
        "    monthly = pd.DataFrame(rows, columns=[\"month\",\"docs\",\"avg_tone\"]).sort_values(\"month\")\n",
        "    monthly.to_csv(YEARLY_CSV, index=False)\n",
        "    json.dump({\n",
        "        \"processed_count\": processed_count,\n",
        "        \"processed_urls_tail\": processed_tail[-20000:],  # retain a long tail for resume\n",
        "        \"counts\": counts, \"tone_sum\": tone_sum, \"tone_n\": tone_n\n",
        "    }, open(STATE_PATH, \"w\"))\n",
        "    print(f\"Saved → {YEARLY_CSV}\")\n",
        "\n",
        "# ------------------------ Reset (if previously wrong year) ------------------------\n",
        "if RESET_STATE:\n",
        "    if STATE_PATH.exists():\n",
        "        STATE_PATH.unlink()\n",
        "    if YEARLY_CSV.exists():\n",
        "        YEARLY_CSV.unlink()\n",
        "\n",
        "# ------------------------ Plan + resume ------------------------\n",
        "s = session()\n",
        "urls_all = build_gkg_urls()\n",
        "print(f\"[{YEAR}] planned hourly files: {len(urls_all):,}\")\n",
        "\n",
        "if STATE_PATH.exists():\n",
        "    state = json.load(open(STATE_PATH))\n",
        "    processed_count     = int(state.get(\"processed_count\", 0))\n",
        "    processed_urls_tail = state.get(\"processed_urls_tail\", [])\n",
        "    counts   = defaultdict(int,   state.get(\"counts\", {}))\n",
        "    tone_sum = defaultdict(float, state.get(\"tone_sum\", {}))\n",
        "    tone_n   = defaultdict(int,   state.get(\"tone_n\", {}))\n",
        "    print(f\"Resuming: {processed_count:,} files done previously.\")\n",
        "else:\n",
        "    processed_count, processed_urls_tail = 0, []\n",
        "    counts, tone_sum, tone_n = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "already = set(processed_urls_tail)\n",
        "todo = [u for u in urls_all if u not in already]\n",
        "if MAX_FILES is not None:\n",
        "    todo = todo[:MAX_FILES]\n",
        "print(f\"[{YEAR}] processing this session: {len(todo):,} files\")\n",
        "\n",
        "uk_re = re.compile(UK_REGEX, re.I)\n",
        "theme_re = re.compile(THEME_PATTERN, re.I)\n",
        "\n",
        "# ------------------------ Main loop ------------------------\n",
        "t0 = time.time()\n",
        "docs_seen = 0\n",
        "\n",
        "for i, url in enumerate(todo, 1):\n",
        "    try:\n",
        "        z = zipfile.ZipFile(io.BytesIO(get(url, timeout=90, s=s).content))\n",
        "        name = z.namelist()[0]\n",
        "        df = pd.read_csv(\n",
        "            z.open(name),\n",
        "            sep=\"\\t\", header=None, quoting=3, low_memory=False,\n",
        "            usecols=[DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I],\n",
        "            dtype={DATE_I:str, V2THEMES_I:str, V2LOCS_I:str, V2TONE_I:str}\n",
        "        )\n",
        "        docs_seen += process_df(df, counts, tone_sum, tone_n, uk_re, theme_re)\n",
        "        processed_count += 1\n",
        "        processed_urls_tail.append(url)\n",
        "        if len(processed_urls_tail) > 20000:\n",
        "            processed_urls_tail = processed_urls_tail[-20000:]\n",
        "    except Exception:\n",
        "        # skip bad zips quietly\n",
        "        pass\n",
        "\n",
        "    if i % PROGRESS_EVERY == 0:\n",
        "        elapsed = max(1e-6, time.time()-t0)\n",
        "        rate = i / elapsed\n",
        "        eta_min = (len(todo)-i) / max(1e-6, rate) / 60\n",
        "        print(f\"[{YEAR}] {i}/{len(todo)} | {rate:.2f} files/s | ETA≈{eta_min:.1f}m | docs_kept={docs_seen:,}\")\n",
        "\n",
        "    if i % CHECKPOINT_EVERY == 0:\n",
        "        write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "\n",
        "# final write for the year\n",
        "write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "print(f\"[{YEAR}] Done. This session processed: {len(todo):,} | Total processed so far: {processed_count:,} | Docs kept: {docs_seen:,}\")\n",
        "\n",
        "# ------------------------ Append to master ------------------------\n",
        "try:\n",
        "    ydf = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # sanity: ensure we really wrote YEAR\n",
        "    years_found = set(ydf[\"month\"].dt.year.dropna().unique().tolist())\n",
        "    if years_found != {YEAR}:\n",
        "        raise ValueError(f\"Year mismatch: expected {YEAR}, found {sorted(years_found)} in {YEARLY_CSV}\")\n",
        "\n",
        "    if MASTER_CSV.exists():\n",
        "        m = pd.read_csv(MASTER_CSV, parse_dates=[\"month\"])\n",
        "        m = (pd.concat([m, ydf], ignore_index=True)\n",
        "               .drop_duplicates(subset=\"month\", keep=\"last\")\n",
        "               .sort_values(\"month\")\n",
        "               .reset_index(drop=True))\n",
        "    else:\n",
        "        m = ydf\n",
        "    m.to_csv(MASTER_CSV, index=False)\n",
        "    print(\"Appended to master:\", MASTER_CSV, m.shape)\n",
        "except Exception as e:\n",
        "    print(\"Append-to-master skipped:\", e)\n",
        "\n",
        "# ------------------------ Pretty print monthly for YEAR ------------------------\n",
        "try:\n",
        "    df = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # build a complete monthly index for YEAR so missing months show as NaN\n",
        "    full = (df.set_index(\"month\")\n",
        "              .reindex(pd.date_range(f\"{YEAR}-01-01\", f\"{YEAR}-12-01\", freq=\"MS\"))\n",
        "              .rename_axis(\"Month\").reset_index())\n",
        "    full[\"Docs\"]    = pd.to_numeric(full[\"docs\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "    full[\"AvgTone\"] = pd.to_numeric(full[\"avg_tone\"], errors=\"coerce\")\n",
        "    print(full[[\"Month\",\"Docs\",\"AvgTone\"]].to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Preview skipped:\", e)"
      ],
      "metadata": {
        "id": "WD8hctmnMkxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2021"
      ],
      "metadata": {
        "id": "cqIMFXZTKFkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import re, io, zipfile, math, time, json, logging, requests, pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "# ------------------------ CONFIG ------------------------\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")  # change if you like\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "YEAR = 2021\n",
        "RESET_STATE = True\n",
        "MAX_FILES = None\n",
        "HOURLY_ONLY = True\n",
        "CHECKPOINT_EVERY = 200\n",
        "PROGRESS_EVERY   = 100\n",
        "\n",
        "# Date window from YEAR (dynamic!)\n",
        "START_TS = f\"{YEAR}0101000000\"  # YYYY-01-01 00:00:00\n",
        "END_TS   = f\"{YEAR}1231235959\"  # YYYY-12-31 23:59:59\n",
        "# Filters\n",
        "UK_REGEX = r'#(?:GBR|GB|UK|United Kingdom|England|Scotland|Wales|Northern Ireland)#'\n",
        "THEME_PATTERN = r'(?:HOUS|MORTGAG|REMORTGAG|RENT|TENAN|REAL[ _]?ESTATE|PROPERTY|HOME[ _]?PRIC|HOUSE[ _]?PRIC|RIGHTMOVE|ZOOPLA|LANDLORD|BUY[ -]?TO[ -]?LET)'\n",
        "\n",
        "# GKG v2.1 columns (0-based indices)\n",
        "DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I = 1, 8, 10, 15\n",
        "\n",
        "# Paths\n",
        "OUT_DIR = BASE_DIR / f\"gdelt_run_{YEAR}\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "YEARLY_CSV = OUT_DIR / f\"gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "STATE_PATH = OUT_DIR / \"state.json\"\n",
        "MASTER_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
        "MASTER_CSV = BASE_DIR / \"gdelt_uk_housing_monthly_STITCHING.csv\"\n",
        "\n",
        "# ------------------------ Networking helpers ------------------------\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-colab/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def get(url, timeout=60, s=None):\n",
        "    s = s or session()\n",
        "    if url.startswith(\"https://\"):  # avoid SSL hiccups in Colab\n",
        "        url = \"http://\" + url[len(\"https://\"):]\n",
        "    r = s.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "# ------------------------ GDELT plumbing ------------------------\n",
        "def build_gkg_urls():\n",
        "    \"\"\"Collect GKG v2.1 .zip URLs within the YEAR bounds (top-of-hour if HOURLY_ONLY).\"\"\"\n",
        "    txt = get(MASTER_URL, timeout=90).text\n",
        "    urls = []\n",
        "    for line in txt.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3:\n",
        "            continue\n",
        "        url = parts[2]\n",
        "        m = re.search(r'/gdeltv2/(\\d{14})\\.gkg\\.csv\\.zip$', url)\n",
        "        if not m:\n",
        "            continue\n",
        "        ts = m.group(1)\n",
        "        if START_TS <= ts <= END_TS:\n",
        "            if HOURLY_ONLY and ts[10:12] != \"00\":  # only top-of-hour files\n",
        "                continue\n",
        "            urls.append(url.replace(\"https://\", \"http://\"))\n",
        "    urls.sort()\n",
        "    return urls\n",
        "\n",
        "def month_from_dateint(dateint):\n",
        "    dt = datetime.strptime(str(dateint), \"%Y%m%d%H%M%S\")\n",
        "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "\n",
        "def doc_tone(v2tone):\n",
        "    try:\n",
        "        # V2Tone = \"docTone, pos, neg, polarity, activityRefDensity, selfGroupRefDensity, wordCount\"\n",
        "        return float(str(v2tone).split(\",\")[0])\n",
        "    except Exception:\n",
        "        return math.nan\n",
        "\n",
        "def process_df(df, counts, tone_sum, tone_n, uk_re, theme_re):\n",
        "    m_uk = df[V2LOCS_I].astype(str).str.contains(uk_re, na=False, regex=True)\n",
        "    m_th = df[V2THEMES_I].astype(str).str.contains(theme_re, na=False, regex=True)\n",
        "    sub = df.loc[m_uk & m_th, [DATE_I, V2TONE_I]]\n",
        "    if sub.empty:\n",
        "        return 0\n",
        "    months = sub[DATE_I].astype(str).apply(month_from_dateint)\n",
        "    tones  = sub[V2TONE_I].apply(doc_tone)\n",
        "    kept = 0\n",
        "    for m, t in zip(months, tones):\n",
        "        counts[m] += 1\n",
        "        if not math.isnan(t):\n",
        "            tone_sum[m] += t\n",
        "            tone_n[m] += 1\n",
        "        kept += 1\n",
        "    return kept\n",
        "\n",
        "def write_year_output(processed_count, processed_tail, counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m]/tone_n[m]) if tone_n[m] else None\n",
        "        rows.append({\"month\": f\"{m}-01\", \"docs\": n, \"avg_tone\": avg})\n",
        "    monthly = pd.DataFrame(rows, columns=[\"month\",\"docs\",\"avg_tone\"]).sort_values(\"month\")\n",
        "    monthly.to_csv(YEARLY_CSV, index=False)\n",
        "    json.dump({\n",
        "        \"processed_count\": processed_count,\n",
        "        \"processed_urls_tail\": processed_tail[-20000:],  # retain a long tail for resume\n",
        "        \"counts\": counts, \"tone_sum\": tone_sum, \"tone_n\": tone_n\n",
        "    }, open(STATE_PATH, \"w\"))\n",
        "    print(f\"Saved → {YEARLY_CSV}\")\n",
        "\n",
        "# ------------------------ Reset (if previously wrong year) ------------------------\n",
        "if RESET_STATE:\n",
        "    if STATE_PATH.exists():\n",
        "        STATE_PATH.unlink()\n",
        "    if YEARLY_CSV.exists():\n",
        "        YEARLY_CSV.unlink()\n",
        "\n",
        "# ------------------------ Plan + resume ------------------------\n",
        "s = session()\n",
        "urls_all = build_gkg_urls()\n",
        "print(f\"[{YEAR}] planned hourly files: {len(urls_all):,}\")\n",
        "\n",
        "if STATE_PATH.exists():\n",
        "    state = json.load(open(STATE_PATH))\n",
        "    processed_count     = int(state.get(\"processed_count\", 0))\n",
        "    processed_urls_tail = state.get(\"processed_urls_tail\", [])\n",
        "    counts   = defaultdict(int,   state.get(\"counts\", {}))\n",
        "    tone_sum = defaultdict(float, state.get(\"tone_sum\", {}))\n",
        "    tone_n   = defaultdict(int,   state.get(\"tone_n\", {}))\n",
        "    print(f\"Resuming: {processed_count:,} files done previously.\")\n",
        "else:\n",
        "    processed_count, processed_urls_tail = 0, []\n",
        "    counts, tone_sum, tone_n = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "already = set(processed_urls_tail)\n",
        "todo = [u for u in urls_all if u not in already]\n",
        "if MAX_FILES is not None:\n",
        "    todo = todo[:MAX_FILES]\n",
        "print(f\"[{YEAR}] processing this session: {len(todo):,} files\")\n",
        "\n",
        "uk_re = re.compile(UK_REGEX, re.I)\n",
        "theme_re = re.compile(THEME_PATTERN, re.I)\n",
        "\n",
        "# ------------------------ Main loop ------------------------\n",
        "t0 = time.time()\n",
        "docs_seen = 0\n",
        "\n",
        "for i, url in enumerate(todo, 1):\n",
        "    try:\n",
        "        z = zipfile.ZipFile(io.BytesIO(get(url, timeout=90, s=s).content))\n",
        "        name = z.namelist()[0]\n",
        "        df = pd.read_csv(\n",
        "            z.open(name),\n",
        "            sep=\"\\t\", header=None, quoting=3, low_memory=False,\n",
        "            usecols=[DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I],\n",
        "            dtype={DATE_I:str, V2THEMES_I:str, V2LOCS_I:str, V2TONE_I:str}\n",
        "        )\n",
        "        docs_seen += process_df(df, counts, tone_sum, tone_n, uk_re, theme_re)\n",
        "        processed_count += 1\n",
        "        processed_urls_tail.append(url)\n",
        "        if len(processed_urls_tail) > 20000:\n",
        "            processed_urls_tail = processed_urls_tail[-20000:]\n",
        "    except Exception:\n",
        "        # skip bad zips quietly\n",
        "        pass\n",
        "\n",
        "    if i % PROGRESS_EVERY == 0:\n",
        "        elapsed = max(1e-6, time.time()-t0)\n",
        "        rate = i / elapsed\n",
        "        eta_min = (len(todo)-i) / max(1e-6, rate) / 60\n",
        "        print(f\"[{YEAR}] {i}/{len(todo)} | {rate:.2f} files/s | ETA≈{eta_min:.1f}m | docs_kept={docs_seen:,}\")\n",
        "\n",
        "    if i % CHECKPOINT_EVERY == 0:\n",
        "        write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "\n",
        "# final write for the year\n",
        "write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "print(f\"[{YEAR}] Done. This session processed: {len(todo):,} | Total processed so far: {processed_count:,} | Docs kept: {docs_seen:,}\")\n",
        "\n",
        "# ------------------------ Append to master ------------------------\n",
        "try:\n",
        "    ydf = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # sanity: ensure we really wrote YEAR\n",
        "    years_found = set(ydf[\"month\"].dt.year.dropna().unique().tolist())\n",
        "    if years_found != {YEAR}:\n",
        "        raise ValueError(f\"Year mismatch: expected {YEAR}, found {sorted(years_found)} in {YEARLY_CSV}\")\n",
        "\n",
        "    if MASTER_CSV.exists():\n",
        "        m = pd.read_csv(MASTER_CSV, parse_dates=[\"month\"])\n",
        "        m = (pd.concat([m, ydf], ignore_index=True)\n",
        "               .drop_duplicates(subset=\"month\", keep=\"last\")\n",
        "               .sort_values(\"month\")\n",
        "               .reset_index(drop=True))\n",
        "    else:\n",
        "        m = ydf\n",
        "    m.to_csv(MASTER_CSV, index=False)\n",
        "    print(\"Appended to master:\", MASTER_CSV, m.shape)\n",
        "except Exception as e:\n",
        "    print(\"Append-to-master skipped:\", e)\n",
        "\n",
        "# ------------------------ Pretty print monthly for YEAR ------------------------\n",
        "try:\n",
        "    df = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # build a complete monthly index for YEAR so missing months show as NaN\n",
        "    full = (df.set_index(\"month\")\n",
        "              .reindex(pd.date_range(f\"{YEAR}-01-01\", f\"{YEAR}-12-01\", freq=\"MS\"))\n",
        "              .rename_axis(\"Month\").reset_index())\n",
        "    full[\"Docs\"]    = pd.to_numeric(full[\"docs\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "    full[\"AvgTone\"] = pd.to_numeric(full[\"avg_tone\"], errors=\"coerce\")\n",
        "    print(full[[\"Month\",\"Docs\",\"AvgTone\"]].to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Preview skipped:\", e)"
      ],
      "metadata": {
        "id": "Z2g35uW9Mrjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2022"
      ],
      "metadata": {
        "id": "273dJYo57-xP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import re, io, zipfile, math, time, json, logging, requests, pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "# ------------------------ CONFIG ------------------------\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")  # change if you like\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "YEAR = 2022                     # <<< set the year you want to process\n",
        "RESET_STATE = True              # <<< set True if you previously ran the wrong year; else False\n",
        "MAX_FILES = None                # cap files processed this run; None = process all remaining\n",
        "HOURLY_ONLY = True              # process top-of-hour files only (lighter than every 15 minutes)\n",
        "CHECKPOINT_EVERY = 200\n",
        "PROGRESS_EVERY   = 100\n",
        "\n",
        "# Date window from YEAR (dynamic!)\n",
        "START_TS = f\"{YEAR}0101000000\"  # YYYY-01-01 00:00:00\n",
        "END_TS   = f\"{YEAR}1231235959\"  # YYYY-12-31 23:59:59\n",
        "# Filters\n",
        "UK_REGEX = r'#(?:GBR|GB|UK|United Kingdom|England|Scotland|Wales|Northern Ireland)#'\n",
        "THEME_PATTERN = r'(?:HOUS|MORTGAG|REMORTGAG|RENT|TENAN|REAL[ _]?ESTATE|PROPERTY|HOME[ _]?PRIC|HOUSE[ _]?PRIC|RIGHTMOVE|ZOOPLA|LANDLORD|BUY[ -]?TO[ -]?LET)'\n",
        "\n",
        "# GKG v2.1 columns (0-based indices)\n",
        "DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I = 1, 8, 10, 15\n",
        "\n",
        "# Paths\n",
        "OUT_DIR = BASE_DIR / f\"gdelt_run_{YEAR}\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "YEARLY_CSV = OUT_DIR / f\"gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "STATE_PATH = OUT_DIR / \"state.json\"\n",
        "MASTER_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
        "MASTER_CSV = BASE_DIR / \"gdelt_uk_housing_monthly_STITCHING.csv\"\n",
        "\n",
        "# ------------------------ Networking helpers ------------------------\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-colab/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def get(url, timeout=60, s=None):\n",
        "    s = s or session()\n",
        "    if url.startswith(\"https://\"):  # avoid SSL hiccups in Colab\n",
        "        url = \"http://\" + url[len(\"https://\"):]\n",
        "    r = s.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "# ------------------------ GDELT plumbing ------------------------\n",
        "def build_gkg_urls():\n",
        "    \"\"\"Collect GKG v2.1 .zip URLs within the YEAR bounds (top-of-hour if HOURLY_ONLY).\"\"\"\n",
        "    txt = get(MASTER_URL, timeout=90).text\n",
        "    urls = []\n",
        "    for line in txt.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3:\n",
        "            continue\n",
        "        url = parts[2]\n",
        "        m = re.search(r'/gdeltv2/(\\d{14})\\.gkg\\.csv\\.zip$', url)\n",
        "        if not m:\n",
        "            continue\n",
        "        ts = m.group(1)\n",
        "        if START_TS <= ts <= END_TS:\n",
        "            if HOURLY_ONLY and ts[10:12] != \"00\":  # only top-of-hour files\n",
        "                continue\n",
        "            urls.append(url.replace(\"https://\", \"http://\"))\n",
        "    urls.sort()\n",
        "    return urls\n",
        "\n",
        "def month_from_dateint(dateint):\n",
        "    dt = datetime.strptime(str(dateint), \"%Y%m%d%H%M%S\")\n",
        "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "\n",
        "def doc_tone(v2tone):\n",
        "    try:\n",
        "        # V2Tone = \"docTone, pos, neg, polarity, activityRefDensity, selfGroupRefDensity, wordCount\"\n",
        "        return float(str(v2tone).split(\",\")[0])\n",
        "    except Exception:\n",
        "        return math.nan\n",
        "\n",
        "def process_df(df, counts, tone_sum, tone_n, uk_re, theme_re):\n",
        "    m_uk = df[V2LOCS_I].astype(str).str.contains(uk_re, na=False, regex=True)\n",
        "    m_th = df[V2THEMES_I].astype(str).str.contains(theme_re, na=False, regex=True)\n",
        "    sub = df.loc[m_uk & m_th, [DATE_I, V2TONE_I]]\n",
        "    if sub.empty:\n",
        "        return 0\n",
        "    months = sub[DATE_I].astype(str).apply(month_from_dateint)\n",
        "    tones  = sub[V2TONE_I].apply(doc_tone)\n",
        "    kept = 0\n",
        "    for m, t in zip(months, tones):\n",
        "        counts[m] += 1\n",
        "        if not math.isnan(t):\n",
        "            tone_sum[m] += t\n",
        "            tone_n[m] += 1\n",
        "        kept += 1\n",
        "    return kept\n",
        "\n",
        "def write_year_output(processed_count, processed_tail, counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m]/tone_n[m]) if tone_n[m] else None\n",
        "        rows.append({\"month\": f\"{m}-01\", \"docs\": n, \"avg_tone\": avg})\n",
        "    monthly = pd.DataFrame(rows, columns=[\"month\",\"docs\",\"avg_tone\"]).sort_values(\"month\")\n",
        "    monthly.to_csv(YEARLY_CSV, index=False)\n",
        "    json.dump({\n",
        "        \"processed_count\": processed_count,\n",
        "        \"processed_urls_tail\": processed_tail[-20000:],  # retain a long tail for resume\n",
        "        \"counts\": counts, \"tone_sum\": tone_sum, \"tone_n\": tone_n\n",
        "    }, open(STATE_PATH, \"w\"))\n",
        "    print(f\"Saved → {YEARLY_CSV}\")\n",
        "\n",
        "# ------------------------ Reset (if previously wrong year) ------------------------\n",
        "if RESET_STATE:\n",
        "    if STATE_PATH.exists():\n",
        "        STATE_PATH.unlink()\n",
        "    if YEARLY_CSV.exists():\n",
        "        YEARLY_CSV.unlink()\n",
        "\n",
        "# ------------------------ Plan + resume ------------------------\n",
        "s = session()\n",
        "urls_all = build_gkg_urls()\n",
        "print(f\"[{YEAR}] planned hourly files: {len(urls_all):,}\")\n",
        "\n",
        "if STATE_PATH.exists():\n",
        "    state = json.load(open(STATE_PATH))\n",
        "    processed_count     = int(state.get(\"processed_count\", 0))\n",
        "    processed_urls_tail = state.get(\"processed_urls_tail\", [])\n",
        "    counts   = defaultdict(int,   state.get(\"counts\", {}))\n",
        "    tone_sum = defaultdict(float, state.get(\"tone_sum\", {}))\n",
        "    tone_n   = defaultdict(int,   state.get(\"tone_n\", {}))\n",
        "    print(f\"Resuming: {processed_count:,} files done previously.\")\n",
        "else:\n",
        "    processed_count, processed_urls_tail = 0, []\n",
        "    counts, tone_sum, tone_n = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "already = set(processed_urls_tail)\n",
        "todo = [u for u in urls_all if u not in already]\n",
        "if MAX_FILES is not None:\n",
        "    todo = todo[:MAX_FILES]\n",
        "print(f\"[{YEAR}] processing this session: {len(todo):,} files\")\n",
        "\n",
        "uk_re = re.compile(UK_REGEX, re.I)\n",
        "theme_re = re.compile(THEME_PATTERN, re.I)\n",
        "\n",
        "# ------------------------ Main loop ------------------------\n",
        "t0 = time.time()\n",
        "docs_seen = 0\n",
        "\n",
        "for i, url in enumerate(todo, 1):\n",
        "    try:\n",
        "        z = zipfile.ZipFile(io.BytesIO(get(url, timeout=90, s=s).content))\n",
        "        name = z.namelist()[0]\n",
        "        df = pd.read_csv(\n",
        "            z.open(name),\n",
        "            sep=\"\\t\", header=None, quoting=3, low_memory=False,\n",
        "            usecols=[DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I],\n",
        "            dtype={DATE_I:str, V2THEMES_I:str, V2LOCS_I:str, V2TONE_I:str}\n",
        "        )\n",
        "        docs_seen += process_df(df, counts, tone_sum, tone_n, uk_re, theme_re)\n",
        "        processed_count += 1\n",
        "        processed_urls_tail.append(url)\n",
        "        if len(processed_urls_tail) > 20000:\n",
        "            processed_urls_tail = processed_urls_tail[-20000:]\n",
        "    except Exception:\n",
        "        # skip bad zips quietly\n",
        "        pass\n",
        "\n",
        "    if i % PROGRESS_EVERY == 0:\n",
        "        elapsed = max(1e-6, time.time()-t0)\n",
        "        rate = i / elapsed\n",
        "        eta_min = (len(todo)-i) / max(1e-6, rate) / 60\n",
        "        print(f\"[{YEAR}] {i}/{len(todo)} | {rate:.2f} files/s | ETA≈{eta_min:.1f}m | docs_kept={docs_seen:,}\")\n",
        "\n",
        "    if i % CHECKPOINT_EVERY == 0:\n",
        "        write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "\n",
        "# final write for the year\n",
        "write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "print(f\"[{YEAR}] Done. This session processed: {len(todo):,} | Total processed so far: {processed_count:,} | Docs kept: {docs_seen:,}\")\n",
        "\n",
        "# ------------------------ Append to master ------------------------\n",
        "try:\n",
        "    ydf = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # sanity: ensure we really wrote YEAR\n",
        "    years_found = set(ydf[\"month\"].dt.year.dropna().unique().tolist())\n",
        "    if years_found != {YEAR}:\n",
        "        raise ValueError(f\"Year mismatch: expected {YEAR}, found {sorted(years_found)} in {YEARLY_CSV}\")\n",
        "\n",
        "    if MASTER_CSV.exists():\n",
        "        m = pd.read_csv(MASTER_CSV, parse_dates=[\"month\"])\n",
        "        m = (pd.concat([m, ydf], ignore_index=True)\n",
        "               .drop_duplicates(subset=\"month\", keep=\"last\")\n",
        "               .sort_values(\"month\")\n",
        "               .reset_index(drop=True))\n",
        "    else:\n",
        "        m = ydf\n",
        "    m.to_csv(MASTER_CSV, index=False)\n",
        "    print(\"Appended to master:\", MASTER_CSV, m.shape)\n",
        "except Exception as e:\n",
        "    print(\"Append-to-master skipped:\", e)\n",
        "\n",
        "# ------------------------ Pretty print monthly for YEAR ------------------------\n",
        "try:\n",
        "    df = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # build a complete monthly index for YEAR so missing months show as NaN\n",
        "    full = (df.set_index(\"month\")\n",
        "              .reindex(pd.date_range(f\"{YEAR}-01-01\", f\"{YEAR}-12-01\", freq=\"MS\"))\n",
        "              .rename_axis(\"Month\").reset_index())\n",
        "    full[\"Docs\"]    = pd.to_numeric(full[\"docs\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "    full[\"AvgTone\"] = pd.to_numeric(full[\"avg_tone\"], errors=\"coerce\")\n",
        "    print(full[[\"Month\",\"Docs\",\"AvgTone\"]].to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Preview skipped:\", e)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK2IC0AgmvDA",
        "outputId": "7c260041-8e84-4d87-db92-e88a97497476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[2022] planned hourly files: 8,759\n",
            "[2022] processing this session: 8,759 files\n",
            "[2022] 100/8759 | 1.87 files/s | ETA≈77.0m | docs_kept=1,105\n",
            "[2022] 200/8759 | 1.63 files/s | ETA≈87.7m | docs_kept=2,397\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 300/8759 | 1.58 files/s | ETA≈89.4m | docs_kept=3,681\n",
            "[2022] 400/8759 | 1.63 files/s | ETA≈85.6m | docs_kept=4,779\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 500/8759 | 1.57 files/s | ETA≈87.6m | docs_kept=6,773\n",
            "[2022] 600/8759 | 1.60 files/s | ETA≈84.8m | docs_kept=8,119\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 700/8759 | 1.62 files/s | ETA≈83.1m | docs_kept=9,261\n",
            "[2022] 800/8759 | 1.62 files/s | ETA≈82.0m | docs_kept=10,709\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 900/8759 | 1.64 files/s | ETA≈79.9m | docs_kept=11,639\n",
            "[2022] 1000/8759 | 1.61 files/s | ETA≈80.2m | docs_kept=12,921\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 1100/8759 | 1.63 files/s | ETA≈78.4m | docs_kept=14,578\n",
            "[2022] 1200/8759 | 1.63 files/s | ETA≈77.3m | docs_kept=16,107\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 1300/8759 | 1.63 files/s | ETA≈76.4m | docs_kept=18,034\n",
            "[2022] 1400/8759 | 1.64 files/s | ETA≈74.9m | docs_kept=20,062\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 1500/8759 | 1.61 files/s | ETA≈75.2m | docs_kept=21,924\n",
            "[2022] 1600/8759 | 1.61 files/s | ETA≈74.2m | docs_kept=23,744\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 1700/8759 | 1.59 files/s | ETA≈73.8m | docs_kept=25,725\n",
            "[2022] 1800/8759 | 1.60 files/s | ETA≈72.6m | docs_kept=27,111\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 1900/8759 | 1.61 files/s | ETA≈70.9m | docs_kept=28,311\n",
            "[2022] 2000/8759 | 1.61 files/s | ETA≈69.9m | docs_kept=29,834\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 2100/8759 | 1.62 files/s | ETA≈68.6m | docs_kept=31,219\n",
            "[2022] 2200/8759 | 1.62 files/s | ETA≈67.7m | docs_kept=32,816\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 2300/8759 | 1.62 files/s | ETA≈66.4m | docs_kept=33,977\n",
            "[2022] 2400/8759 | 1.63 files/s | ETA≈65.1m | docs_kept=35,037\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 2500/8759 | 1.62 files/s | ETA≈64.4m | docs_kept=36,434\n",
            "[2022] 2600/8759 | 1.63 files/s | ETA≈62.9m | docs_kept=37,365\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 2700/8759 | 1.63 files/s | ETA≈61.9m | docs_kept=38,509\n",
            "[2022] 2800/8759 | 1.64 files/s | ETA≈60.7m | docs_kept=39,927\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 2900/8759 | 1.64 files/s | ETA≈59.6m | docs_kept=41,030\n",
            "[2022] 3000/8759 | 1.64 files/s | ETA≈58.6m | docs_kept=42,396\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 3100/8759 | 1.65 files/s | ETA≈57.3m | docs_kept=43,308\n",
            "[2022] 3200/8759 | 1.64 files/s | ETA≈56.5m | docs_kept=44,692\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 3300/8759 | 1.65 files/s | ETA≈55.2m | docs_kept=45,620\n",
            "[2022] 3400/8759 | 1.66 files/s | ETA≈53.9m | docs_kept=46,759\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 3500/8759 | 1.65 files/s | ETA≈53.0m | docs_kept=47,806\n",
            "[2022] 3600/8759 | 1.66 files/s | ETA≈51.8m | docs_kept=48,901\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 3700/8759 | 1.65 files/s | ETA≈51.0m | docs_kept=50,072\n",
            "[2022] 3800/8759 | 1.66 files/s | ETA≈49.8m | docs_kept=50,990\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 3900/8759 | 1.66 files/s | ETA≈48.6m | docs_kept=52,129\n",
            "[2022] 4000/8759 | 1.67 files/s | ETA≈47.6m | docs_kept=53,340\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 4100/8759 | 1.67 files/s | ETA≈46.4m | docs_kept=54,396\n",
            "[2022] 4200/8759 | 1.68 files/s | ETA≈45.3m | docs_kept=55,146\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 4300/8759 | 1.68 files/s | ETA≈44.2m | docs_kept=56,598\n",
            "[2022] 4400/8759 | 1.69 files/s | ETA≈43.1m | docs_kept=57,671\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 4500/8759 | 1.69 files/s | ETA≈42.0m | docs_kept=58,879\n",
            "[2022] 4600/8759 | 1.70 files/s | ETA≈40.9m | docs_kept=59,705\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 4700/8759 | 1.69 files/s | ETA≈40.0m | docs_kept=61,119\n",
            "[2022] 4800/8759 | 1.70 files/s | ETA≈38.9m | docs_kept=62,209\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 4900/8759 | 1.70 files/s | ETA≈37.9m | docs_kept=63,397\n",
            "[2022] 5000/8759 | 1.70 files/s | ETA≈36.8m | docs_kept=64,698\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 5100/8759 | 1.71 files/s | ETA≈35.7m | docs_kept=65,495\n",
            "[2022] 5200/8759 | 1.70 files/s | ETA≈34.9m | docs_kept=66,985\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 5300/8759 | 1.70 files/s | ETA≈33.8m | docs_kept=67,941\n",
            "[2022] 5400/8759 | 1.70 files/s | ETA≈32.8m | docs_kept=69,188\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 5500/8759 | 1.71 files/s | ETA≈31.8m | docs_kept=70,393\n",
            "[2022] 5600/8759 | 1.71 files/s | ETA≈30.7m | docs_kept=71,026\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 5700/8759 | 1.71 files/s | ETA≈29.8m | docs_kept=72,131\n",
            "[2022] 5800/8759 | 1.72 files/s | ETA≈28.7m | docs_kept=73,092\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 5900/8759 | 1.72 files/s | ETA≈27.7m | docs_kept=73,966\n",
            "[2022] 6000/8759 | 1.72 files/s | ETA≈26.7m | docs_kept=75,179\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 6100/8759 | 1.72 files/s | ETA≈25.7m | docs_kept=76,436\n",
            "[2022] 6200/8759 | 1.72 files/s | ETA≈24.8m | docs_kept=77,642\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 6300/8759 | 1.73 files/s | ETA≈23.8m | docs_kept=78,663\n",
            "[2022] 6400/8759 | 1.72 files/s | ETA≈22.8m | docs_kept=80,556\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 6500/8759 | 1.72 files/s | ETA≈21.8m | docs_kept=82,157\n",
            "[2022] 6600/8759 | 1.73 files/s | ETA≈20.8m | docs_kept=83,392\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 6700/8759 | 1.72 files/s | ETA≈19.9m | docs_kept=85,103\n",
            "[2022] 6800/8759 | 1.73 files/s | ETA≈18.9m | docs_kept=85,949\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 6900/8759 | 1.72 files/s | ETA≈18.0m | docs_kept=87,624\n",
            "[2022] 7000/8759 | 1.73 files/s | ETA≈17.0m | docs_kept=89,024\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 7100/8759 | 1.73 files/s | ETA≈16.0m | docs_kept=90,552\n",
            "[2022] 7200/8759 | 1.73 files/s | ETA≈15.1m | docs_kept=92,100\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 7300/8759 | 1.73 files/s | ETA≈14.1m | docs_kept=92,947\n",
            "[2022] 7400/8759 | 1.73 files/s | ETA≈13.1m | docs_kept=94,400\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 7500/8759 | 1.73 files/s | ETA≈12.1m | docs_kept=95,573\n",
            "[2022] 7600/8759 | 1.73 files/s | ETA≈11.1m | docs_kept=96,740\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 7700/8759 | 1.73 files/s | ETA≈10.2m | docs_kept=98,261\n",
            "[2022] 7800/8759 | 1.73 files/s | ETA≈9.2m | docs_kept=99,210\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 7900/8759 | 1.74 files/s | ETA≈8.3m | docs_kept=100,231\n",
            "[2022] 8000/8759 | 1.74 files/s | ETA≈7.3m | docs_kept=101,112\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 8100/8759 | 1.74 files/s | ETA≈6.3m | docs_kept=102,529\n",
            "[2022] 8200/8759 | 1.74 files/s | ETA≈5.4m | docs_kept=103,715\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 8300/8759 | 1.74 files/s | ETA≈4.4m | docs_kept=104,579\n",
            "[2022] 8400/8759 | 1.74 files/s | ETA≈3.4m | docs_kept=105,717\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 8500/8759 | 1.74 files/s | ETA≈2.5m | docs_kept=106,686\n",
            "[2022] 8600/8759 | 1.74 files/s | ETA≈1.5m | docs_kept=107,594\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] 8700/8759 | 1.75 files/s | ETA≈0.6m | docs_kept=108,144\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2022/gdelt_uk_housing_monthly_2022.csv\n",
            "[2022] Done. This session processed: 8,759 | Total processed so far: 6,080 | Docs kept: 108,785\n",
            "Appended to master: /content/drive/MyDrive/msc_project/gdelt/gdelt_uk_housing_monthly_STITCHING.csv (47, 3)\n",
            "     Month  Docs   AvgTone\n",
            "2022-01-01  9787 -1.650206\n",
            "2022-02-01 10655 -2.277368\n",
            "2022-03-01 11831 -2.157864\n",
            "2022-04-01  8599 -1.889426\n",
            "2022-05-01  8322 -1.319196\n",
            "2022-06-01  7993 -1.258599\n",
            "2022-07-01  8188 -1.527057\n",
            "2022-08-01  8122 -1.488400\n",
            "2022-09-01  9378 -1.226548\n",
            "2022-10-01 10056 -1.589603\n",
            "2022-11-01  8419 -1.293812\n",
            "2022-12-01  7435 -1.095759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2023"
      ],
      "metadata": {
        "id": "VcZaU7rnJyPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import re, io, zipfile, math, time, json, logging, requests, pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "# ------------------------ CONFIG ------------------------\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "YEAR = 2023\n",
        "RESET_STATE = True\n",
        "MAX_FILES = None\n",
        "HOURLY_ONLY = True\n",
        "CHECKPOINT_EVERY = 200\n",
        "PROGRESS_EVERY   = 100\n",
        "\n",
        "# Date window from YEAR (dynamic!)\n",
        "START_TS = f\"{YEAR}0101000000\"  # YYYY-01-01 00:00:00\n",
        "END_TS   = f\"{YEAR}1231235959\"  # YYYY-12-31 23:59:59\n",
        "# Filters\n",
        "UK_REGEX = r'#(?:GBR|GB|UK|United Kingdom|England|Scotland|Wales|Northern Ireland)#'\n",
        "THEME_PATTERN = r'(?:HOUS|MORTGAG|REMORTGAG|RENT|TENAN|REAL[ _]?ESTATE|PROPERTY|HOME[ _]?PRIC|HOUSE[ _]?PRIC|RIGHTMOVE|ZOOPLA|LANDLORD|BUY[ -]?TO[ -]?LET)'\n",
        "\n",
        "# GKG v2.1 columns (0-based indices)\n",
        "DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I = 1, 8, 10, 15\n",
        "\n",
        "# Paths\n",
        "OUT_DIR = BASE_DIR / f\"gdelt_run_{YEAR}\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "YEARLY_CSV = OUT_DIR / f\"gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "STATE_PATH = OUT_DIR / \"state.json\"\n",
        "MASTER_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
        "MASTER_CSV = BASE_DIR / \"gdelt_uk_housing_monthly_STITCHING.csv\"\n",
        "\n",
        "# ------------------------ Networking helpers ------------------------\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-colab/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def get(url, timeout=60, s=None):\n",
        "    s = s or session()\n",
        "    if url.startswith(\"https://\"):  # avoid SSL hiccups in Colab\n",
        "        url = \"http://\" + url[len(\"https://\"):]\n",
        "    r = s.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "# ------------------------ GDELT plumbing ------------------------\n",
        "def build_gkg_urls():\n",
        "    \"\"\"Collect GKG v2.1 .zip URLs within the YEAR bounds (top-of-hour if HOURLY_ONLY).\"\"\"\n",
        "    txt = get(MASTER_URL, timeout=90).text\n",
        "    urls = []\n",
        "    for line in txt.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3:\n",
        "            continue\n",
        "        url = parts[2]\n",
        "        m = re.search(r'/gdeltv2/(\\d{14})\\.gkg\\.csv\\.zip$', url)\n",
        "        if not m:\n",
        "            continue\n",
        "        ts = m.group(1)\n",
        "        if START_TS <= ts <= END_TS:\n",
        "            if HOURLY_ONLY and ts[10:12] != \"00\":  # only top-of-hour files\n",
        "                continue\n",
        "            urls.append(url.replace(\"https://\", \"http://\"))\n",
        "    urls.sort()\n",
        "    return urls\n",
        "\n",
        "def month_from_dateint(dateint):\n",
        "    dt = datetime.strptime(str(dateint), \"%Y%m%d%H%M%S\")\n",
        "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "\n",
        "def doc_tone(v2tone):\n",
        "    try:\n",
        "        # V2Tone = \"docTone, pos, neg, polarity, activityRefDensity, selfGroupRefDensity, wordCount\"\n",
        "        return float(str(v2tone).split(\",\")[0])\n",
        "    except Exception:\n",
        "        return math.nan\n",
        "\n",
        "def process_df(df, counts, tone_sum, tone_n, uk_re, theme_re):\n",
        "    m_uk = df[V2LOCS_I].astype(str).str.contains(uk_re, na=False, regex=True)\n",
        "    m_th = df[V2THEMES_I].astype(str).str.contains(theme_re, na=False, regex=True)\n",
        "    sub = df.loc[m_uk & m_th, [DATE_I, V2TONE_I]]\n",
        "    if sub.empty:\n",
        "        return 0\n",
        "    months = sub[DATE_I].astype(str).apply(month_from_dateint)\n",
        "    tones  = sub[V2TONE_I].apply(doc_tone)\n",
        "    kept = 0\n",
        "    for m, t in zip(months, tones):\n",
        "        counts[m] += 1\n",
        "        if not math.isnan(t):\n",
        "            tone_sum[m] += t\n",
        "            tone_n[m] += 1\n",
        "        kept += 1\n",
        "    return kept\n",
        "\n",
        "def write_year_output(processed_count, processed_tail, counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m]/tone_n[m]) if tone_n[m] else None\n",
        "        rows.append({\"month\": f\"{m}-01\", \"docs\": n, \"avg_tone\": avg})\n",
        "    monthly = pd.DataFrame(rows, columns=[\"month\",\"docs\",\"avg_tone\"]).sort_values(\"month\")\n",
        "    monthly.to_csv(YEARLY_CSV, index=False)\n",
        "    json.dump({\n",
        "        \"processed_count\": processed_count,\n",
        "        \"processed_urls_tail\": processed_tail[-20000:],  # retain a long tail for resume\n",
        "        \"counts\": counts, \"tone_sum\": tone_sum, \"tone_n\": tone_n\n",
        "    }, open(STATE_PATH, \"w\"))\n",
        "    print(f\"Saved → {YEARLY_CSV}\")\n",
        "\n",
        "# ------------------------ Reset (if previously wrong year) ------------------------\n",
        "if RESET_STATE:\n",
        "    if STATE_PATH.exists():\n",
        "        STATE_PATH.unlink()\n",
        "    if YEARLY_CSV.exists():\n",
        "        YEARLY_CSV.unlink()\n",
        "\n",
        "# ------------------------ Plan + resume ------------------------\n",
        "s = session()\n",
        "urls_all = build_gkg_urls()\n",
        "print(f\"[{YEAR}] planned hourly files: {len(urls_all):,}\")\n",
        "\n",
        "if STATE_PATH.exists():\n",
        "    state = json.load(open(STATE_PATH))\n",
        "    processed_count     = int(state.get(\"processed_count\", 0))\n",
        "    processed_urls_tail = state.get(\"processed_urls_tail\", [])\n",
        "    counts   = defaultdict(int,   state.get(\"counts\", {}))\n",
        "    tone_sum = defaultdict(float, state.get(\"tone_sum\", {}))\n",
        "    tone_n   = defaultdict(int,   state.get(\"tone_n\", {}))\n",
        "    print(f\"Resuming: {processed_count:,} files done previously.\")\n",
        "else:\n",
        "    processed_count, processed_urls_tail = 0, []\n",
        "    counts, tone_sum, tone_n = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "already = set(processed_urls_tail)\n",
        "todo = [u for u in urls_all if u not in already]\n",
        "if MAX_FILES is not None:\n",
        "    todo = todo[:MAX_FILES]\n",
        "print(f\"[{YEAR}] processing this session: {len(todo):,} files\")\n",
        "\n",
        "uk_re = re.compile(UK_REGEX, re.I)\n",
        "theme_re = re.compile(THEME_PATTERN, re.I)\n",
        "\n",
        "# ------------------------ Main loop ------------------------\n",
        "t0 = time.time()\n",
        "docs_seen = 0\n",
        "\n",
        "for i, url in enumerate(todo, 1):\n",
        "    try:\n",
        "        z = zipfile.ZipFile(io.BytesIO(get(url, timeout=90, s=s).content))\n",
        "        name = z.namelist()[0]\n",
        "        df = pd.read_csv(\n",
        "            z.open(name),\n",
        "            sep=\"\\t\", header=None, quoting=3, low_memory=False,\n",
        "            usecols=[DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I],\n",
        "            dtype={DATE_I:str, V2THEMES_I:str, V2LOCS_I:str, V2TONE_I:str}\n",
        "        )\n",
        "        docs_seen += process_df(df, counts, tone_sum, tone_n, uk_re, theme_re)\n",
        "        processed_count += 1\n",
        "        processed_urls_tail.append(url)\n",
        "        if len(processed_urls_tail) > 20000:\n",
        "            processed_urls_tail = processed_urls_tail[-20000:]\n",
        "    except Exception:\n",
        "        # skip bad zips quietly\n",
        "        pass\n",
        "\n",
        "    if i % PROGRESS_EVERY == 0:\n",
        "        elapsed = max(1e-6, time.time()-t0)\n",
        "        rate = i / elapsed\n",
        "        eta_min = (len(todo)-i) / max(1e-6, rate) / 60\n",
        "        print(f\"[{YEAR}] {i}/{len(todo)} | {rate:.2f} files/s | ETA≈{eta_min:.1f}m | docs_kept={docs_seen:,}\")\n",
        "\n",
        "    if i % CHECKPOINT_EVERY == 0:\n",
        "        write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "\n",
        "# final write for the year\n",
        "write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "print(f\"[{YEAR}] Done. This session processed: {len(todo):,} | Total processed so far: {processed_count:,} | Docs kept: {docs_seen:,}\")\n",
        "\n",
        "# ------------------------ Append to master ------------------------\n",
        "try:\n",
        "    ydf = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # sanity: ensure we really wrote YEAR\n",
        "    years_found = set(ydf[\"month\"].dt.year.dropna().unique().tolist())\n",
        "    if years_found != {YEAR}:\n",
        "        raise ValueError(f\"Year mismatch: expected {YEAR}, found {sorted(years_found)} in {YEARLY_CSV}\")\n",
        "\n",
        "    if MASTER_CSV.exists():\n",
        "        m = pd.read_csv(MASTER_CSV, parse_dates=[\"month\"])\n",
        "        m = (pd.concat([m, ydf], ignore_index=True)\n",
        "               .drop_duplicates(subset=\"month\", keep=\"last\")\n",
        "               .sort_values(\"month\")\n",
        "               .reset_index(drop=True))\n",
        "    else:\n",
        "        m = ydf\n",
        "    m.to_csv(MASTER_CSV, index=False)\n",
        "    print(\"Appended to master:\", MASTER_CSV, m.shape)\n",
        "except Exception as e:\n",
        "    print(\"Append-to-master skipped:\", e)\n",
        "\n",
        "# ------------------------ Pretty print monthly for YEAR ------------------------\n",
        "try:\n",
        "    df = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # build a complete monthly index for YEAR so missing months show as NaN\n",
        "    full = (df.set_index(\"month\")\n",
        "              .reindex(pd.date_range(f\"{YEAR}-01-01\", f\"{YEAR}-12-01\", freq=\"MS\"))\n",
        "              .rename_axis(\"Month\").reset_index())\n",
        "    full[\"Docs\"]    = pd.to_numeric(full[\"docs\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "    full[\"AvgTone\"] = pd.to_numeric(full[\"avg_tone\"], errors=\"coerce\")\n",
        "    print(full[[\"Month\",\"Docs\",\"AvgTone\"]].to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Preview skipped:\", e)\n",
        "\n"
      ],
      "metadata": {
        "id": "GvBH8XOkK1AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2024"
      ],
      "metadata": {
        "id": "Ls3DS7pe8FaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import re, io, zipfile, math, time, json, logging, requests, pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "# ------------------------ CONFIG ------------------------\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")  # change if you like\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "YEAR = 2024                     # <<< set the year you want to process\n",
        "RESET_STATE = True              # <<< set True if you previously ran the wrong year; else False\n",
        "MAX_FILES = None                # cap files processed this run; None = process all remaining\n",
        "HOURLY_ONLY = True              # process top-of-hour files only (lighter than every 15 minutes)\n",
        "CHECKPOINT_EVERY = 200\n",
        "PROGRESS_EVERY   = 100\n",
        "\n",
        "# Date window from YEAR (dynamic!)\n",
        "START_TS = f\"{YEAR}0101000000\"  # YYYY-01-01 00:00:00\n",
        "END_TS   = f\"{YEAR}1231235959\"  # YYYY-12-31 23:59:59\n",
        "# Filters\n",
        "UK_REGEX = r'#(?:GBR|GB|UK|United Kingdom|England|Scotland|Wales|Northern Ireland)#'\n",
        "THEME_PATTERN = r'(?:HOUS|MORTGAG|REMORTGAG|RENT|TENAN|REAL[ _]?ESTATE|PROPERTY|HOME[ _]?PRIC|HOUSE[ _]?PRIC|RIGHTMOVE|ZOOPLA|LANDLORD|BUY[ -]?TO[ -]?LET)'\n",
        "\n",
        "# GKG v2.1 columns (0-based indices)\n",
        "DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I = 1, 8, 10, 15\n",
        "\n",
        "# Paths\n",
        "OUT_DIR = BASE_DIR / f\"gdelt_run_{YEAR}\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "YEARLY_CSV = OUT_DIR / f\"gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "STATE_PATH = OUT_DIR / \"state.json\"\n",
        "MASTER_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
        "MASTER_CSV = BASE_DIR / \"gdelt_uk_housing_monthly_STITCHING.csv\"\n",
        "\n",
        "# ------------------------ Networking helpers ------------------------\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-colab/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def get(url, timeout=60, s=None):\n",
        "    s = s or session()\n",
        "    if url.startswith(\"https://\"):  # avoid SSL hiccups in Colab\n",
        "        url = \"http://\" + url[len(\"https://\"):]\n",
        "    r = s.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "# ------------------------ GDELT plumbing ------------------------\n",
        "def build_gkg_urls():\n",
        "    \"\"\"Collect GKG v2.1 .zip URLs within the YEAR bounds (top-of-hour if HOURLY_ONLY).\"\"\"\n",
        "    txt = get(MASTER_URL, timeout=90).text\n",
        "    urls = []\n",
        "    for line in txt.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3:\n",
        "            continue\n",
        "        url = parts[2]\n",
        "        m = re.search(r'/gdeltv2/(\\d{14})\\.gkg\\.csv\\.zip$', url)\n",
        "        if not m:\n",
        "            continue\n",
        "        ts = m.group(1)\n",
        "        if START_TS <= ts <= END_TS:\n",
        "            if HOURLY_ONLY and ts[10:12] != \"00\":  # only top-of-hour files\n",
        "                continue\n",
        "            urls.append(url.replace(\"https://\", \"http://\"))\n",
        "    urls.sort()\n",
        "    return urls\n",
        "\n",
        "def month_from_dateint(dateint):\n",
        "    dt = datetime.strptime(str(dateint), \"%Y%m%d%H%M%S\")\n",
        "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "\n",
        "def doc_tone(v2tone):\n",
        "    try:\n",
        "        # V2Tone = \"docTone, pos, neg, polarity, activityRefDensity, selfGroupRefDensity, wordCount\"\n",
        "        return float(str(v2tone).split(\",\")[0])\n",
        "    except Exception:\n",
        "        return math.nan\n",
        "\n",
        "def process_df(df, counts, tone_sum, tone_n, uk_re, theme_re):\n",
        "    m_uk = df[V2LOCS_I].astype(str).str.contains(uk_re, na=False, regex=True)\n",
        "    m_th = df[V2THEMES_I].astype(str).str.contains(theme_re, na=False, regex=True)\n",
        "    sub = df.loc[m_uk & m_th, [DATE_I, V2TONE_I]]\n",
        "    if sub.empty:\n",
        "        return 0\n",
        "    months = sub[DATE_I].astype(str).apply(month_from_dateint)\n",
        "    tones  = sub[V2TONE_I].apply(doc_tone)\n",
        "    kept = 0\n",
        "    for m, t in zip(months, tones):\n",
        "        counts[m] += 1\n",
        "        if not math.isnan(t):\n",
        "            tone_sum[m] += t\n",
        "            tone_n[m] += 1\n",
        "        kept += 1\n",
        "    return kept\n",
        "\n",
        "def write_year_output(processed_count, processed_tail, counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m]/tone_n[m]) if tone_n[m] else None\n",
        "        rows.append({\"month\": f\"{m}-01\", \"docs\": n, \"avg_tone\": avg})\n",
        "    monthly = pd.DataFrame(rows, columns=[\"month\",\"docs\",\"avg_tone\"]).sort_values(\"month\")\n",
        "    monthly.to_csv(YEARLY_CSV, index=False)\n",
        "    json.dump({\n",
        "        \"processed_count\": processed_count,\n",
        "        \"processed_urls_tail\": processed_tail[-20000:],  # retain a long tail for resume\n",
        "        \"counts\": counts, \"tone_sum\": tone_sum, \"tone_n\": tone_n\n",
        "    }, open(STATE_PATH, \"w\"))\n",
        "    print(f\"Saved → {YEARLY_CSV}\")\n",
        "\n",
        "# ------------------------ Reset (if previously wrong year) ------------------------\n",
        "if RESET_STATE:\n",
        "    if STATE_PATH.exists():\n",
        "        STATE_PATH.unlink()\n",
        "    if YEARLY_CSV.exists():\n",
        "        YEARLY_CSV.unlink()\n",
        "\n",
        "# ------------------------ Plan + resume ------------------------\n",
        "s = session()\n",
        "urls_all = build_gkg_urls()\n",
        "print(f\"[{YEAR}] planned hourly files: {len(urls_all):,}\")\n",
        "\n",
        "if STATE_PATH.exists():\n",
        "    state = json.load(open(STATE_PATH))\n",
        "    processed_count     = int(state.get(\"processed_count\", 0))\n",
        "    processed_urls_tail = state.get(\"processed_urls_tail\", [])\n",
        "    counts   = defaultdict(int,   state.get(\"counts\", {}))\n",
        "    tone_sum = defaultdict(float, state.get(\"tone_sum\", {}))\n",
        "    tone_n   = defaultdict(int,   state.get(\"tone_n\", {}))\n",
        "    print(f\"Resuming: {processed_count:,} files done previously.\")\n",
        "else:\n",
        "    processed_count, processed_urls_tail = 0, []\n",
        "    counts, tone_sum, tone_n = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "already = set(processed_urls_tail)\n",
        "todo = [u for u in urls_all if u not in already]\n",
        "if MAX_FILES is not None:\n",
        "    todo = todo[:MAX_FILES]\n",
        "print(f\"[{YEAR}] processing this session: {len(todo):,} files\")\n",
        "\n",
        "uk_re = re.compile(UK_REGEX, re.I)\n",
        "theme_re = re.compile(THEME_PATTERN, re.I)\n",
        "\n",
        "# ------------------------ Main loop ------------------------\n",
        "t0 = time.time()\n",
        "docs_seen = 0\n",
        "\n",
        "for i, url in enumerate(todo, 1):\n",
        "    try:\n",
        "        z = zipfile.ZipFile(io.BytesIO(get(url, timeout=90, s=s).content))\n",
        "        name = z.namelist()[0]\n",
        "        df = pd.read_csv(\n",
        "            z.open(name),\n",
        "            sep=\"\\t\", header=None, quoting=3, low_memory=False,\n",
        "            usecols=[DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I],\n",
        "            dtype={DATE_I:str, V2THEMES_I:str, V2LOCS_I:str, V2TONE_I:str}\n",
        "        )\n",
        "        docs_seen += process_df(df, counts, tone_sum, tone_n, uk_re, theme_re)\n",
        "        processed_count += 1\n",
        "        processed_urls_tail.append(url)\n",
        "        if len(processed_urls_tail) > 20000:\n",
        "            processed_urls_tail = processed_urls_tail[-20000:]\n",
        "    except Exception:\n",
        "        # skip bad zips quietly\n",
        "        pass\n",
        "\n",
        "    if i % PROGRESS_EVERY == 0:\n",
        "        elapsed = max(1e-6, time.time()-t0)\n",
        "        rate = i / elapsed\n",
        "        eta_min = (len(todo)-i) / max(1e-6, rate) / 60\n",
        "        print(f\"[{YEAR}] {i}/{len(todo)} | {rate:.2f} files/s | ETA≈{eta_min:.1f}m | docs_kept={docs_seen:,}\")\n",
        "\n",
        "    if i % CHECKPOINT_EVERY == 0:\n",
        "        write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "\n",
        "# final write for the year\n",
        "write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "print(f\"[{YEAR}] Done. This session processed: {len(todo):,} | Total processed so far: {processed_count:,} | Docs kept: {docs_seen:,}\")\n",
        "\n",
        "# ------------------------ Append to master ------------------------\n",
        "try:\n",
        "    ydf = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # sanity: ensure we really wrote YEAR\n",
        "    years_found = set(ydf[\"month\"].dt.year.dropna().unique().tolist())\n",
        "    if years_found != {YEAR}:\n",
        "        raise ValueError(f\"Year mismatch: expected {YEAR}, found {sorted(years_found)} in {YEARLY_CSV}\")\n",
        "\n",
        "    if MASTER_CSV.exists():\n",
        "        m = pd.read_csv(MASTER_CSV, parse_dates=[\"month\"])\n",
        "        m = (pd.concat([m, ydf], ignore_index=True)\n",
        "               .drop_duplicates(subset=\"month\", keep=\"last\")\n",
        "               .sort_values(\"month\")\n",
        "               .reset_index(drop=True))\n",
        "    else:\n",
        "        m = ydf\n",
        "    m.to_csv(MASTER_CSV, index=False)\n",
        "    print(\"Appended to master:\", MASTER_CSV, m.shape)\n",
        "except Exception as e:\n",
        "    print(\"Append-to-master skipped:\", e)\n",
        "\n",
        "# ------------------------ Pretty print monthly for YEAR ------------------------\n",
        "try:\n",
        "    df = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # build a complete monthly index for YEAR so missing months show as NaN\n",
        "    full = (df.set_index(\"month\")\n",
        "              .reindex(pd.date_range(f\"{YEAR}-01-01\", f\"{YEAR}-12-01\", freq=\"MS\"))\n",
        "              .rename_axis(\"Month\").reset_index())\n",
        "    full[\"Docs\"]    = pd.to_numeric(full[\"docs\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "    full[\"AvgTone\"] = pd.to_numeric(full[\"avg_tone\"], errors=\"coerce\")\n",
        "    print(full[[\"Month\",\"Docs\",\"AvgTone\"]].to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Preview skipped:\", e)\n",
        "# ========================= end =========================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs1SR51d8HBD",
        "outputId": "78768c32-cc05-40c6-b852-5479ffcb7a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[2024] planned hourly files: 8,784\n",
            "[2024] processing this session: 8,784 files\n",
            "[2024] 100/8784 | 1.29 files/s | ETA≈112.3m | docs_kept=2,007\n",
            "[2024] 200/8784 | 1.34 files/s | ETA≈106.6m | docs_kept=3,537\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 300/8784 | 1.24 files/s | ETA≈114.4m | docs_kept=6,104\n",
            "[2024] 400/8784 | 1.26 files/s | ETA≈111.1m | docs_kept=7,980\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 500/8784 | 1.23 files/s | ETA≈112.0m | docs_kept=10,277\n",
            "[2024] 600/8784 | 1.20 files/s | ETA≈113.9m | docs_kept=12,709\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 700/8784 | 1.22 files/s | ETA≈110.2m | docs_kept=14,558\n",
            "[2024] 800/8784 | 1.20 files/s | ETA≈110.5m | docs_kept=16,853\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 900/8784 | 1.22 files/s | ETA≈107.7m | docs_kept=19,266\n",
            "[2024] 1000/8784 | 1.22 files/s | ETA≈106.1m | docs_kept=21,309\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 1100/8784 | 1.21 files/s | ETA≈105.5m | docs_kept=23,640\n",
            "[2024] 1200/8784 | 1.23 files/s | ETA≈103.0m | docs_kept=25,667\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 1300/8784 | 1.21 files/s | ETA≈103.3m | docs_kept=28,236\n",
            "[2024] 1400/8784 | 1.22 files/s | ETA≈101.2m | docs_kept=30,096\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 1500/8784 | 1.21 files/s | ETA≈99.9m | docs_kept=32,541\n",
            "[2024] 1600/8784 | 1.21 files/s | ETA≈99.1m | docs_kept=34,757\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 1700/8784 | 1.21 files/s | ETA≈97.3m | docs_kept=36,288\n",
            "[2024] 1800/8784 | 1.20 files/s | ETA≈96.7m | docs_kept=38,777\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 1900/8784 | 1.21 files/s | ETA≈94.7m | docs_kept=40,905\n",
            "[2024] 2000/8784 | 1.21 files/s | ETA≈93.6m | docs_kept=43,094\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 2100/8784 | 1.21 files/s | ETA≈92.3m | docs_kept=45,621\n",
            "[2024] 2200/8784 | 1.22 files/s | ETA≈90.1m | docs_kept=47,035\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 2300/8784 | 1.21 files/s | ETA≈89.4m | docs_kept=50,030\n",
            "[2024] 2400/8784 | 1.22 files/s | ETA≈87.5m | docs_kept=51,511\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 2500/8784 | 1.22 files/s | ETA≈86.1m | docs_kept=53,878\n",
            "[2024] 2600/8784 | 1.22 files/s | ETA≈84.7m | docs_kept=56,319\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 2700/8784 | 1.22 files/s | ETA≈82.9m | docs_kept=58,051\n",
            "[2024] 2800/8784 | 1.21 files/s | ETA≈82.2m | docs_kept=60,527\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 2900/8784 | 1.22 files/s | ETA≈80.6m | docs_kept=62,089\n",
            "[2024] 3000/8784 | 1.21 files/s | ETA≈79.5m | docs_kept=63,769\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 3100/8784 | 1.21 files/s | ETA≈78.2m | docs_kept=65,743\n",
            "[2024] 3200/8784 | 1.22 files/s | ETA≈76.5m | docs_kept=67,409\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 3300/8784 | 1.21 files/s | ETA≈75.4m | docs_kept=69,736\n",
            "[2024] 3400/8784 | 1.22 files/s | ETA≈73.7m | docs_kept=71,396\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 3500/8784 | 1.22 files/s | ETA≈72.5m | docs_kept=73,623\n",
            "[2024] 3600/8784 | 1.22 files/s | ETA≈70.9m | docs_kept=75,284\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 3700/8784 | 1.22 files/s | ETA≈69.4m | docs_kept=77,355\n",
            "[2024] 3800/8784 | 1.22 files/s | ETA≈68.3m | docs_kept=79,697\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 3900/8784 | 1.22 files/s | ETA≈66.7m | docs_kept=81,233\n",
            "[2024] 4000/8784 | 1.22 files/s | ETA≈65.5m | docs_kept=83,666\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 4100/8784 | 1.22 files/s | ETA≈64.0m | docs_kept=85,751\n",
            "[2024] 4200/8784 | 1.22 files/s | ETA≈62.4m | docs_kept=87,498\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 4300/8784 | 1.22 files/s | ETA≈61.2m | docs_kept=89,927\n",
            "[2024] 4400/8784 | 1.23 files/s | ETA≈59.6m | docs_kept=91,518\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 4500/8784 | 1.23 files/s | ETA≈58.2m | docs_kept=94,079\n",
            "[2024] 4600/8784 | 1.23 files/s | ETA≈56.6m | docs_kept=95,940\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 4700/8784 | 1.23 files/s | ETA≈55.2m | docs_kept=98,077\n",
            "[2024] 4800/8784 | 1.23 files/s | ETA≈54.0m | docs_kept=100,442\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 4900/8784 | 1.23 files/s | ETA≈52.5m | docs_kept=102,464\n",
            "[2024] 5000/8784 | 1.23 files/s | ETA≈51.2m | docs_kept=104,477\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 5100/8784 | 1.23 files/s | ETA≈49.7m | docs_kept=106,238\n",
            "[2024] 5200/8784 | 1.24 files/s | ETA≈48.3m | docs_kept=107,712\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 5300/8784 | 1.23 files/s | ETA≈47.0m | docs_kept=109,878\n",
            "[2024] 5400/8784 | 1.24 files/s | ETA≈45.5m | docs_kept=111,303\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 5500/8784 | 1.24 files/s | ETA≈44.3m | docs_kept=113,564\n",
            "[2024] 5600/8784 | 1.24 files/s | ETA≈42.8m | docs_kept=115,305\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 5700/8784 | 1.24 files/s | ETA≈41.4m | docs_kept=116,784\n",
            "[2024] 5800/8784 | 1.24 files/s | ETA≈40.1m | docs_kept=118,531\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 5900/8784 | 1.24 files/s | ETA≈38.6m | docs_kept=120,099\n",
            "[2024] 6000/8784 | 1.24 files/s | ETA≈37.3m | docs_kept=122,304\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 6100/8784 | 1.24 files/s | ETA≈35.9m | docs_kept=123,779\n",
            "[2024] 6200/8784 | 1.24 files/s | ETA≈34.6m | docs_kept=125,899\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 6300/8784 | 1.24 files/s | ETA≈33.3m | docs_kept=127,978\n",
            "[2024] 6400/8784 | 1.25 files/s | ETA≈31.8m | docs_kept=129,581\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 6500/8784 | 1.24 files/s | ETA≈30.6m | docs_kept=132,106\n",
            "[2024] 6600/8784 | 1.25 files/s | ETA≈29.2m | docs_kept=134,002\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 6700/8784 | 1.25 files/s | ETA≈27.8m | docs_kept=135,904\n",
            "[2024] 6800/8784 | 1.25 files/s | ETA≈26.5m | docs_kept=137,915\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 6900/8784 | 1.25 files/s | ETA≈25.1m | docs_kept=139,386\n",
            "[2024] 7000/8784 | 1.25 files/s | ETA≈23.8m | docs_kept=141,991\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 7100/8784 | 1.25 files/s | ETA≈22.4m | docs_kept=143,813\n",
            "[2024] 7200/8784 | 1.25 files/s | ETA≈21.1m | docs_kept=146,149\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 7300/8784 | 1.25 files/s | ETA≈19.8m | docs_kept=148,203\n",
            "[2024] 7400/8784 | 1.25 files/s | ETA≈18.4m | docs_kept=149,782\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 7500/8784 | 1.25 files/s | ETA≈17.1m | docs_kept=152,556\n",
            "[2024] 7600/8784 | 1.26 files/s | ETA≈15.7m | docs_kept=154,486\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 7700/8784 | 1.25 files/s | ETA≈14.4m | docs_kept=156,578\n",
            "[2024] 7800/8784 | 1.26 files/s | ETA≈13.1m | docs_kept=158,556\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 7900/8784 | 1.26 files/s | ETA≈11.7m | docs_kept=160,523\n",
            "[2024] 8000/8784 | 1.26 files/s | ETA≈10.4m | docs_kept=162,450\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 8100/8784 | 1.26 files/s | ETA≈9.0m | docs_kept=163,950\n",
            "[2024] 8200/8784 | 1.26 files/s | ETA≈7.7m | docs_kept=165,835\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 8300/8784 | 1.26 files/s | ETA≈6.4m | docs_kept=167,590\n",
            "[2024] 8400/8784 | 1.26 files/s | ETA≈5.1m | docs_kept=169,244\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 8500/8784 | 1.26 files/s | ETA≈3.8m | docs_kept=171,846\n",
            "[2024] 8600/8784 | 1.26 files/s | ETA≈2.4m | docs_kept=173,711\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] 8700/8784 | 1.27 files/s | ETA≈1.1m | docs_kept=174,596\n",
            "Saved → /content/drive/MyDrive/msc_project/gdelt/gdelt_run_2024/gdelt_uk_housing_monthly_2024.csv\n",
            "[2024] Done. This session processed: 8,784 | Total processed so far: 8,500 | Docs kept: 175,787\n",
            "Appended to master: /content/drive/MyDrive/msc_project/gdelt/gdelt_uk_housing_monthly_STITCHING.csv (71, 3)\n",
            "     Month  Docs   AvgTone\n",
            "2024-01-01 15510 -1.746478\n",
            "2024-02-01 15799 -1.457962\n",
            "2024-03-01 15592 -0.993905\n",
            "2024-04-01 15276 -1.586345\n",
            "2024-05-01 14532 -1.276888\n",
            "2024-06-01 14252 -0.894222\n",
            "2024-07-01 15514 -0.886735\n",
            "2024-08-01 12917 -1.414678\n",
            "2024-09-01 13948 -0.944154\n",
            "2024-10-01 15355 -1.140835\n",
            "2024-11-01 14335 -0.930308\n",
            "2024-12-01 12757 -0.933601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2025"
      ],
      "metadata": {
        "id": "QHHjvYMMJ26j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import re, io, zipfile, math, time, json, logging, requests, pandas as pd\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "# ------------------------ CONFIG ------------------------\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/msc_project/gdelt\")\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "YEAR = 2025\n",
        "RESET_STATE = True\n",
        "MAX_FILES = None\n",
        "HOURLY_ONLY = True\n",
        "CHECKPOINT_EVERY = 200\n",
        "PROGRESS_EVERY   = 100\n",
        "\n",
        "# Date window from YEAR (dynamic!)\n",
        "START_TS = f\"{YEAR}0101000000\"  # YYYY-01-01 00:00:00\n",
        "END_TS   = f\"{YEAR}0630235959\"  # YYYY-12-31 23:59:59\n",
        "# Filters\n",
        "UK_REGEX = r'#(?:GBR|GB|UK|United Kingdom|England|Scotland|Wales|Northern Ireland)#'\n",
        "THEME_PATTERN = r'(?:HOUS|MORTGAG|REMORTGAG|RENT|TENAN|REAL[ _]?ESTATE|PROPERTY|HOME[ _]?PRIC|HOUSE[ _]?PRIC|RIGHTMOVE|ZOOPLA|LANDLORD|BUY[ -]?TO[ -]?LET)'\n",
        "\n",
        "# GKG v2.1 columns (0-based indices)\n",
        "DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I = 1, 8, 10, 15\n",
        "\n",
        "# Paths\n",
        "OUT_DIR = BASE_DIR / f\"gdelt_run_{YEAR}\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "YEARLY_CSV = OUT_DIR / f\"gdelt_uk_housing_monthly_{YEAR}.csv\"\n",
        "STATE_PATH = OUT_DIR / \"state.json\"\n",
        "MASTER_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
        "MASTER_CSV = BASE_DIR / \"gdelt_uk_housing_monthly_STITCHING.csv\"\n",
        "\n",
        "# ------------------------ Networking helpers ------------------------\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
        "\n",
        "def session():\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": \"gdelt-colab/1.0\"})\n",
        "    retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429,500,502,503,504])\n",
        "    s.mount(\"http://\",  HTTPAdapter(max_retries=retries))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "    return s\n",
        "\n",
        "def get(url, timeout=60, s=None):\n",
        "    s = s or session()\n",
        "    if url.startswith(\"https://\"):  # avoid SSL hiccups in Colab\n",
        "        url = \"http://\" + url[len(\"https://\"):]\n",
        "    r = s.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "# ------------------------ GDELT plumbing ------------------------\n",
        "def build_gkg_urls():\n",
        "    \"\"\"Collect GKG v2.1 .zip URLs within the YEAR bounds (top-of-hour if HOURLY_ONLY).\"\"\"\n",
        "    txt = get(MASTER_URL, timeout=90).text\n",
        "    urls = []\n",
        "    for line in txt.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3:\n",
        "            continue\n",
        "        url = parts[2]\n",
        "        m = re.search(r'/gdeltv2/(\\d{14})\\.gkg\\.csv\\.zip$', url)\n",
        "        if not m:\n",
        "            continue\n",
        "        ts = m.group(1)\n",
        "        if START_TS <= ts <= END_TS:\n",
        "            if HOURLY_ONLY and ts[10:12] != \"00\":  # only top-of-hour files\n",
        "                continue\n",
        "            urls.append(url.replace(\"https://\", \"http://\"))\n",
        "    urls.sort()\n",
        "    return urls\n",
        "\n",
        "def month_from_dateint(dateint):\n",
        "    dt = datetime.strptime(str(dateint), \"%Y%m%d%H%M%S\")\n",
        "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
        "\n",
        "def doc_tone(v2tone):\n",
        "    try:\n",
        "        # V2Tone = \"docTone, pos, neg, polarity, activityRefDensity, selfGroupRefDensity, wordCount\"\n",
        "        return float(str(v2tone).split(\",\")[0])\n",
        "    except Exception:\n",
        "        return math.nan\n",
        "\n",
        "def process_df(df, counts, tone_sum, tone_n, uk_re, theme_re):\n",
        "    m_uk = df[V2LOCS_I].astype(str).str.contains(uk_re, na=False, regex=True)\n",
        "    m_th = df[V2THEMES_I].astype(str).str.contains(theme_re, na=False, regex=True)\n",
        "    sub = df.loc[m_uk & m_th, [DATE_I, V2TONE_I]]\n",
        "    if sub.empty:\n",
        "        return 0\n",
        "    months = sub[DATE_I].astype(str).apply(month_from_dateint)\n",
        "    tones  = sub[V2TONE_I].apply(doc_tone)\n",
        "    kept = 0\n",
        "    for m, t in zip(months, tones):\n",
        "        counts[m] += 1\n",
        "        if not math.isnan(t):\n",
        "            tone_sum[m] += t\n",
        "            tone_n[m] += 1\n",
        "        kept += 1\n",
        "    return kept\n",
        "\n",
        "def write_year_output(processed_count, processed_tail, counts, tone_sum, tone_n):\n",
        "    rows = []\n",
        "    for m in sorted(counts.keys()):\n",
        "        n = counts[m]\n",
        "        avg = (tone_sum[m]/tone_n[m]) if tone_n[m] else None\n",
        "        rows.append({\"month\": f\"{m}-01\", \"docs\": n, \"avg_tone\": avg})\n",
        "    monthly = pd.DataFrame(rows, columns=[\"month\",\"docs\",\"avg_tone\"]).sort_values(\"month\")\n",
        "    monthly.to_csv(YEARLY_CSV, index=False)\n",
        "    json.dump({\n",
        "        \"processed_count\": processed_count,\n",
        "        \"processed_urls_tail\": processed_tail[-20000:],  # retain a long tail for resume\n",
        "        \"counts\": counts, \"tone_sum\": tone_sum, \"tone_n\": tone_n\n",
        "    }, open(STATE_PATH, \"w\"))\n",
        "    print(f\"Saved → {YEARLY_CSV}\")\n",
        "\n",
        "# ------------------------ Reset (if previously wrong year) ------------------------\n",
        "if RESET_STATE:\n",
        "    if STATE_PATH.exists():\n",
        "        STATE_PATH.unlink()\n",
        "    if YEARLY_CSV.exists():\n",
        "        YEARLY_CSV.unlink()\n",
        "\n",
        "# ------------------------ Plan + resume ------------------------\n",
        "s = session()\n",
        "urls_all = build_gkg_urls()\n",
        "print(f\"[{YEAR}] planned hourly files: {len(urls_all):,}\")\n",
        "\n",
        "if STATE_PATH.exists():\n",
        "    state = json.load(open(STATE_PATH))\n",
        "    processed_count     = int(state.get(\"processed_count\", 0))\n",
        "    processed_urls_tail = state.get(\"processed_urls_tail\", [])\n",
        "    counts   = defaultdict(int,   state.get(\"counts\", {}))\n",
        "    tone_sum = defaultdict(float, state.get(\"tone_sum\", {}))\n",
        "    tone_n   = defaultdict(int,   state.get(\"tone_n\", {}))\n",
        "    print(f\"Resuming: {processed_count:,} files done previously.\")\n",
        "else:\n",
        "    processed_count, processed_urls_tail = 0, []\n",
        "    counts, tone_sum, tone_n = defaultdict(int), defaultdict(float), defaultdict(int)\n",
        "\n",
        "already = set(processed_urls_tail)\n",
        "todo = [u for u in urls_all if u not in already]\n",
        "if MAX_FILES is not None:\n",
        "    todo = todo[:MAX_FILES]\n",
        "print(f\"[{YEAR}] processing this session: {len(todo):,} files\")\n",
        "\n",
        "uk_re = re.compile(UK_REGEX, re.I)\n",
        "theme_re = re.compile(THEME_PATTERN, re.I)\n",
        "\n",
        "# ------------------------ Main loop ------------------------\n",
        "t0 = time.time()\n",
        "docs_seen = 0\n",
        "\n",
        "for i, url in enumerate(todo, 1):\n",
        "    try:\n",
        "        z = zipfile.ZipFile(io.BytesIO(get(url, timeout=90, s=s).content))\n",
        "        name = z.namelist()[0]\n",
        "        df = pd.read_csv(\n",
        "            z.open(name),\n",
        "            sep=\"\\t\", header=None, quoting=3, low_memory=False,\n",
        "            usecols=[DATE_I, V2THEMES_I, V2LOCS_I, V2TONE_I],\n",
        "            dtype={DATE_I:str, V2THEMES_I:str, V2LOCS_I:str, V2TONE_I:str}\n",
        "        )\n",
        "        docs_seen += process_df(df, counts, tone_sum, tone_n, uk_re, theme_re)\n",
        "        processed_count += 1\n",
        "        processed_urls_tail.append(url)\n",
        "        if len(processed_urls_tail) > 20000:\n",
        "            processed_urls_tail = processed_urls_tail[-20000:]\n",
        "    except Exception:\n",
        "        # skip bad zips quietly\n",
        "        pass\n",
        "\n",
        "    if i % PROGRESS_EVERY == 0:\n",
        "        elapsed = max(1e-6, time.time()-t0)\n",
        "        rate = i / elapsed\n",
        "        eta_min = (len(todo)-i) / max(1e-6, rate) / 60\n",
        "        print(f\"[{YEAR}] {i}/{len(todo)} | {rate:.2f} files/s | ETA≈{eta_min:.1f}m | docs_kept={docs_seen:,}\")\n",
        "\n",
        "    if i % CHECKPOINT_EVERY == 0:\n",
        "        write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "\n",
        "# final write for the year\n",
        "write_year_output(processed_count, processed_urls_tail, counts, tone_sum, tone_n)\n",
        "print(f\"[{YEAR}] Done. This session processed: {len(todo):,} | Total processed so far: {processed_count:,} | Docs kept: {docs_seen:,}\")\n",
        "\n",
        "# ------------------------ Append to master ------------------------\n",
        "try:\n",
        "    ydf = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # sanity: ensure we really wrote YEAR\n",
        "    years_found = set(ydf[\"month\"].dt.year.dropna().unique().tolist())\n",
        "    if years_found != {YEAR}:\n",
        "        raise ValueError(f\"Year mismatch: expected {YEAR}, found {sorted(years_found)} in {YEARLY_CSV}\")\n",
        "\n",
        "    if MASTER_CSV.exists():\n",
        "        m = pd.read_csv(MASTER_CSV, parse_dates=[\"month\"])\n",
        "        m = (pd.concat([m, ydf], ignore_index=True)\n",
        "               .drop_duplicates(subset=\"month\", keep=\"last\")\n",
        "               .sort_values(\"month\")\n",
        "               .reset_index(drop=True))\n",
        "    else:\n",
        "        m = ydf\n",
        "    m.to_csv(MASTER_CSV, index=False)\n",
        "    print(\"Appended to master:\", MASTER_CSV, m.shape)\n",
        "except Exception as e:\n",
        "    print(\"Append-to-master skipped:\", e)\n",
        "\n",
        "# ------------------------ Pretty print monthly for YEAR ------------------------\n",
        "try:\n",
        "    df = pd.read_csv(YEARLY_CSV, parse_dates=[\"month\"]).sort_values(\"month\")\n",
        "    # build a complete monthly index for YEAR so missing months show as NaN\n",
        "    full = (df.set_index(\"month\")\n",
        "              .reindex(pd.date_range(f\"{YEAR}-01-01\", f\"{YEAR}-12-01\", freq=\"MS\"))\n",
        "              .rename_axis(\"Month\").reset_index())\n",
        "    full[\"Docs\"]    = pd.to_numeric(full[\"docs\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "    full[\"AvgTone\"] = pd.to_numeric(full[\"avg_tone\"], errors=\"coerce\")\n",
        "    print(full[[\"Month\",\"Docs\",\"AvgTone\"]].to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Preview skipped:\", e)\n",
        "\n"
      ],
      "metadata": {
        "id": "hrvrosuwLE9N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}