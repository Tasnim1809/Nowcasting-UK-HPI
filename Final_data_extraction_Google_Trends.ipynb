{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This file merges the 5 files downloaded from Google Keyword Planner App."
      ],
      "metadata": {
        "id": "5J6dy2ohMlm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "WLLldeDAMl3W",
        "outputId": "c139a497-3174-4d1c-a052-2a0e064178cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-522b6177-c0ad-4b25-8136-ef79b164d409\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-522b6177-c0ad-4b25-8136-ef79b164d409\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving buying_demand_keywords.csv to buying_demand_keywords.csv\n",
            "Saving economic_policy_keywords.csv to economic_policy_keywords.csv\n",
            "Saving market_awareness_keywords.csv to market_awareness_keywords.csv\n",
            "Saving mortgage_financing_keywords.csv to mortgage_financing_keywords.csv\n",
            "Saving renting and affordability_keywords.csv to renting and affordability_keywords.csv\n",
            "Saving trends_long_weekly.csv to trends_long_weekly.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytrends"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH-Tbhq2fSxb",
        "outputId": "3e67b039-76ed-43d7-fe2c-a80e97dfbb7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytrends\n",
            "  Downloading pytrends-4.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.12/dist-packages (from pytrends) (2.32.4)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.12/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from pytrends) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25->pytrends) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25->pytrends) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25->pytrends) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25->pytrends) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0->pytrends) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0->pytrends) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0->pytrends) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0->pytrends) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.17.0)\n",
            "Downloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging and Cleaning"
      ],
      "metadata": {
        "id": "RfeFfoeA4Si_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, glob, os\n",
        "\n",
        "# -------- settings --------\n",
        "TOP_N_PER_THEME = 300   # set None to skip the top-N file\n",
        "\n",
        "# -------- locate files --------\n",
        "csv_paths = glob.glob(\"*keywords.csv\")\n",
        "print(\"Files found:\", csv_paths, \"\\n\")\n",
        "assert csv_paths, \"No *keywords.csv files found in the working folder.\"\n",
        "\n",
        "# -------- helpers --------\n",
        "def infer_theme(filename: str) -> str:\n",
        "    name = os.path.splitext(os.path.basename(filename.lower()))[0]\n",
        "    if \"buy\" in name or \"demand\" in name: return \"buying_demand\"\n",
        "    if \"mortgage\" in name or \"financing\" in name: return \"mortgage_financing\"\n",
        "    if \"market\" in name or \"awareness\" in name: return \"market_awareness\"\n",
        "    if \"rent\" in name or \"afford\" in name: return \"renting_affordability\"\n",
        "    if \"policy\" in name or \"economic\" in name: return \"economic_policy\"\n",
        "    return \"unknown\"\n",
        "\n",
        "def read_kp_file(path: str) -> pd.DataFrame:\n",
        "    # Keyword Planner CSVs are typically UTF-16 + tab\n",
        "    df = pd.read_csv(path, encoding=\"utf-16\", sep=\"\\t\")\n",
        "    # standardise headers\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    # keep only the two columns we need\n",
        "    assert \"Keyword\" in df.columns, f\"'Keyword' not found in {path}\"\n",
        "    assert \"Avg. monthly searches\" in df.columns, f\"'Avg. monthly searches' not found in {path}\"\n",
        "    out = pd.DataFrame({\n",
        "        \"keyword\": df[\"Keyword\"].astype(str).str.lower().str.strip(),\n",
        "        \"avg_monthly_searches\": pd.to_numeric(\n",
        "            df[\"Avg. monthly searches\"].astype(str).str.replace(\",\", \"\", regex=False),\n",
        "            errors=\"coerce\"\n",
        "        )\n",
        "    })\n",
        "    out[\"theme\"] = infer_theme(path)\n",
        "    # tidy\n",
        "    out = out[out[\"keyword\"].str.len() > 0].drop_duplicates(subset=\"keyword\").reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "# -------- process & merge --------\n",
        "frames = []\n",
        "for p in csv_paths:\n",
        "    cur = read_kp_file(p)\n",
        "    frames.append(cur)\n",
        "    print(f\"✓ {os.path.basename(p)} — {len(cur)} rows, volumes non-null: {cur['avg_monthly_searches'].notna().sum()}\")\n",
        "\n",
        "master = pd.concat(frames, ignore_index=True)\n",
        "# if the same keyword appears in multiple themes, keep the highest volume row\n",
        "master = (master.sort_values(\"avg_monthly_searches\", ascending=False)\n",
        "                .drop_duplicates(subset=\"keyword\", keep=\"first\")\n",
        "                .sort_values([\"theme\",\"keyword\"])\n",
        "                .reset_index(drop=True))\n",
        "\n",
        "master.to_csv(\"master_keywords_with_volume.csv\", index=False)\n",
        "print(\"\\nSaved: master_keywords_with_volume.csv\")\n",
        "print(\"Per-theme counts:\\n\", master.groupby(\"theme\").size())\n",
        "print(\"\\nVolume summary (non-null):\\n\", master.loc[master[\"avg_monthly_searches\"].notna(),\"avg_monthly_searches\"].describe())\n",
        "\n",
        "# -------- optional: top-N per theme --------\n",
        "if TOP_N_PER_THEME is not None:\n",
        "    topN = (master.dropna(subset=[\"avg_monthly_searches\"])\n",
        "                   .sort_values([\"theme\",\"avg_monthly_searches\"], ascending=[True, False])\n",
        "                   .groupby(\"theme\", as_index=False)\n",
        "                   .head(TOP_N_PER_THEME)\n",
        "                   .reset_index(drop=True))\n",
        "    topN.to_csv(\"master_keywords_top.csv\", index=False)\n",
        "    print(f\"\\nSaved top {TOP_N_PER_THEME} per theme → master_keywords_top.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDB5n_EQSBik",
        "outputId": "4401c405-5a45-4402-9258-180f3f09e7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files found: ['renting and affordability_keywords.csv', 'economic_policy_keywords.csv', 'buying_demand_keywords.csv', 'market_awareness_keywords.csv', 'mortgage_financing_keywords.csv'] \n",
            "\n",
            "✓ renting and affordability_keywords.csv — 1367 rows, volumes non-null: 1356\n",
            "✓ economic_policy_keywords.csv — 3192 rows, volumes non-null: 3192\n",
            "✓ buying_demand_keywords.csv — 2055 rows, volumes non-null: 2054\n",
            "✓ market_awareness_keywords.csv — 2709 rows, volumes non-null: 2665\n",
            "✓ mortgage_financing_keywords.csv — 4540 rows, volumes non-null: 4530\n",
            "\n",
            "Saved: master_keywords_with_volume.csv\n",
            "Per-theme counts:\n",
            " theme\n",
            "buying_demand            2031\n",
            "economic_policy          3055\n",
            "market_awareness         2707\n",
            "mortgage_financing       4453\n",
            "renting_affordability    1365\n",
            "dtype: int64\n",
            "\n",
            "Volume summary (non-null):\n",
            " count     13545.000000\n",
            "mean       2185.939461\n",
            "std       21578.221515\n",
            "min           0.000000\n",
            "25%          50.000000\n",
            "50%          50.000000\n",
            "75%         500.000000\n",
            "max      500000.000000\n",
            "Name: avg_monthly_searches, dtype: float64\n",
            "\n",
            "Saved top 300 per theme → master_keywords_top.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting only top 5 keywords from each theme"
      ],
      "metadata": {
        "id": "ZO-07pR2X_t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, glob, os\n",
        "\n",
        "# ---- 1) Find your files ----\n",
        "csv_paths = glob.glob(\"*keywords.csv\")\n",
        "assert csv_paths, \"No *keywords.csv files found.\"\n",
        "print(\"Found:\", csv_paths, \"\\n\")\n",
        "\n",
        "# ---- 2) Helpers ----\n",
        "def infer_theme(filename: str) -> str:\n",
        "    name = os.path.splitext(os.path.basename(filename.lower()))[0]\n",
        "    if \"buy\" in name or \"demand\" in name: return \"buying_demand\"\n",
        "    if \"mortgage\" in name or \"financing\" in name: return \"mortgage_financing\"\n",
        "    if \"market\" in name or \"awareness\" in name: return \"market_awareness\"\n",
        "    if \"rent\" in name or \"afford\" in name: return \"renting_affordability\"\n",
        "    if \"policy\" in name or \"economic\" in name: return \"economic_policy\"\n",
        "    return \"unknown\"\n",
        "\n",
        "def read_kp(path: str) -> pd.DataFrame:\n",
        "    # Keyword Planner CSVs are UTF-16 + TAB\n",
        "    df = pd.read_csv(path, encoding=\"utf-16\", sep=\"\\t\")\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    assert \"Keyword\" in df.columns and \"Avg. monthly searches\" in df.columns, \\\n",
        "        f\"Expected columns missing in {path}\"\n",
        "    out = pd.DataFrame({\n",
        "        \"keyword\": df[\"Keyword\"].astype(str).str.lower().str.strip(),\n",
        "        \"avg_monthly_searches\": pd.to_numeric(\n",
        "            df[\"Avg. monthly searches\"].astype(str).str.replace(\",\", \"\", regex=False),\n",
        "            errors=\"coerce\"\n",
        "        )\n",
        "    })\n",
        "    out[\"theme\"] = infer_theme(path)\n",
        "    out = out[out[\"keyword\"].str.len() > 0]\n",
        "    return out\n",
        "\n",
        "# ---- 3) Per-file: select Top 5 by volume ----\n",
        "top_per_file = []\n",
        "for p in csv_paths:\n",
        "    cur = read_kp(p)\n",
        "    # Drop NaNs, sort, then take top 5\n",
        "    cur = cur.dropna(subset=[\"avg_monthly_searches\"])\n",
        "    top5 = (cur.sort_values(\"avg_monthly_searches\", ascending=False)\n",
        "                .head(5)\n",
        "                .reset_index(drop=True))\n",
        "    print(f\"✓ {os.path.basename(p)} → picked {len(top5)} rows\")\n",
        "    # Save optional per-file top 5\n",
        "    top5.to_csv(f\"{infer_theme(p)}_top5.csv\", index=False)\n",
        "    top_per_file.append(top5)\n",
        "\n",
        "# ---- 4) Merge the five Top-5 into one file ----\n",
        "merged_top5 = pd.concat(top_per_file, ignore_index=True)\n",
        "\n",
        "# If the same keyword appears in more than one theme, keep the highest-volume instance\n",
        "merged_top5 = (merged_top5.sort_values(\"avg_monthly_searches\", ascending=False)\n",
        "                           .drop_duplicates(subset=\"keyword\", keep=\"first\")\n",
        "                           .sort_values([\"theme\",\"avg_monthly_searches\"], ascending=[True, False])\n",
        "                           .reset_index(drop=True))\n",
        "\n",
        "merged_top5.to_csv(\"master_keywords_top5_per_file_merged.csv\", index=False)\n",
        "\n",
        "print(\"\\nCounts per theme:\")\n",
        "print(merged_top5[\"theme\"].value_counts())\n",
        "print(\"\\nSaved:\")\n",
        "print(\" - master_keywords_top5_per_file_merged.csv\")\n",
        "print(\" - <theme>_top5.csv for each file\")\n",
        "print(\"\\nPreview:\")\n",
        "print(merged_top5.head(25))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "rS_bPT1HYACJ",
        "outputId": "d38bdfd8-c780-4fad-9bdd-7b4baa5985b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "No *keywords.csv files found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3765867791.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ---- 1) Find your files ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcsv_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*keywords.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mcsv_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No *keywords.csv files found.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: No *keywords.csv files found."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting top 20 keywords from each theme"
      ],
      "metadata": {
        "id": "Y0C7BzJRcme7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the full master file (already has avg_monthly_searches + theme)\n",
        "kw = pd.read_csv(\"master_keywords_with_volume.csv\")\n",
        "\n",
        "# Keep only non-null search volumes\n",
        "kw = kw.dropna(subset=[\"avg_monthly_searches\"])\n",
        "\n",
        "# Pick top 20 keywords per theme\n",
        "top20 = (kw.sort_values([\"theme\",\"avg_monthly_searches\"], ascending=[True, False])\n",
        "            .groupby(\"theme\", as_index=False)\n",
        "            .head(20)\n",
        "            .reset_index(drop=True))\n",
        "\n",
        "print(top20[\"theme\"].value_counts())\n",
        "top20.to_csv(\"master_keywords_top20.csv\", index=False)\n",
        "print(\"Saved: master_keywords_top20.csv (100 rows)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgHNZ-w6cqgz",
        "outputId": "2ae97d6a-c0ad-4136-ec85-5dda39e975c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theme\n",
            "buying_demand            20\n",
            "economic_policy          20\n",
            "market_awareness         20\n",
            "mortgage_financing       20\n",
            "renting_affordability    20\n",
            "Name: count, dtype: int64\n",
            "✅ Saved: master_keywords_top20.csv (100 rows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the selected keyword search intensity from Google Trends"
      ],
      "metadata": {
        "id": "C9IT8U7EtDdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pytrends\n",
        "import pandas as pd, numpy as np, time\n",
        "from pytrends.request import TrendReq\n",
        "\n",
        "KEYWORD_FILE = \"master_keywords_top20.csv\"\n",
        "GEO = \"GB\"\n",
        "TIMEFRAME = \"2005-01-01 2025-06-30\"  # Jan 2005 → June 2025\n",
        "\n",
        "kw = pd.read_csv(KEYWORD_FILE)\n",
        "kw[\"keyword\"] = kw[\"keyword\"].astype(str).str.strip().str.lower()\n",
        "kw = kw.drop_duplicates(subset=\"keyword\").reset_index(drop=True)\n",
        "print(\"Total keywords:\", len(kw))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJFSCclStK8v",
        "outputId": "c4c2c30e-8caa-43db-d034-fd9a96ade40a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total keywords: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the data at weekly frequency in batches of 5 keywords"
      ],
      "metadata": {
        "id": "s8M1TarstQJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pytrends = TrendReq(hl='en-GB', tz=0)\n",
        "\n",
        "def batches(lst, n=5):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i+n]\n",
        "\n",
        "theme_map = dict(kw[[\"keyword\",\"theme\"]].values)\n",
        "\n",
        "weekly_frames, failed = [], []\n",
        "\n",
        "for group in batches(list(kw[\"keyword\"]), 5):\n",
        "    try:\n",
        "        pytrends.build_payload(group, geo=GEO, timeframe=TIMEFRAME)\n",
        "        df = pytrends.interest_over_time()\n",
        "        if df.empty:\n",
        "            failed.extend(group); time.sleep(2); continue\n",
        "        df = df.drop(columns=[c for c in [\"isPartial\"] if c in df.columns])\n",
        "        df = df.reset_index().rename(columns={\"date\":\"date\"})\n",
        "        long = df.melt(id_vars=\"date\", var_name=\"keyword\", value_name=\"hits\")\n",
        "        long[\"theme\"] = long[\"keyword\"].map(theme_map)\n",
        "        weekly_frames.append(long)\n",
        "    except Exception as e:\n",
        "        failed.extend(group)\n",
        "    time.sleep(1.5)  # polite delay to avoid throttling\n",
        "\n",
        "trends_long_weekly = (pd.concat(weekly_frames, ignore_index=True)\n",
        "                      if weekly_frames else\n",
        "                      pd.DataFrame(columns=[\"date\",\"keyword\",\"hits\",\"theme\"]))\n",
        "# types\n",
        "trends_long_weekly[\"date\"] = pd.to_datetime(trends_long_weekly[\"date\"])\n",
        "trends_long_weekly[\"hits\"] = pd.to_numeric(trends_long_weekly[\"hits\"], errors=\"coerce\")\n",
        "\n",
        "print(\"Downloaded weekly rows:\", len(trends_long_weekly))\n",
        "print(\"Failed keywords:\", len(failed))\n",
        "trends_long_weekly.to_csv(\"trends_long_weekly.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wKJ8_9DtPNr",
        "outputId": "7afec547-5ea6-440f-eb53-5d13fd84a4c1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded weekly rows: 18450\n",
            "Failed keywords: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (failed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDcpwwbPunTM",
        "outputId": "f324f05a-eb8a-4079-9635-94c2c87447e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mortgage rates', '0 interest credit cards', 'annual percentage rate', 'apr rate', 'bank england base rate', 'bank of england interest rate', 'bank of england interest rate uk', 'bank of england lending rate', 'bank of england official bank rate', 'barclays mortgage rates', 'rightmove sold prices', 'zoopla house prices', 'comparative market', 'house prices', 'house prices crash', 'right move sold', 'right move sold prices', 'rightmove house prices', 'rightmove sold', 'rightmove sold house prices', 'home loan mortgage calculator', 'home loan mortgage rates', 'house loan repayment calculator', 'mortgage calculator', 'mortgage estimator']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "failed_keys = ['mortgage rates', '0 interest credit cards', 'annual percentage rate', 'apr rate', 'bank england base rate', 'bank of england interest rate', 'bank of england interest rate uk', 'bank of england lending rate', 'bank of england official bank rate', 'barclays mortgage rates', 'rightmove sold prices', 'zoopla house prices', 'comparative market', 'house prices', 'house prices crash', 'right move sold', 'right move sold prices', 'rightmove house prices', 'rightmove sold', 'rightmove sold house prices', 'home loan mortgage calculator', 'home loan mortgage rates', 'house loan repayment calculator', 'mortgage calculator', 'mortgage estimator']\n",
        "\n",
        "\n",
        "new_kw = pd.DataFrame()\n",
        "for i, row in kw.iterrows():\n",
        "  if row[\"keyword\"] in failed_keys:\n",
        "    new_kw = kw[kw[\"keyword\"].isin(failed_keys)].copy()\n",
        "print(new_kw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9_fKhCJhYIS",
        "outputId": "f6b91d9f-101c-4eb8-e48e-bc7c8b10239b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               keyword  avg_monthly_searches  \\\n",
            "25                      mortgage rates              500000.0   \n",
            "26             0 interest credit cards               50000.0   \n",
            "27              annual percentage rate               50000.0   \n",
            "28                            apr rate               50000.0   \n",
            "29              bank england base rate               50000.0   \n",
            "35       bank of england interest rate               50000.0   \n",
            "36    bank of england interest rate uk               50000.0   \n",
            "37        bank of england lending rate               50000.0   \n",
            "38  bank of england official bank rate               50000.0   \n",
            "39             barclays mortgage rates               50000.0   \n",
            "40               rightmove sold prices              500000.0   \n",
            "41                 zoopla house prices              500000.0   \n",
            "42                  comparative market               50000.0   \n",
            "43                        house prices               50000.0   \n",
            "44                  house prices crash               50000.0   \n",
            "50                     right move sold               50000.0   \n",
            "51              right move sold prices               50000.0   \n",
            "52              rightmove house prices               50000.0   \n",
            "53                      rightmove sold               50000.0   \n",
            "54         rightmove sold house prices               50000.0   \n",
            "60       home loan mortgage calculator              500000.0   \n",
            "61            home loan mortgage rates              500000.0   \n",
            "62     house loan repayment calculator              500000.0   \n",
            "63                 mortgage calculator              500000.0   \n",
            "64                  mortgage estimator              500000.0   \n",
            "\n",
            "                 theme  \n",
            "25     economic_policy  \n",
            "26     economic_policy  \n",
            "27     economic_policy  \n",
            "28     economic_policy  \n",
            "29     economic_policy  \n",
            "35     economic_policy  \n",
            "36     economic_policy  \n",
            "37     economic_policy  \n",
            "38     economic_policy  \n",
            "39     economic_policy  \n",
            "40    market_awareness  \n",
            "41    market_awareness  \n",
            "42    market_awareness  \n",
            "43    market_awareness  \n",
            "44    market_awareness  \n",
            "50    market_awareness  \n",
            "51    market_awareness  \n",
            "52    market_awareness  \n",
            "53    market_awareness  \n",
            "54    market_awareness  \n",
            "60  mortgage_financing  \n",
            "61  mortgage_financing  \n",
            "62  mortgage_financing  \n",
            "63  mortgage_financing  \n",
            "64  mortgage_financing  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytrends = TrendReq(hl='en-GB', tz=0)\n",
        "\n",
        "def batches(lst, n=5):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i+n]\n",
        "\n",
        "theme_map = dict(new_kw[[\"keyword\",\"theme\"]].values)\n",
        "\n",
        "weekly_frames, failed = [], []\n",
        "\n",
        "for group in batches(list(new_kw[\"keyword\"]), 5):\n",
        "    try:\n",
        "        pytrends.build_payload(group, geo=GEO, timeframe=TIMEFRAME)\n",
        "        df = pytrends.interest_over_time()\n",
        "        if df.empty:\n",
        "            failed.extend(group); time.sleep(2); continue\n",
        "        df = df.drop(columns=[c for c in [\"isPartial\"] if c in df.columns])\n",
        "        df = df.reset_index().rename(columns={\"date\":\"date\"})\n",
        "        long = df.melt(id_vars=\"date\", var_name=\"keyword\", value_name=\"hits\")\n",
        "        long[\"theme\"] = long[\"keyword\"].map(theme_map)\n",
        "        weekly_frames.append(long)\n",
        "    except Exception as e:\n",
        "        failed.extend(group)\n",
        "    time.sleep(1.5)  # polite delay to avoid throttling\n",
        "\n",
        "trends_long_weekly_2 = (pd.concat(weekly_frames, ignore_index=True)\n",
        "                      if weekly_frames else\n",
        "                      pd.DataFrame(columns=[\"date\",\"keyword\",\"hits\",\"theme\"]))\n",
        "# types\n",
        "trends_long_weekly_2[\"date\"] = pd.to_datetime(trends_long_weekly[\"date\"])\n",
        "trends_long_weekly_2[\"hits\"] = pd.to_numeric(trends_long_weekly[\"hits\"], errors=\"coerce\")\n",
        "\n",
        "print(\"Downloaded weekly rows:\", len(trends_long_weekly_2))\n",
        "print(\"Failed keywords:\", len(failed))\n",
        "trends_long_weekly_2.to_csv(\"trends_long_weekly_2.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgwEfjnggT1z",
        "outputId": "7f66518c-533b-45d8-9a0d-7b7ec31301b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded weekly rows: 6150\n",
            "Failed keywords: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(failed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geUSszD2mGKG",
        "outputId": "0312754f-165c-4ebe-df87-c756aea9333c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([trends_long_weekly, trends_long_weekly_2], ignore_index=True)\n",
        "combined_df.to_csv(\"trends_long_weekly_combined.csv\", index=False)"
      ],
      "metadata": {
        "id": "oAveOzZxogny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversion of weekly frequency to monthly"
      ],
      "metadata": {
        "id": "WLw7HnB31IyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df[\"date\"] = pd.to_datetime(combined_df[\"date\"])\n",
        "combined_df[\"hits\"] = pd.to_numeric(combined_df[\"hits\"], errors=\"coerce\")\n",
        "\n",
        "trends_long_monthly = (\n",
        "    combined_df\n",
        "      .set_index(\"date\")\n",
        "      .groupby([\"keyword\",\"theme\"])\n",
        "      .resample(\"MS\")[\"hits\"]\n",
        "      .mean()\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "trends_wide_monthly = trends_long_monthly.pivot_table(index=\"date\", columns=\"keyword\", values=\"hits\")\n",
        "\n",
        "trends_long_monthly.to_csv(\"trends_long_monthly.csv\", index=False)\n",
        "trends_wide_monthly.to_csv(\"trends_wide_monthly.csv\")\n",
        "print(\"Saved: trends_long_monthly.csv and trends_wide_monthly.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-CgWHNH1Mtq",
        "outputId": "20afc05a-912b-4b41-b67e-376a05f92f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: trends_long_monthly.csv and trends_wide_monthly.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering the sparse keywords"
      ],
      "metadata": {
        "id": "a9bYqKXD1lEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-NA count per keyword\n",
        "non_na = trends_long_monthly.groupby(\"keyword\")[\"hits\"].apply(lambda s: s.notna().sum())\n",
        "keep_min24 = non_na[non_na >= 24].index\n",
        "\n",
        "# Zero share\n",
        "zero_share = (trends_long_monthly\n",
        "              .groupby(\"keyword\")[\"hits\"]\n",
        "              .apply(lambda s: (s.fillna(0)==0).mean()))\n",
        "\n",
        "keep_nonflat = zero_share[zero_share < 0.80].index  # keep if <80% zeros\n",
        "\n",
        "keep_keywords = set(keep_min24).intersection(set(keep_nonflat))\n",
        "tlm_f = trends_long_monthly[trends_long_monthly[\"keyword\"].isin(keep_keywords)].copy()\n",
        "\n",
        "print(\"Kept keywords:\", tlm_f[\"keyword\"].nunique(), \"of\", trends_long_monthly[\"keyword\"].nunique())\n",
        "tlm_f.to_csv(\"trends_long_monthly_filtered.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U12IxEWd1n4D",
        "outputId": "13b24cfe-e822-4144-bf7c-ac412d863f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kept keywords: 77 of 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating theme indices"
      ],
      "metadata": {
        "id": "mUKKEgko2JJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theme_monthly = (tlm_f\n",
        "                 .groupby([\"theme\",\"date\"])[\"hits\"]\n",
        "                 .mean()\n",
        "                 .reset_index())\n",
        "\n",
        "theme_wide = theme_monthly.pivot(index=\"date\", columns=\"theme\", values=\"hits\").reset_index()\n",
        "theme_wide = theme_wide.rename(columns={\n",
        "    \"buying_demand\": \"trend_buying_demand\",\n",
        "    \"mortgage_financing\": \"trend_mortgage_financing\",\n",
        "    \"market_awareness\": \"trend_market_awareness\",\n",
        "    \"renting_affordability\": \"trend_renting_affordability\",\n",
        "    \"economic_policy\": \"trend_economic_policy\"\n",
        "}).sort_values(\"date\")\n",
        "\n",
        "theme_wide.to_csv(\"theme_trends_monthly.csv\", index=False)\n",
        "print(\"Saved theme_trends_monthly.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR_F5T2_2Mbh",
        "outputId": "785b7c6b-c4d1-43e6-f590-b7d325776db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved theme_trends_monthly.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardisation of the indices"
      ],
      "metadata": {
        "id": "ibOjhZTm3-zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "theme_wide = pd.read_csv(\"theme_trends_monthly.csv\", parse_dates=[\"date\"])\n",
        "\n",
        "# Standardise each theme index (z-score)\n",
        "theme_std = theme_wide.copy()\n",
        "for col in theme_std.columns:\n",
        "    if col != \"date\":\n",
        "        theme_std[col] = (theme_std[col] - theme_std[col].mean()) / theme_std[col].std(ddof=0)\n",
        "\n",
        "theme_std.to_csv(\"theme_trends_monthly_standardised.csv\", index=False)\n",
        "print(\"Saved: theme_trends_monthly_standardised.csv\")\n"
      ],
      "metadata": {
        "id": "h-bbFENH4Cnw",
        "outputId": "b9795f0c-5b80-4789-af34-ce0d18e1b49f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: theme_trends_monthly_standardised.csv\n"
          ]
        }
      ]
    }
  ]
}