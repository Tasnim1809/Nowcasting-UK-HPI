{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Rebuild regional merge (MDY parsing) and save with dd/mm/yyyy formatting\n",
        "!pip -q install XlsxWriter\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from functools import reduce\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/Datasets\")        # <- your folder\n",
        "OUT  = BASE / \"MERGED_OUTPUTS\"; OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- find files by fragments (case-insensitive)\n",
        "def find_one(*frags):\n",
        "    frags = [f.lower() for f in frags]\n",
        "    for p in BASE.iterdir():\n",
        "        if p.is_file() and all(f in p.stem.lower() for f in frags):\n",
        "            return p\n",
        "    raise FileNotFoundError(f\"Missing file with fragments: {frags}\")\n",
        "\n",
        "paths = {\n",
        "    \"HPI\"      : find_one(\"hpi_clean_panel\"),\n",
        "    \"AWE\"      : find_one(\"averageearnings_monthly_cleaned_2005_2025\"),\n",
        "    \"UNEMP\"    : find_one(\"unemploymentrate_monthly_cleaned_2005_2025\"),\n",
        "    \"CPI\"      : find_one(\"cpi_monthly_cleaned_2005_2025\"),\n",
        "    \"APPR\"     : find_one(\"mortgageapprovals_monthly_cleaned_2005_2025\"),\n",
        "    \"MORT2Y\"   : find_one(\"mortgagerate_monthly_cleaned_2005_2025\"),\n",
        "    \"BANKRATE\" : find_one(\"bankrate_monthly_cleaned_2005_2025\"),\n",
        "    \"CONF\"     : find_one(\"ConsumerConfidence_monthly_cleaned_2005_2025_modified\"),\n",
        "    \"CONST\"    : find_one(\"BuildingMaterials_monthly_cleaned_2005_2025\"),\n",
        "    \"TRENDS\"   : find_one(\"trends_long_monthly_new\"),\n",
        "}\n",
        "\n",
        "# --- helpers\n",
        "def read_any(p: Path):\n",
        "    if p.suffix.lower() in (\".xlsx\",\".xls\"): return pd.read_excel(p)\n",
        "    try: return pd.read_csv(p)\n",
        "    except UnicodeDecodeError: return pd.read_csv(p, encoding=\"latin1\")\n",
        "\n",
        "def to_month_mdy(s):\n",
        "    # FIX: use how=\"start\" (not \"start\" positional)\n",
        "    return pd.to_datetime(s, errors=\"coerce\", dayfirst=False).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
        "\n",
        "def clean_numeric(df):\n",
        "    for c in df.columns:\n",
        "        if c == \"Date\": continue\n",
        "        df[c] = (df[c].astype(str)\n",
        "                        .str.replace(\"%\",\"\", regex=False)\n",
        "                        .str.replace(\",\",\"\", regex=False)\n",
        "                        .str.extract(r\"([-+]?\\d*\\.?\\d+)\")[0]\n",
        "                        .astype(float))\n",
        "    return df\n",
        "\n",
        "def load_macro_mdy(path, rename=None, prefix=None):\n",
        "    df = read_any(path).copy()\n",
        "    date_col = \"Date\" if \"Date\" in df.columns else df.columns[0]\n",
        "    df[\"Date\"] = to_month_mdy(df[date_col])\n",
        "    df = df.dropna(subset=[\"Date\"])\n",
        "    df = df.drop(columns=[c for c in df.columns if c.lower() in (\"year\",\"month\") and c!=\"Date\"], errors=\"ignore\")\n",
        "    df = df[[\"Date\"] + [c for c in df.columns if c!=\"Date\"]]\n",
        "    df = clean_numeric(df)\n",
        "    if rename: df = df.rename(columns=rename)\n",
        "    if prefix: df = df.rename(columns={c: f\"{prefix}{c}\" for c in df.columns if c!=\"Date\"})\n",
        "    return (df.sort_values(\"Date\")\n",
        "              .groupby(\"Date\", as_index=False)\n",
        "              .agg({c:\"last\" for c in df.columns if c!=\"Date\"}))\n",
        "\n",
        "# --- load macros (all MDY)\n",
        "awe   = load_macro_mdy(paths[\"AWE\"],   {\"TotalWeeklyEarnings\":\"AWE_Total\",\"RegularWeeklyEarnings\":\"AWE_Regular\",\n",
        "                                        \"awe_total\":\"AWE_Total\",\"awe_regular\":\"AWE_Regular\"})\n",
        "unemp = load_macro_mdy(paths[\"UNEMP\"], {\"UnemploymentRate\":\"UnemploymentRate\"})\n",
        "cpi   = load_macro_mdy(paths[\"CPI\"],   {\"CPI\":\"CPI\"})\n",
        "appr  = load_macro_mdy(paths[\"APPR\"],  {\"MortgageApprovals\":\"MortgageApprovals\"})\n",
        "mort2 = load_macro_mdy(paths[\"MORT2Y\"],{\"MortgageRate\":\"MortgageRate_2YFix\"})\n",
        "br    = load_macro_mdy(paths[\"BANKRATE\"],{\"BankRate\":\"BankRate\"})\n",
        "conf  = load_macro_mdy(paths[\"CONF\"],  {\"ConsumerConfidence\":\"ConsumerConfidence\"})\n",
        "const = load_macro_mdy(paths[\"CONST\"], prefix=\"BM_\")\n",
        "trends= load_macro_mdy(paths[\"TRENDS\"])\n",
        "trends = trends.rename(columns={c: (c if str(c).lower().startswith(\"gt_\") else f\"gt_{c}\") for c in trends.columns if c!=\"Date\"})\n",
        "\n",
        "macro = reduce(lambda l,r: pd.merge(l, r, on=\"Date\", how=\"outer\"),\n",
        "               [awe, unemp, cpi, appr, mort2, br, conf, const, trends])\n",
        "\n",
        "# --- load HPI (explicit MDY)\n",
        "hpi_raw = read_any(paths[\"HPI\"]).copy()\n",
        "if \"Date\" not in hpi_raw.columns:\n",
        "    raise ValueError(\"HPI file must have a 'Date' column.\")\n",
        "hpi_raw[\"Date\"] = to_month_mdy(hpi_raw[\"Date\"])\n",
        "\n",
        "# normalise identifiers\n",
        "for c in list(hpi_raw.columns):\n",
        "    cl = c.lower()\n",
        "    if \"region\" in cl and \"name\" in cl and \"RegionName\" not in hpi_raw.columns:\n",
        "        hpi_raw = hpi_raw.rename(columns={c:\"RegionName\"})\n",
        "    if cl in (\"areacode\",\"area_code\",\"code\") and \"AreaCode\" not in hpi_raw.columns:\n",
        "        hpi_raw = hpi_raw.rename(columns={c:\"AreaCode\"})\n",
        "\n",
        "keep = [c for c in [\"Date\",\"RegionName\",\"AreaCode\",\"AveragePrice\",\"Index\",\"SalesVolume\"] if c in hpi_raw.columns]\n",
        "hpi   = hpi_raw[keep].sort_values([\"RegionName\",\"Date\"] if \"RegionName\" in keep else \"Date\")\n",
        "\n",
        "# --- clip to study window and merge\n",
        "start, end = pd.Timestamp(2005,1,1), pd.Timestamp(2025,6,1)\n",
        "macro = macro[(macro[\"Date\"]>=start) & (macro[\"Date\"]<=end)]\n",
        "hpi   = hpi[(hpi[\"Date\"]>=start) & (hpi[\"Date\"]<=end)]\n",
        "\n",
        "hpi_reg = pd.merge(hpi, macro, on=\"Date\", how=\"left\", validate=\"m:1\")\n",
        "\n",
        "# --- save with dd/mm/yyyy formatting\n",
        "hpi_reg = hpi_reg.copy()\n",
        "hpi_reg[\"Date\"] = pd.to_datetime(hpi_reg[\"Date\"])\n",
        "\n",
        "xlsx_path = OUT / \"HPI_regional_merged_2005_2025_ddmmyyyy_MODIFIED.xlsx\"\n",
        "with pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\", datetime_format=\"dd/mm/yyyy\") as xl:\n",
        "    hpi_reg.to_excel(xl, index=False, sheet_name=\"HPI_Regional\")\n",
        "    ws = xl.sheets[\"HPI_Regional\"]; wb = xl.book\n",
        "    ws.set_column(hpi_reg.columns.get_loc(\"Date\"), hpi_reg.columns.get_loc(\"Date\"), 12,\n",
        "                  wb.add_format({\"num_format\":\"dd/mm/yyyy\"}))\n",
        "\n",
        "csv_path = OUT / \"HPI_regional_merged_2005_2025_ddmmyyyy_MODIFIED.csv\"\n",
        "tmp = hpi_reg.copy(); tmp[\"Date\"] = tmp[\"Date\"].dt.strftime(\"%d/%m/%Y\"); tmp.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"Saved Excel:\", xlsx_path)\n",
        "print(\"Saved CSV  :\", csv_path)\n",
        "files.download(str(xlsx_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "kM6AEyGdEUwg",
        "outputId": "b3ad8457-0e5e-495f-9535-56125fd85a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/175.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m174.1/175.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2913269189.py:43: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  return pd.to_datetime(s, errors=\"coerce\", dayfirst=False).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Excel: /content/drive/MyDrive/Datasets/MERGED_OUTPUTS/HPI_regional_merged_2005_2025_ddmmyyyy_MODIFIED.xlsx\n",
            "Saved CSV  : /content/drive/MyDrive/Datasets/MERGED_OUTPUTS/HPI_regional_merged_2005_2025_ddmmyyyy_MODIFIED.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a88ec200-b452-40ca-99a3-e13f9c2582dc\", \"HPI_regional_merged_2005_2025_ddmmyyyy_MODIFIED.xlsx\", 9435322)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Build regional HPI–macro–GT keywords merge (MDY date parsing)\n",
        "# ============================================================\n",
        "!pip -q install XlsxWriter\n",
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from functools import reduce\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/Datasets\")        # <- your folder\n",
        "OUT  = BASE / \"MERGED_OUTPUTS\"; OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- find files by fragments (case-insensitive)\n",
        "def find_one(*frags):\n",
        "    frags = [f.lower() for f in frags]\n",
        "    for p in BASE.iterdir():\n",
        "        if p.is_file() and all(f in p.stem.lower() for f in frags):\n",
        "            return p\n",
        "    raise FileNotFoundError(f\"Missing file with fragments: {frags}\")\n",
        "\n",
        "paths = {\n",
        "    \"HPI\"      : find_one(\"hpi_clean_panel\"),\n",
        "    \"AWE\"      : find_one(\"averageearnings_monthly_cleaned_2005_2025\"),\n",
        "    \"UNEMP\"    : find_one(\"unemploymentrate_monthly_cleaned_2005_2025\"),\n",
        "    \"CPI\"      : find_one(\"cpi_monthly_cleaned_2005_2025\"),\n",
        "    \"APPR\"     : find_one(\"mortgageapprovals_monthly_cleaned_2005_2025\"),\n",
        "    \"MORT2Y\"   : find_one(\"mortgagerate_monthly_cleaned_2005_2025\"),\n",
        "    \"BANKRATE\" : find_one(\"bankrate_monthly_cleaned_2005_2025\"),\n",
        "    \"CONF\"     : find_one(\"ConsumerConfidence_monthly_cleaned_2005_2025_modified\"),\n",
        "    \"CONST\"    : find_one(\"BuildingMaterials_monthly_cleaned_2005_2025\"),\n",
        "    \"TRENDS\"   : find_one(\"trends_long_monthly_new\"),\n",
        "}\n",
        "\n",
        "# --- helpers\n",
        "def read_any(p: Path):\n",
        "    if p.suffix.lower() in (\".xlsx\",\".xls\"):\n",
        "        return pd.read_excel(p)\n",
        "    try:\n",
        "        return pd.read_csv(p)\n",
        "    except UnicodeDecodeError:\n",
        "        return pd.read_csv(p, encoding=\"latin1\")\n",
        "\n",
        "def to_month_mdy(s):\n",
        "    # parse MDY, convert to month start\n",
        "    return pd.to_datetime(s, errors=\"coerce\", dayfirst=False).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
        "\n",
        "def clean_numeric(df):\n",
        "    for c in df.columns:\n",
        "        if c == \"Date\":\n",
        "            continue\n",
        "        df[c] = (\n",
        "            df[c].astype(str)\n",
        "                 .str.replace(\"%\",\"\", regex=False)\n",
        "                 .str.replace(\",\",\"\", regex=False)\n",
        "                 .str.extract(r\"([-+]?\\d*\\.?\\d+)\")[0]\n",
        "                 .astype(float)\n",
        "        )\n",
        "    return df\n",
        "\n",
        "def load_macro_mdy(path, rename=None, prefix=None):\n",
        "    df = read_any(path).copy()\n",
        "    date_col = \"Date\" if \"Date\" in df.columns else df.columns[0]\n",
        "    df[\"Date\"] = to_month_mdy(df[date_col])\n",
        "    df = df.dropna(subset=[\"Date\"])\n",
        "    df = df.drop(columns=[c for c in df.columns if c.lower() in (\"year\",\"month\") and c!=\"Date\"], errors=\"ignore\")\n",
        "    df = df[[\"Date\"] + [c for c in df.columns if c!=\"Date\"]]\n",
        "    df = clean_numeric(df)\n",
        "    if rename:\n",
        "        df = df.rename(columns=rename)\n",
        "    if prefix:\n",
        "        df = df.rename(columns={c: f\"{prefix}{c}\" for c in df.columns if c!=\"Date\"})\n",
        "    return (\n",
        "        df.sort_values(\"Date\")\n",
        "          .groupby(\"Date\", as_index=False)\n",
        "          .agg({c:\"last\" for c in df.columns if c!=\"Date\"})\n",
        "    )\n",
        "\n",
        "# --- load macro series (all MDY)\n",
        "awe   = load_macro_mdy(paths[\"AWE\"],   {\"TotalWeeklyEarnings\":\"AWE_Total\",\"RegularWeeklyEarnings\":\"AWE_Regular\",\n",
        "                                        \"awe_total\":\"AWE_Total\",\"awe_regular\":\"AWE_Regular\"})\n",
        "unemp = load_macro_mdy(paths[\"UNEMP\"], {\"UnemploymentRate\":\"UnemploymentRate\"})\n",
        "cpi   = load_macro_mdy(paths[\"CPI\"],   {\"CPI\":\"CPI\"})\n",
        "appr  = load_macro_mdy(paths[\"APPR\"],  {\"MortgageApprovals\":\"MortgageApprovals\"})\n",
        "mort2 = load_macro_mdy(paths[\"MORT2Y\"],{\"MortgageRate\":\"MortgageRate_2YFix\"})\n",
        "br    = load_macro_mdy(paths[\"BANKRATE\"],{\"BankRate\":\"BankRate\"})\n",
        "conf  = load_macro_mdy(paths[\"CONF\"],  {\"ConsumerConfidence\":\"ConsumerConfidence\"})\n",
        "const = load_macro_mdy(paths[\"CONST\"], prefix=\"BM_\")\n",
        "\n",
        "# --- load Google Trends keywords (long → wide)\n",
        "tr_raw = read_any(paths[\"TRENDS\"]).copy()\n",
        "# Expect columns: keyword, theme, date, hits\n",
        "tr_raw[\"Date\"] = to_month_mdy(tr_raw[\"date\"])\n",
        "tr_raw = tr_raw.dropna(subset=[\"Date\"])\n",
        "\n",
        "# build safe column names: gt_{theme}__{clean_keyword}\n",
        "tr_raw[\"keyword_clean\"] = (\n",
        "    tr_raw[\"keyword\"]\n",
        "        .astype(str)\n",
        "        .str.lower()\n",
        "        .str.strip()\n",
        "        .str.replace(r\"[^0-9a-z]+\", \"_\", regex=True)\n",
        "        .str.strip(\"_\")\n",
        ")\n",
        "tr_raw[\"colname\"] = \"gt_\" + tr_raw[\"theme\"].astype(str).str.lower() + \"__\" + tr_raw[\"keyword_clean\"]\n",
        "\n",
        "trends = (\n",
        "    tr_raw.pivot_table(index=\"Date\", columns=\"colname\", values=\"hits\", aggfunc=\"mean\")\n",
        "          .sort_index()\n",
        "          .reset_index()\n",
        ")\n",
        "\n",
        "# --- merge all macro + GT keyword data by Date\n",
        "macro = reduce(lambda l,r: pd.merge(l, r, on=\"Date\", how=\"outer\"),\n",
        "               [awe, unemp, cpi, appr, mort2, br, conf, const, trends])\n",
        "\n",
        "# --- load HPI (explicit MDY)\n",
        "hpi_raw = read_any(paths[\"HPI\"]).copy()\n",
        "if \"Date\" not in hpi_raw.columns:\n",
        "    raise ValueError(\"HPI file must have a 'Date' column.\")\n",
        "hpi_raw[\"Date\"] = to_month_mdy(hpi_raw[\"Date\"])\n",
        "\n",
        "# normalise identifiers\n",
        "for c in list(hpi_raw.columns):\n",
        "    cl = c.lower()\n",
        "    if \"region\" in cl and \"name\" in cl and \"RegionName\" not in hpi_raw.columns:\n",
        "        hpi_raw = hpi_raw.rename(columns={c:\"RegionName\"})\n",
        "    if cl in (\"areacode\",\"area_code\",\"code\") and \"AreaCode\" not in hpi_raw.columns:\n",
        "        hpi_raw = hpi_raw.rename(columns={c:\"AreaCode\"})\n",
        "\n",
        "keep = [c for c in [\"Date\",\"RegionName\",\"AreaCode\",\"AveragePrice\",\"Index\",\"SalesVolume\"] if c in hpi_raw.columns]\n",
        "hpi   = hpi_raw[keep].sort_values([\"RegionName\",\"Date\"] if \"RegionName\" in keep else \"Date\")\n",
        "\n",
        "# --- clip to study window and merge\n",
        "start, end = pd.Timestamp(2005,1,1), pd.Timestamp(2025,6,1)\n",
        "macro = macro[(macro[\"Date\"]>=start) & (macro[\"Date\"]<=end)]\n",
        "hpi   = hpi[(hpi[\"Date\"]>=start) & (hpi[\"Date\"]<=end)]\n",
        "\n",
        "hpi_reg = pd.merge(hpi, macro, on=\"Date\", how=\"left\", validate=\"m:1\")\n",
        "\n",
        "# --- save with dd/mm/yyyy formatting\n",
        "hpi_reg = hpi_reg.copy()\n",
        "hpi_reg[\"Date\"] = pd.to_datetime(hpi_reg[\"Date\"])\n",
        "\n",
        "xlsx_path = OUT / \"HPI_regional_merged_2005_2025_ddmmyyyy_MODIFIED_with_GTkeywords.xlsx\"\n",
        "with pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\", datetime_format=\"dd/mm/yyyy\") as xl:\n",
        "    hpi_reg.to_excel(xl, index=False, sheet_name=\"HPI_Regional\")\n",
        "    ws = xl.sheets[\"HPI_Regional\"]; wb = xl.book\n",
        "    ws.set_column(hpi_reg.columns.get_loc(\"Date\"), hpi_reg.columns.get_loc(\"Date\"), 12,\n",
        "                  wb.add_format({\"num_format\":\"dd/mm/yyyy\"}))\n",
        "\n",
        "csv_path = OUT / \"HPI_regional_merged_2005_2025_ddmmyyyy_MODIFIED_with_GTkeywords.csv\"\n",
        "tmp = hpi_reg.copy(); tmp[\"Date\"] = tmp[\"Date\"].dt.strftime(\"%d/%m/%Y\"); tmp.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"Saved Excel:\", xlsx_path)\n",
        "print(\"Saved CSV  :\", csv_path)\n",
        "files.download(str(xlsx_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "FLJEbYBiwA3R",
        "outputId": "03d7f3da-86b4-4bdd-8c81-443066d3772f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved Excel: /content/drive/MyDrive/Datasets/MERGED_OUTPUTS/HPI_regional_merged_2005_2025_ddmmyyyy_MODIFIED_with_GTkeywords.xlsx\n",
            "Saved CSV  : /content/drive/MyDrive/Datasets/MERGED_OUTPUTS/HPI_regional_merged_2005_2025_ddmmyyyy_MODIFIED_with_GTkeywords.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_abc2c482-ee7b-4ad1-a7b9-62d8bdb4694a\", \"HPI_regional_merged_2005_2025_ddmmyyyy_MODIFIED_with_GTkeywords.xlsx\", 41838776)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}